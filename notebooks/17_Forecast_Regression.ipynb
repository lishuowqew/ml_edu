{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ. –ú–æ–¥–µ–ª–∏."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> üöÄ –í —ç—Ç–æ–π –ø—Ä–∞–∫—Ç–∏–∫–µ –Ω–∞–º –ø–æ–Ω–∞–¥–æ–±—è—Ç—Å—è: `etna==2.10.0, numpy==1.26.4, pandas==1.5.3, matplotlib==3.10.3, seaborn==0.13.2, lightgbm==4.6.0, prophet==1.1.6` \n",
    "\n",
    "> üöÄ –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –≤—ã –∏—Ö –º–æ–∂–µ—Ç–µ —Å –ø–æ–º–æ—â—å—é –∫–æ–º–∞–Ω–¥—ã: `%pip install etna==2.10.0 numpy==1.26.4 pandas==1.5.3 matplotlib==3.10.3 seaborn==0.13.2 lightgbm==4.6.0 prophet==1.1.6` \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ\n",
    "\n",
    "* [–ó–∞–≥—Ä—É–∑–∫–∞ –≤—ã–±–æ—Ä–æ–∫](#–ó–∞–≥—Ä—É–∑–∫–∞-–≤—ã–±–æ—Ä–æ–∫)\n",
    "* [–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –º–æ–¥–µ–ª–∏](#–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ-–º–æ–¥–µ–ª–∏)\n",
    "* [–ú–æ–¥–µ–ª–∏ —Ä–µ–≥—Ä–µ—Å—Å–æ—Ä—ã](#–ú–æ–¥–µ–ª–∏-—Ä–µ–≥—Ä–µ—Å—Å–æ—Ä—ã)\n",
    "  * [–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤](#–ì–µ–Ω–µ—Ä–∞—Ü–∏—è-–ø—Ä–∏–∑–Ω–∞–∫–æ–≤)\n",
    "  * [–õ–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è](#–õ–∏–Ω–µ–π–Ω–∞—è-—Ä–µ–≥—Ä–µ—Å—Å–∏—è)\n",
    "  * [LGBM : –ë—É—Å—Ç–∏–Ω–≥](#LGBM-:-–ë—É—Å—Ç–∏–Ω–≥)\n",
    "* [–ó–∞–∫–ª—é—á–µ–Ω–∏–µ](#–ó–∞–∫–ª—é—á–µ–Ω–∏–µ)\n",
    "* [–í–æ–ø—Ä–æ—Å—ã –¥–ª—è –∑–∞–∫—Ä–µ–ø–ª–µ–Ω–∏—è](#–í–æ–ø—Ä–æ—Å—ã-–¥–ª—è-–∑–∞–∫—Ä–µ–ø–ª–µ–Ω–∏—è)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü—Ä–∏–≤–µ—Ç! –í –ø—Ä–µ–¥—ã–¥—É—â–µ–º –Ω–æ—É—Ç–±—É–∫–µ –º—ã –ø–æ–¥–≥–æ—Ç–æ–≤–∏–ª–∏ –≤—ã–±–æ—Ä–∫–∏, –Ω–∞ –∫–æ—Ç–æ—Ä—ã—Ö —Ç–µ–ø–µ—Ä—å –±—É–¥–µ–º —É—á–∏—Ç—å –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥–µ–ª–∏. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "_cell_id": "hazT9XSLxttiqRH6"
   },
   "outputs": [],
   "source": [
    "from datetime import date, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "from etna.datasets.tsdataset import TSDataset\n",
    "from etna.models import HoltWintersModel\n",
    "from etna.pipeline import Pipeline\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from prophet.make_holidays import make_holidays_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ó–∞–≥—Ä—É–∑–∫–∞ –≤—ã–±–æ—Ä–æ–∫"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–ª—è –Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏–º —Ä–∞–Ω–µ–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–µ –≤—ã–±–æ—Ä–∫–∏."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "_cell_id": "BPY0vaQ3gzUo2r8u"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((222288, 5), (2728, 5))"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpath = Path().cwd() / \"ts_datasets\"\n",
    "\n",
    "train_fpath = dpath / \"train.csv\"\n",
    "train_df = pd.read_csv(train_fpath, parse_dates=[\"timestamp\"], index_col=0)\n",
    "\n",
    "test_fpath = dpath / \"test.csv\"\n",
    "test_df = pd.read_csv(test_fpath, parse_dates=[\"timestamp\"], index_col=0)\n",
    "\n",
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í –¥–∞–ª—å–Ω–µ–π—à–µ–º –Ω–∞–º —Ç–∞–∫–∂–µ –ø–æ–Ω–∞–¥–æ–±–∏—Ç—Å—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ: \n",
    "- –≥–æ—Ä–∏–∑–æ–Ω—Ç–µ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è (–≤ –Ω–∞—à–µ–º —Å–ª—É—á–∞–µ —Ö–æ—Ç–∏–º –Ω–∞ 31 –¥–µ–Ω—å);\n",
    "- –¥–∞—Ç–∞ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö (1 –¥–µ–∫–∞–±—Ä—è 2016–≥).\n",
    "\n",
    "–í—ã–Ω–µ—Å–µ–º –∏—Ö –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ, —á—Ç–æ–±—ã –º–µ–Ω—å—à–µ —Ö–∞—Ä–¥–∫–æ–¥–∏—Ç—å. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "_cell_id": "ZIjDI0y4MJ4tijz9"
   },
   "outputs": [],
   "source": [
    "HORIZON = 31\n",
    "SPLIT_DATE = date(2016, 12, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –º–æ–¥–µ–ª–∏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ù–∞—á–Ω—ë–º —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π. –¢–∞–∫–∏–º –º–æ–¥–µ–ª—è–º –ø–æ —Å—É—Ç–∏ –Ω–µ –Ω—É–∂–Ω–æ –æ–±—É—á–µ–Ω–∏–µ, —Ç.–∫. –æ–Ω–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç —á–∏—Å—Ç–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏, –Ω–∞ –±–∞–∑–µ –∫–æ—Ç–æ—Ä—ã—Ö –≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤ –±—É–¥—É—â–µ–µ. \n",
    "\n",
    "–í —ç—Ç–æ—Ç —Ä–∞–∑ –º—ã –ø–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –º–æ–¥–µ–ª—å –•–æ–ª—å—Ç–∞-–£–∏–Ω—Ç–µ—Ä—Å–∞. –ó–≤—É—á–∏—Ç –ø–∞—Ñ–æ—Å–Ω–æ, –Ω–æ –ø–æ–¥ –∫—Ä–∞—Å–∏–≤—ã–º –Ω–∞–∑–≤–∞–Ω–∏—è–º —Å–∫—Ä—ã–≤–∞–µ—Ç—Å—è —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–µ —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ. \n",
    "\n",
    "–≠—Ç–∞ –º–æ–¥–µ–ª—å —É–∂–µ –µ—Å—Ç—å –≤ ETNA, –¥–∞–≤–∞–π—Ç–µ –µ—é –∏ –≤–æ—Å–ø–æ–ª—å–∑—É–µ–º—Å—è."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢–∞–∫ –∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ–º —Å ETNA, —Å–Ω–æ–≤–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Ç–∞–±–ª–∏—Ü—ã –≤ ETNA-–¥–∞—Ç–∞—Å–µ—Ç—ã –∏ –∏–º–∏—Ç–∏—Ä—É–µ–º `segment` –¥–ª—è –Ω–∞—à–µ–≥–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä—è–¥–∞. –°–¥–µ–ª–∞–µ–º —ç—Ç–æ –∫–∞–∫ –≤ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ, —Ç–∞–∫ –∏ –≤ —Ç–µ—Å—Ç–æ–≤–æ–π. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "_cell_id": "rNViH4toETaFg82e"
   },
   "outputs": [],
   "source": [
    "train_ts_df = train_df.copy(deep=True)\n",
    "test_ts_df = test_df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "_cell_id": "omJrza0g6VBbXkD5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–¢–µ—Å—Ç—ã –ø—Ä–æ—à–ª–∏! –í—Å—ë —Ö–æ—Ä–æ—à–æ!\n"
     ]
    }
   ],
   "source": [
    "# TODO - –¥–æ–±–∞–≤—å—Ç–µ –∫–æ–ª–æ–Ω–∫—É segment –≤ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏\n",
    "#    –æ–±—ä–¥–∏–Ω–∏—Ç–µ –∫–æ–ª–æ–Ω–∫–∏ country, store –∏ product —á–µ—Ä–µ–∑ –Ω–∏–∂–Ω–µ–µ –ø–æ–¥—á—ë—Ä–∫–∏–≤–∞–Ω–∏–µ (_)\n",
    "train_ts_df[\"segment\"] = train_ts_df[\"country\"] + \"_\" + train_ts_df[\"store\"] + \"_\" + train_ts_df[\"product\"]\n",
    "test_ts_df[\"segment\"] = test_ts_df[\"country\"] + \"_\" + test_ts_df[\"store\"] + \"_\" + test_ts_df[\"product\"]\n",
    "\n",
    "assert \"segment\" in train_ts_df.columns, \"–ö–æ–ª–æ–Ω–∫–∞ `segment` –Ω–µ –Ω–∞–π–¥–µ–Ω–∞\"\n",
    "assert \"segment\" in test_ts_df.columns, \"–ö–æ–ª–æ–Ω–∫–∞ `segment` –Ω–µ –Ω–∞–π–¥–µ–Ω–∞\"\n",
    "\n",
    "print(\"–¢–µ—Å—Ç—ã –ø—Ä–æ—à–ª–∏! –í—Å—ë —Ö–æ—Ä–æ—à–æ!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "_cell_id": "wvWD9eXlxA3ZPHjN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–¢–µ—Å—Ç—ã –ø—Ä–æ—à–ª–∏! –í—Å—ë —Ö–æ—Ä–æ—à–æ!\n"
     ]
    }
   ],
   "source": [
    "assert \"segment\" in train_ts_df.columns, \"–ö–æ–ª–æ–Ω–∫–∞ `segment` –Ω–µ –Ω–∞–π–¥–µ–Ω–∞\"\n",
    "assert \"segment\" in test_ts_df.columns, \"–ö–æ–ª–æ–Ω–∫–∞ `segment` –Ω–µ –Ω–∞–π–¥–µ–Ω–∞\"\n",
    "\n",
    "print(\"–¢–µ—Å—Ç—ã –ø—Ä–æ—à–ª–∏! –í—Å—ë —Ö–æ—Ä–æ—à–æ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "_cell_id": "9Ph9XDREyRMvm1ji"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–¢–µ—Å—Ç—ã –ø—Ä–æ—à–ª–∏! –í—Å—ë —Ö–æ—Ä–æ—à–æ!\n"
     ]
    }
   ],
   "source": [
    "# TODO - —É–¥–∞–ª–∏—Ç–µ –∫–æ–ª–æ–Ω–∫–∏ country, store –∏ product\n",
    "train_ts_df = train_ts_df.drop(columns=[\"country\", \"store\", \"product\"])\n",
    "test_ts_df = test_ts_df.drop(columns=[\"country\", \"store\", \"product\"])\n",
    "\n",
    "train_cols = train_ts_df.columns\n",
    "test_cols = test_ts_df.columns\n",
    "\n",
    "for col_name in [\"country\", \"store\", \"product\"]:\n",
    "    assert col_name not in train_cols, f\"–í –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ –≤—Å—ë –µ—â—ë –µ—Å—Ç—å –∫–æ–ª–æ–Ω–∫–∞ {col_name}\"\n",
    "    assert col_name not in test_cols, f\"–í —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ –≤—Å—ë –µ—â—ë –µ—Å—Ç—å –∫–æ–ª–æ–Ω–∫–∞ {col_name}\"\n",
    "\n",
    "print(\"–¢–µ—Å—Ç—ã –ø—Ä–æ—à–ª–∏! –í—Å—ë —Ö–æ—Ä–æ—à–æ!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "_cell_id": "j5IQqT6WUKVG6kly"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–¢–µ—Å—Ç—ã –ø—Ä–æ—à–ª–∏! –í—Å—ë —Ö–æ—Ä–æ—à–æ!\n"
     ]
    }
   ],
   "source": [
    "train_cols = train_ts_df.columns\n",
    "test_cols = test_ts_df.columns \n",
    "\n",
    "for col_name in [\"country\", \"store\", \"product\"]:\n",
    "    assert col_name not in train_cols, f\"–í –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ –≤—Å—ë –µ—â—ë –µ—Å—Ç—å –∫–æ–ª–æ–Ω–∫–∞ {col_name}\"\n",
    "    assert col_name not in test_cols, f\"–í —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ –≤—Å—ë –µ—â—ë –µ—Å—Ç—å –∫–æ–ª–æ–Ω–∫–∞ {col_name}\"\n",
    "\n",
    "print(\"–¢–µ—Å—Ç—ã –ø—Ä–æ—à–ª–∏! –í—Å—ë —Ö–æ—Ä–æ—à–æ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_id": "RVADwWBdvReoDuMT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–¢–µ—Å—Ç—ã –ø—Ä–æ—à–ª–∏! –í—Å—ë —Ö–æ—Ä–æ—à–æ!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from etna.datasets.tsdataset import TSDataset\n",
    "\n",
    "\n",
    "train_df[\"segment\"] = train_df[\"country\"] + \"_\" + train_df[\"store\"] + \"_\" + train_df[\"product\"]\n",
    "test_df[\"segment\"] = test_df[\"country\"] + \"_\" + test_df[\"store\"] + \"_\" + test_df[\"product\"]\n",
    "\n",
    "\n",
    "assert \"segment\" in train_df.columns, \"train_df Áº∫Â∞ë segment Âàó\"\n",
    "assert \"segment\" in test_df.columns, \"test_df Áº∫Â∞ë segment Âàó\"\n",
    "\n",
    "\n",
    "\n",
    "required_cols = [\"timestamp\", \"segment\", \"target\"]\n",
    "assert all(col in train_df.columns for col in required_cols), \"train_df Áº∫Â∞ëÂøÖË¶ÅÂàó\"\n",
    "assert all(col in test_df.columns for col in required_cols), \"test_df Áº∫Â∞ëÂøÖË¶ÅÂàó\"\n",
    "\n",
    "\n",
    "train_wide_df = TSDataset.to_dataset(train_df)  \n",
    "test_wide_df = TSDataset.to_dataset(test_df)\n",
    "\n",
    "\n",
    "train_ts_df = TSDataset(df=train_wide_df, freq=\"D\")\n",
    "test_ts_df = TSDataset(df=test_wide_df, freq=\"D\")\n",
    "\n",
    "\n",
    "\n",
    "assert isinstance(train_ts_df, TSDataset), f\"–ù–µ–≤–µ—Ä–Ω—ã–π —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö: {type(train_ts_df)}\"\n",
    "assert isinstance(test_ts_df, TSDataset), f\"–ù–µ–≤–µ—Ä–Ω—ã–π —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö: {type(test_ts_df)}\"\n",
    "\n",
    "print(\"–¢–µ—Å—Ç—ã –ø—Ä–æ—à–ª–∏! –í—Å—ë —Ö–æ—Ä–æ—à–æ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "_cell_id": "QEvt6fBnkhrIZfu8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–¢–µ—Å—Ç—ã –ø—Ä–æ—à–ª–∏! –í—Å—ë —Ö–æ—Ä–æ—à–æ!\n"
     ]
    }
   ],
   "source": [
    "assert isinstance(train_ts_df, TSDataset), f\"–ù–µ–≤–µ—Ä–Ω—ã–π —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö: {type(train_ts_df)}\"\n",
    "assert isinstance(test_ts_df, TSDataset), f\"–ù–µ–≤–µ—Ä–Ω—ã–π —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö: {type(test_ts_df)}\"\n",
    "\n",
    "print(\"–¢–µ—Å—Ç—ã –ø—Ä–æ—à–ª–∏! –í—Å—ë —Ö–æ—Ä–æ—à–æ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–∞—Ç–∞—Å–µ—Ç—ã —Å–∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã, –º–æ–∂–Ω–æ \"—É—á–∏—Ç—å\" –º–æ–¥–µ–ª—å. \n",
    "\n",
    "–ù–æ –Ω–∞ –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–∏ –ª–µ–∂–∏—Ç –≤–æ–ø—Ä–æ—Å: \"–ê –Ω–∞ —á—ë–º —É—á–∏—Ç—å, –µ—Å–ª–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤-—Ç–æ –Ω–µ—Ç?\". –ò —ç—Ç–æ —Ö–æ—Ä–æ—à–∏–π –≤–æ–ø—Ä–æ—Å. ETNA –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ñ–∏—á–∏ –ø—Ä—è–º–æ —á–µ—Ä–µ–∑ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏.\n",
    "\n",
    "–í –Ω–∞—à–µ–º —Å–ª—É—á–∞–µ –¥–æ–±–∞–≤–∏–º: \n",
    "- –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç—Ä–µ–Ω–¥–µ –≤ –¥–∞–Ω–Ω—ã—Ö;\n",
    "- –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–µ–∑–æ–Ω–Ω–æ–π —Å–æ—Å—Ç–∞–≤–ª—è—é—â–µ–π.\n",
    "\n",
    "<details>\n",
    "    <summary>ü§ì –ù–∞–ø–æ–º–∏–Ω–∞–ª–∫–∞ –ø—Ä–æ —Ç—Ä–µ–Ω–¥—ã –∏ —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç—å [–ù–∞–∂–º–∏ –Ω–∞ –º–µ–Ω—è]</summary>\n",
    "\n",
    "**–¢—Ä–µ–Ω–¥** - —ç—Ç–æ –∫–∞–∫–∞—è-—Ç–æ –æ–±—â–∞—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å, —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω–∞—è –¥–ª—è –≤—Å–µ–≥–æ —Ä—è–¥–∞ —Ü–µ–ª–∏–∫–æ–º. **–°–µ–∑–æ–Ω–Ω–æ—Å—Ç—å** - —ç—Ç–æ –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ –ø–æ–≤—Ç–æ—Ä—è—é—â–∞—è—Å—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞. –û–±–µ —ç—Ç–∏ —Å–æ—Å—Ç–∞–≤–ª—è—é—â–∏–µ —á–∞—Å—Ç–æ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤ —Ä—è–¥–µ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ. \n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "_cell_id": "9l0ZLMQ9JrH6ND1i"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(model = HoltWintersModel(trend = 'add', damped_trend = False, seasonal = 'add', seasonal_periods = 7, initialization_method = 'estimated', initial_level = None, initial_trend = None, initial_seasonal = None, use_boxcox = False, bounds = None, dates = None, freq = None, missing = 'none', smoothing_level = None, smoothing_trend = None, smoothing_seasonal = None, damping_trend = None, ), transforms = (), horizon = 31, )"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = HoltWintersModel(\n",
    "    trend=\"add\", \n",
    "    seasonal=\"add\", \n",
    "    seasonal_periods=7,\n",
    ")\n",
    "pipeline = Pipeline(model=model, horizon=HORIZON)\n",
    "\n",
    "pipeline.fit(train_ts_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ú–æ–¥–µ–ª—å –æ–±—É—á–∏–ª–∏, —Ç–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø–æ–ª—É—á–∏—Ç—å. \n",
    "\n",
    "–ù–µ–º–Ω–æ–≥–æ –ø–æ–∏–≥—Ä–∞–µ–º—Å—è —Å –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–æ–∫, —á—Ç–æ–±—ã –¥–∞–ª—å—à–µ –±—ã–ª–æ —É–¥–æ–±–Ω–µ–µ —Å—á–∏—Ç–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "_cell_id": "Tz46UzUDK7aFsxT1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>y_test</th>\n",
       "      <th>country_x</th>\n",
       "      <th>product_x</th>\n",
       "      <th>store_x</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>country_y</th>\n",
       "      <th>product_y</th>\n",
       "      <th>store_y</th>\n",
       "      <th>country</th>\n",
       "      <th>store</th>\n",
       "      <th>product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Chocolate</td>\n",
       "      <td>Donots</td>\n",
       "      <td>215.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Donots</td>\n",
       "      <td>Chocolate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-12-02</td>\n",
       "      <td>212.0</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Chocolate</td>\n",
       "      <td>Donots</td>\n",
       "      <td>219.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Donots</td>\n",
       "      <td>Chocolate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-12-03</td>\n",
       "      <td>210.0</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Chocolate</td>\n",
       "      <td>Donots</td>\n",
       "      <td>213.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Donots</td>\n",
       "      <td>Chocolate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-12-04</td>\n",
       "      <td>231.0</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Chocolate</td>\n",
       "      <td>Donots</td>\n",
       "      <td>233.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Donots</td>\n",
       "      <td>Chocolate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-12-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Chocolate</td>\n",
       "      <td>Donots</td>\n",
       "      <td>215.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Donots</td>\n",
       "      <td>Chocolate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   timestamp  y_test country_x  product_x store_x  y_pred country_y product_y  \\\n",
       "0 2016-12-01     NaN    Canada  Chocolate  Donots   215.0       NaN       NaN   \n",
       "1 2016-12-02   212.0    Canada  Chocolate  Donots   219.0       NaN       NaN   \n",
       "2 2016-12-03   210.0    Canada  Chocolate  Donots   213.0       NaN       NaN   \n",
       "3 2016-12-04   231.0    Canada  Chocolate  Donots   233.0       NaN       NaN   \n",
       "4 2016-12-05     NaN    Canada  Chocolate  Donots   215.0       NaN       NaN   \n",
       "\n",
       "  store_y country   store    product  \n",
       "0     NaN  Canada  Donots  Chocolate  \n",
       "1     NaN  Canada  Donots  Chocolate  \n",
       "2     NaN  Canada  Donots  Chocolate  \n",
       "3     NaN  Canada  Donots  Chocolate  \n",
       "4     NaN  Canada  Donots  Chocolate  "
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# –ø–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è \n",
    "ts_forecast_df = pipeline.forecast()\n",
    "\n",
    "# –≤ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –ø–µ—Ä–µ–∏–º–µ–Ω—É–µ–º –≤ y_test (—ç—Ç–æ true –∑–Ω–∞—á–µ–Ω–∏—è)\n",
    "forecast_test_df = test_ts_df.to_pandas(flatten=True).rename(columns={\"target\": \"y_test\"})\n",
    "\n",
    "# –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –ø–µ—Ä–µ–∏–º–µ–Ω—É–µ–º –≤ y_pred (—ç—Ç–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è)\n",
    "forecast_df = ts_forecast_df.to_pandas(flatten=True).rename(columns={\"target\": \"y_pred\"})\n",
    "\n",
    "# —Å–ª–µ–ø–∏–º —Ç–∞–±–ª–∏—Ü—ã, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å –æ–±—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
    "hotl_winters_predictions = forecast_test_df.merge(\n",
    "    forecast_df, \n",
    "    on=[\"timestamp\", \"segment\"], \n",
    "    how=\"outer\"\n",
    ")\n",
    "\n",
    "# —Ä–∞–∑–¥–µ–ª—è–µ–º segment –æ–±—Ä–∞—Ç–Ω–æ –Ω–∞ 3 —Å–æ—Å—Ç–∞–≤–ª—è—é—â–∏–µ \n",
    "hotl_winters_predictions[\"country\"] = hotl_winters_predictions[\"segment\"].str.split(\"_\").str[0]\n",
    "hotl_winters_predictions[\"store\"] = hotl_winters_predictions[\"segment\"].str.split(\"_\").str[1]\n",
    "hotl_winters_predictions[\"product\"] = hotl_winters_predictions[\"segment\"].str.split(\"_\").str[2]\n",
    "hotl_winters_predictions = hotl_winters_predictions.drop(columns=[\"segment\"])\n",
    "\n",
    "# –æ–∫—Ä—É–≥–ª—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –æ—Ç –º–æ–¥–µ–ª–∏, —Ç.–∫. –ø—Ä–æ–¥–∞–≤–∞—Ç—å —á–∞—Å—Ç—å –ø—ã—à–∫–∏ - –Ω–µ –≤–∞—Ä–∏–∞–Ω—Ç \n",
    "hotl_winters_predictions[\"y_pred\"] = np.ceil(hotl_winters_predictions[\"y_pred\"])#.astype(int)\n",
    "\n",
    "hotl_winters_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "_cell_id": "QqZeEiFioPOi9QkK"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2728, 12)"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hotl_winters_predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û—Ç—Ä–∏—Å—É–µ–º –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤ —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏ —Å –∏—Å—Ç–∏–Ω–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –Ω–∞ –≥—Ä–∞—Ñ–∏–∫–µ. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "_cell_id": "zSDu4flZ0fOjahoc"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02930e6b90b44bd6999eed60aa708958",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='country', options=('Canada', 'Finland', 'Italy', 'Kenya', 'Norway'‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@widgets.interact(\n",
    "    country=widgets.Dropdown(options=hotl_winters_predictions[\"country\"].unique()),\n",
    "    store=widgets.Dropdown(options=hotl_winters_predictions[\"store\"].unique()),\n",
    "    products=widgets.Dropdown(options=hotl_winters_predictions[\"product\"].unique()),\n",
    "    start_date=widgets.DatePicker(value=hotl_winters_predictions[\"timestamp\"].min()),\n",
    ")\n",
    "def show_holt_winters_predictions(country: str, store: str, products: str, start_date: date):\n",
    "    start_date = pd.Timestamp(start_date)\n",
    "    plot_df = hotl_winters_predictions[\n",
    "        (hotl_winters_predictions[\"country\"] == country)\n",
    "        & (hotl_winters_predictions[\"store\"] == store)\n",
    "        & (hotl_winters_predictions[\"product\"] == products)\n",
    "        & (hotl_winters_predictions[\"timestamp\"].dt.date >= start_date.date())\n",
    "    ]\n",
    "\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=1, figsize=(15, 4))\n",
    "\n",
    "    axs.plot(plot_df[\"timestamp\"], plot_df[\"y_pred\"], marker=\"o\", label=\"pred\")\n",
    "    # —É–±–µ—Ä—ë–º –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è, —á—Ç–æ–±—ã —Ç–æ—á–∫–∏ —Å–æ–µ–¥–∏–Ω–∏–ª–∏—Å—å –º–µ–∂–¥—É —Å–æ–±–æ–π - –≤–æ –∏–º—è —Å—É–±—ä–∫—Ç–∏–≤–Ω–æ–≥–æ –ø—Ä–µ–∫—Ä–∞—Å–Ω–æ–≥–æ \n",
    "    plot_df = plot_df.dropna()\n",
    "    axs.plot(plot_df[\"timestamp\"], plot_df[\"y_test\"], marker=\"o\", label=\"true\")\n",
    "    axs.set_xlabel(\"–î–∞—Ç–∞\")\n",
    "    axs.set_ylabel(\"–ö–æ–ª-–≤–æ –ø—Ä–æ–¥–∞–Ω–Ω—ã—Ö –ø—ã—à–µ–∫\")\n",
    "    axs.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ß—Ç–æ–±—ã –Ω–µ –ø–æ—Ç–µ—Ä—è—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ä–∞–±–æ—Ç—ã –º–æ–¥–µ–ª–∏, —Å–æ—Ö—Ä–∞–Ω–∏–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤ —Ñ–∞–π–ª."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "_cell_id": "E9dwNOeU0MXmB9vk"
   },
   "outputs": [],
   "source": [
    "pred_dpath = Path(\"forecast_predictions\")\n",
    "pred_dpath.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pred_fpath = pred_dpath / \"holt_winters_predictions.csv\"\n",
    "hotl_winters_predictions.to_csv(pred_fpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ú–æ–¥–µ–ª–∏ —Ä–µ–≥—Ä–µ—Å—Å–æ—Ä—ã"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–µ –∂–µ –º–æ–¥–µ–ª–∏, —á—Ç–æ –∏ –≤ \"—Å—ã—Ä–æ–π\" —Ä–µ–≥—Ä–µ—Å—Å–∏–∏. –ù–∞–ø—Ä–∏–º–µ—Ä, –ª–∏–Ω–µ–π–Ω—É—é —Ä–µ–≥—Ä–µ—Å—Å–∏—é –∏–ª–∏ LGBM. –ò—Ö –º—ã –∏ –ø–æ–≤–∞—Ä–∏–º –¥–ª—è –Ω–∞—à–∏—Ö –ø—ã—à–µ–∫.\n",
    "\n",
    "–ù–æ –µ—Å—Ç—å –±–æ–ª—å—à–æ–µ –∂–∏—Ä–Ω–æ–µ **–ù–û**. –ó–¥–µ—Å—å –º—ã —É–∂–µ –Ω–µ —Å–º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å ETNA, –∏ –≤—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –Ω—É–∂–Ω–æ –±—É–¥–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ä—É—á–∫–∞–º–∏. –ß—Ç–æ –∂, –ø–æ–≥–Ω–∞–ª–∏! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü—Ä–∏–∑–Ω–∞–∫–∏ –Ω—É–∂–Ω—ã –±—É–¥—É—Ç –∫–∞–∫ –¥–ª—è –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏, —Ç–∞–∫ –∏ –¥–ª—è —Ç–µ—Å—Ç–æ–≤–æ–π. –ü–æ—ç—Ç–æ–º—É –æ–±—ä–µ–¥–∏–Ω–∏–º –∏—Ö –æ–±—Ä–∞—Ç–Ω–æ –≤ –µ–¥–∏–Ω—É—é —Ç–∞–±–ª–∏—Ü—É, —Å–≥–µ–Ω–µ—Ä–∏–º –ø—Ä–∏–∑–Ω–∞–∫–∏, –∞ –ø–æ—Ç–æ–º —Å–Ω–æ–≤–∞ —Ä–∞–∑–¥–µ–ª–∏–º –ø–æ —Ç–æ–π –∂–µ –¥–∞—Ç–µ 1 –¥–µ–∫–∞–±—Ä—è 2016. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "_cell_id": "khbD51V9lK79jOnU"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>store</th>\n",
       "      <th>product</th>\n",
       "      <th>target</th>\n",
       "      <th>segment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>Canada</td>\n",
       "      <td>Donots</td>\n",
       "      <td>Chocolate</td>\n",
       "      <td>300.0</td>\n",
       "      <td>Canada_Donots_Chocolate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-02</th>\n",
       "      <td>Canada</td>\n",
       "      <td>Donots</td>\n",
       "      <td>Chocolate</td>\n",
       "      <td>281.0</td>\n",
       "      <td>Canada_Donots_Chocolate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-03</th>\n",
       "      <td>Canada</td>\n",
       "      <td>Donots</td>\n",
       "      <td>Chocolate</td>\n",
       "      <td>297.0</td>\n",
       "      <td>Canada_Donots_Chocolate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-04</th>\n",
       "      <td>Canada</td>\n",
       "      <td>Donots</td>\n",
       "      <td>Chocolate</td>\n",
       "      <td>235.0</td>\n",
       "      <td>Canada_Donots_Chocolate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-05</th>\n",
       "      <td>Canada</td>\n",
       "      <td>Donots</td>\n",
       "      <td>Chocolate</td>\n",
       "      <td>240.0</td>\n",
       "      <td>Canada_Donots_Chocolate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           country   store    product  target                  segment\n",
       "timestamp                                                             \n",
       "2010-01-01  Canada  Donots  Chocolate   300.0  Canada_Donots_Chocolate\n",
       "2010-01-02  Canada  Donots  Chocolate   281.0  Canada_Donots_Chocolate\n",
       "2010-01-03  Canada  Donots  Chocolate   297.0  Canada_Donots_Chocolate\n",
       "2010-01-04  Canada  Donots  Chocolate   235.0  Canada_Donots_Chocolate\n",
       "2010-01-05  Canada  Donots  Chocolate   240.0  Canada_Donots_Chocolate"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat((train_df, test_df)).set_index(\"timestamp\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ù–µ—Ç –µ–¥–∏–Ω–æ–≥–æ –≤–µ—Ä–Ω–æ–≥–æ —Å–ø–æ—Å–æ–±–∞ –∫–∞–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –≥–µ–Ω–µ—Ä–∏—Ç—å –∏ –∫–∞–∫, —Ç—É—Ç –≤–∞—Å –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç —Ç–æ–ª—å–∫–æ –≤–∞—à–∞ —Ñ–∞–Ω—Ç–∞–∑–∏—è –∏ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ. \n",
    "\n",
    "–ú—ã —Å–æ–∑–¥–∞–¥–∏–º —Ç—Ä–∏ —Ç–∏–ø–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: \n",
    "- –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –¥–Ω—è—Ö;\n",
    "- –ø—Ä–∏–∑–Ω–∞–∫–∏ –æ –ø—Ä–∞–∑–¥–Ω–∏–∫–∞—Ö;\n",
    "- –ø—Ä–∏–∑–Ω–∞–∫–∏ –æ \"–ª–∞–≥–∞—Ö\".\n",
    "\n",
    "<details>\n",
    "    <summary>ü§ì –ß—Ç–æ —Ç–∞–∫–æ–µ –ª–∞–≥–∏? [–ù–∞–∂–º–∏ –Ω–∞ –º–µ–Ω—è]</summary>\n",
    "\n",
    "**–õ–∞–≥–∏** - —ç—Ç–æ –∫–∞–∫–∏–µ-—Ç–æ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä—è–¥–∞. –ù–∞–ø—Ä–∏–º–µ—Ä, –ø–µ—Ä–≤—ã–π –ª–∞–≥ - —ç—Ç–æ –≤—á–µ—Ä–∞—à–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ, –∞ –ø—è—Ç—ã–π –ª–∞–≥ - —ç—Ç–æ –∑–Ω–∞—á–µ–Ω–∏–µ 5 –¥–Ω–µ–π –Ω–∞–∑–∞–¥ –∏ —Ç.–¥. –¢–∞–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –æ—á–µ–Ω—å –≤–∞–∂–Ω—ã –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, —Ç.–∫. –∏–º –Ω—É–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ—à–ª–æ–º —Ä—è–¥–∞. \n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ì–µ–Ω–µ—Ä–∏—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏ (—Ñ–∏—á–∏) –Ω—É–∂–Ω–æ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ä—è–¥–∞ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ—Å—Ç–∏, —Ç.–∫. –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ä—è–¥–∞ –º—ã –±—É–¥–µ–º —É—á–∏—Ç—å —Å–≤–æ—é —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å. –í –∏—Ç–æ–≥–µ —É –Ω–∞—Å –ø–æ–ª—É—á–∏—Ç—Å—è 90 –º–æ–¥–µ–ª–µ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ (–Ω–∞ —Å–∞–º–æ–º –¥–µ–ª–µ 88, —Ç.–∫. 2 –ø—É—Å—Ç—ã—Ö —Ä—è–¥–∞ –º—ã –≤—ã–∫–∏–Ω—É–ª–∏). –ê –Ω–µ –æ–¥–Ω–∞, –∫–∞–∫ –≤ ETNA. \n",
    "\n",
    "–î–∞–≤–∞–π—Ç–µ –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞ –ø–æ—Å–º–æ—Ç—Ä–∏–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ä—è–¥–∞, –∞ –ø–æ—Ç–æ–º –Ω–∞–ø–∏—à–µ–º —Ü–∏–∫–ª (–ø–∞–π–ø–ª–∞–π–Ω), –∫–æ—Ç–æ—Ä—ã–π –±—É–¥–µ—Ç –ø—Ä–æ—Ö–æ–¥–∏—Ç—å –ø–æ –∫–∞–∂–¥–æ–º—É —Ä—è–¥—É. \n",
    "\n",
    "–î–æ–ø—É—Å—Ç–∏–º –≤—ã–±–µ—Ä–µ–º –∫–æ—Ñ–µ–π–Ω—É—é –ø—ã—à–∫—É –≤ –ø—ã—à–µ—á–Ω–æ–π Pyshka –≤ –°–∏–Ω–≥–∞–ø—É—Ä–µ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "_cell_id": "XLwNTbWclFhGhAM8"
   },
   "outputs": [],
   "source": [
    "SAMPLE_COUNTRY = \"Singapore\"\n",
    "SAMPLE_STORE = \"Pyshka\"\n",
    "SAMPLE_PRODUCT = \"Coffee\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "_cell_id": "k5tjwLcdc2gIijec"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>country</th>\n",
       "      <th>store</th>\n",
       "      <th>product</th>\n",
       "      <th>target</th>\n",
       "      <th>segment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>212184</th>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>Pyshka</td>\n",
       "      <td>Coffee</td>\n",
       "      <td>1045.0</td>\n",
       "      <td>Singapore_Pyshka_Coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212185</th>\n",
       "      <td>2010-01-02</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>Pyshka</td>\n",
       "      <td>Coffee</td>\n",
       "      <td>1010.0</td>\n",
       "      <td>Singapore_Pyshka_Coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212186</th>\n",
       "      <td>2010-01-03</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>Pyshka</td>\n",
       "      <td>Coffee</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>Singapore_Pyshka_Coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212187</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>Pyshka</td>\n",
       "      <td>Coffee</td>\n",
       "      <td>846.0</td>\n",
       "      <td>Singapore_Pyshka_Coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212188</th>\n",
       "      <td>2010-01-05</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>Pyshka</td>\n",
       "      <td>Coffee</td>\n",
       "      <td>770.0</td>\n",
       "      <td>Singapore_Pyshka_Coffee</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        timestamp    country   store product  target                  segment\n",
       "212184 2010-01-01  Singapore  Pyshka  Coffee  1045.0  Singapore_Pyshka_Coffee\n",
       "212185 2010-01-02  Singapore  Pyshka  Coffee  1010.0  Singapore_Pyshka_Coffee\n",
       "212186 2010-01-03  Singapore  Pyshka  Coffee  1040.0  Singapore_Pyshka_Coffee\n",
       "212187 2010-01-04  Singapore  Pyshka  Coffee   846.0  Singapore_Pyshka_Coffee\n",
       "212188 2010-01-05  Singapore  Pyshka  Coffee   770.0  Singapore_Pyshka_Coffee"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAMPLE_COUNTRY = \"Singapore\"\n",
    "SAMPLE_STORE = \"Pyshka\"\n",
    "SAMPLE_PRODUCT = \"Coffee\"\n",
    "\n",
    "# TODO - –æ—Ç—Ñ–∏–ª—å—Ç—Ä—É–π—Ç–µ —Ç–æ–ª—å–∫–æ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ä—è–¥–∞\n",
    "\n",
    "sample_df = train_df[\n",
    "    (train_df[\"country\"] == SAMPLE_COUNTRY) & \n",
    "    (train_df[\"store\"] == SAMPLE_STORE) & \n",
    "    (train_df[\"product\"] == SAMPLE_PRODUCT)\n",
    "]\n",
    "\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "_cell_id": "ASQLA05dVwVNDRGh"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>country</th>\n",
       "      <th>store</th>\n",
       "      <th>product</th>\n",
       "      <th>target</th>\n",
       "      <th>segment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>212184</th>\n",
       "      <td>2010-01-01</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>Pyshka</td>\n",
       "      <td>Coffee</td>\n",
       "      <td>1045.0</td>\n",
       "      <td>Singapore_Pyshka_Coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212185</th>\n",
       "      <td>2010-01-02</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>Pyshka</td>\n",
       "      <td>Coffee</td>\n",
       "      <td>1010.0</td>\n",
       "      <td>Singapore_Pyshka_Coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212186</th>\n",
       "      <td>2010-01-03</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>Pyshka</td>\n",
       "      <td>Coffee</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>Singapore_Pyshka_Coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212187</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>Pyshka</td>\n",
       "      <td>Coffee</td>\n",
       "      <td>846.0</td>\n",
       "      <td>Singapore_Pyshka_Coffee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212188</th>\n",
       "      <td>2010-01-05</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>Pyshka</td>\n",
       "      <td>Coffee</td>\n",
       "      <td>770.0</td>\n",
       "      <td>Singapore_Pyshka_Coffee</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        timestamp    country   store product  target                  segment\n",
       "212184 2010-01-01  Singapore  Pyshka  Coffee  1045.0  Singapore_Pyshka_Coffee\n",
       "212185 2010-01-02  Singapore  Pyshka  Coffee  1010.0  Singapore_Pyshka_Coffee\n",
       "212186 2010-01-03  Singapore  Pyshka  Coffee  1040.0  Singapore_Pyshka_Coffee\n",
       "212187 2010-01-04  Singapore  Pyshka  Coffee   846.0  Singapore_Pyshka_Coffee\n",
       "212188 2010-01-05  Singapore  Pyshka  Coffee   770.0  Singapore_Pyshka_Coffee"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "_cell_id": "qsABHet5TnNxGx60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–¢–µ—Å—Ç—ã –ø—Ä–æ—à–ª–∏! –í—Å—ë —Ö–æ—Ä–æ—à–æ!\n"
     ]
    }
   ],
   "source": [
    "assert set(sample_df[\"country\"].unique()) == set([SAMPLE_COUNTRY]), \"–î–æ–ª–∂–µ–Ω –±—ã—Ç—å —Ç–æ–ª—å–∫–æ –°–∏–Ω–≥–∞–ø—É—Ä –≤ –¥–∞–Ω–Ω—ã—Ö\"\n",
    "assert set(sample_df[\"store\"].unique()) == set([SAMPLE_STORE]), \"–î–æ–ª–∂–Ω–∞ –±—ã—Ç—å —Ç–æ–ª—å–∫–æ –æ–¥–Ω–∞ –ø—ã—à–µ—á–Ω–∞—è –≤ –¥–∞–Ω–Ω—ã—Ö\"\n",
    "assert set(sample_df[\"product\"].unique()) == set([SAMPLE_PRODUCT]), \"–î–æ–ª–∂–Ω–∞ –±—ã—Ç—å —Ç–æ–ª—å–∫–æ –∫–æ—Ñ–µ–π–Ω–∞—è –ø—ã—à–∫–∞ –≤ –¥–∞–Ω–Ω—ã—Ö\"\n",
    "\n",
    "print(\"–¢–µ—Å—Ç—ã –ø—Ä–æ—à–ª–∏! –í—Å—ë —Ö–æ—Ä–æ—à–æ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û—Å—Ç–∞–≤–∏–º —Ç–æ–ª—å–∫–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–∞—Ç–µ –∏ —Ü–µ–ª–µ–≤—É—é –∫–æ–ª–æ–Ω–∫—É. –û—Å—Ç–∞–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ —Å–æ–¥–µ—Ä–∂–∞—Ç –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –∑–Ω–∞—á–µ–Ω–∏—è –∏ –º–æ–¥–µ–ª–∏ –æ–Ω–∏ –Ω–∏—á–µ–º –Ω–µ –ø–æ–º–æ–≥—É—Ç."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "_cell_id": "1Bdi0w7O3pB2ZyHb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>212184</th>\n",
       "      <td>1045.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212185</th>\n",
       "      <td>1010.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212186</th>\n",
       "      <td>1040.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212187</th>\n",
       "      <td>846.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212188</th>\n",
       "      <td>770.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        target\n",
       "212184  1045.0\n",
       "212185  1010.0\n",
       "212186  1040.0\n",
       "212187   846.0\n",
       "212188   770.0"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df = sample_df[[\"target\"]]\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ù–∞—á–Ω—ë–º —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –¥–Ω—è—Ö. –°—é–¥–∞ –æ—Ç–Ω–æ—Å–∏—Ç—Å—è —Ç–∞–∫–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∫–∞–∫ –¥–µ–Ω—å –º–µ—Å—è—Ü–∞, –¥–µ–Ω—å –Ω–µ–¥–µ–ª–∏, —á–µ—Ç–≤–µ—Ä—Ç—å –≥–æ–¥–∞, –∫ –∫–æ—Ç–æ—Ä–æ–º—É –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∏—Ç —Ç–µ–∫—É—â–∏–π timestamp, –ø–æ—Ä—è–¥–∫–æ–≤—ã–π –¥–µ–Ω—å –≥–æ–¥–∞, –ø–æ—Ä—è–¥–∫–æ–≤–∞—è –Ω–µ–¥–µ–ª—è –≥–æ–¥–∞ –∏ —Ç.–¥ –∏ —Ç.–ø. –í—Å—ë, —á—Ç–æ –≤—ã —Ç–æ–ª—å–∫–æ –º–æ–∂–µ—Ç–µ –ø—Ä–∏–¥—É–º–∞—Ç—å.  \n",
    "\n",
    "–î–∞–≤–∞–π—Ç–µ —Å–¥–µ–ª–∞–µ–º –æ—Ç–¥–µ–ª—å–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–∞–∫–∏—Ö —Ñ–∏—á, –æ–Ω–∞ –Ω–∞–º –ø—Ä–∏–≥–æ–¥–∏—Ç—Å—è –µ—â—ë –≤ –ø–∞–π–ø–ª–∞–π–Ω–µ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_id": "9DviDMiQRWW8il0a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sales</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>cos_weekday</th>\n",
       "      <th>sin_weekday</th>\n",
       "      <th>is_leap_year</th>\n",
       "      <th>dayofyear</th>\n",
       "      <th>cos_day</th>\n",
       "      <th>sin_day</th>\n",
       "      <th>quarter</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>weekofyear</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>is_month_start</th>\n",
       "      <th>is_month_end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-01-01</th>\n",
       "      <td>100</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>0.974928</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999852</td>\n",
       "      <td>0.017213</td>\n",
       "      <td>1</td>\n",
       "      <td>2025</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-02</th>\n",
       "      <td>120</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>0.433884</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>0.999407</td>\n",
       "      <td>0.034422</td>\n",
       "      <td>1</td>\n",
       "      <td>2025</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-03</th>\n",
       "      <td>90</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>-0.433884</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>0.998667</td>\n",
       "      <td>0.051620</td>\n",
       "      <td>1</td>\n",
       "      <td>2025</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-04</th>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>-0.974928</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>0.997630</td>\n",
       "      <td>0.068802</td>\n",
       "      <td>1</td>\n",
       "      <td>2025</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-05</th>\n",
       "      <td>130</td>\n",
       "      <td>6</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>-0.781831</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "      <td>0.996298</td>\n",
       "      <td>0.085965</td>\n",
       "      <td>1</td>\n",
       "      <td>2025</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            sales  dayofweek  cos_weekday  sin_weekday  is_leap_year  \\\n",
       "2025-01-01    100          2    -0.222521     0.974928         False   \n",
       "2025-01-02    120          3    -0.900969     0.433884         False   \n",
       "2025-01-03     90          4    -0.900969    -0.433884         False   \n",
       "2025-01-04    110          5    -0.222521    -0.974928         False   \n",
       "2025-01-05    130          6     0.623490    -0.781831         False   \n",
       "\n",
       "            dayofyear   cos_day   sin_day  quarter  year  month  day  \\\n",
       "2025-01-01          1  0.999852  0.017213        1  2025      1    1   \n",
       "2025-01-02          2  0.999407  0.034422        1  2025      1    2   \n",
       "2025-01-03          3  0.998667  0.051620        1  2025      1    3   \n",
       "2025-01-04          4  0.997630  0.068802        1  2025      1    4   \n",
       "2025-01-05          5  0.996298  0.085965        1  2025      1    5   \n",
       "\n",
       "            weekofyear  is_weekend  is_month_start  is_month_end  \n",
       "2025-01-01           1           0            True         False  \n",
       "2025-01-02           1           0           False         False  \n",
       "2025-01-03           1           0           False         False  \n",
       "2025-01-04           1           1           False         False  \n",
       "2025-01-05           1           1           False         False  "
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def create_dtime_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"ÂàõÂª∫Êó∂Èó¥Áõ∏ÂÖ≥ÁâπÂæÅÔºà‰æùËµñDataFrameÁöÑdatetimeÁ¥¢ÂºïÔºâ\"\"\"\n",
    "    df_new = df.copy()\n",
    "    \n",
    "    \n",
    "    df_new[\"dayofweek\"] = df_new.index.dayofweek.tolist()\n",
    "    df_new[\"cos_weekday\"] = np.cos(df_new[\"dayofweek\"] / 7 * 2 * np.pi)\n",
    "    df_new[\"sin_weekday\"] = np.sin(df_new[\"dayofweek\"] / 7 * 2 * np.pi)\n",
    "    \n",
    "    \n",
    "    df_new[\"is_leap_year\"] = df_new.index.is_leap_year.tolist()\n",
    "    df_new[\"dayofyear\"] = df_new.index.dayofyear.tolist()\n",
    "    df_new[\"cos_day\"] = np.cos(df_new[\"dayofyear\"] / (365 + df_new[\"is_leap_year\"]) * 2 * np.pi)\n",
    "    df_new[\"sin_day\"] = np.sin(df_new[\"dayofyear\"] / (365 + df_new[\"is_leap_year\"]) * 2 * np.pi)\n",
    "    \n",
    "   \n",
    "    df_new[\"quarter\"] = df_new.index.quarter.tolist()\n",
    "    df_new[\"year\"] = df_new.index.year.tolist()\n",
    "    df_new[\"month\"] = df_new.index.month.tolist()\n",
    "    df_new[\"day\"] = df_new.index.day.tolist()\n",
    "    df_new[\"weekofyear\"] = df_new.index.isocalendar().week.tolist()\n",
    "    \n",
    "    \n",
    "    df_new[\"is_weekend\"] = 0\n",
    "    df_new.loc[df_new[\"dayofweek\"].isin([5, 6]), \"is_weekend\"] = 1\n",
    "    \n",
    "    \n",
    "    df_new[\"is_month_start\"] = df_new.index.is_month_start.tolist()\n",
    "    df_new[\"is_month_end\"] = df_new.index.is_month_end.tolist()\n",
    "    \n",
    "    return df_new\n",
    "\n",
    "\n",
    "\n",
    "dates = pd.date_range(start=\"2025-01-01\", periods=5, freq=\"D\")\n",
    "sample_df = pd.DataFrame({\n",
    "    \"sales\": [100, 120, 90, 110, 130]\n",
    "}, index=dates)\n",
    "\n",
    "\n",
    "\n",
    "sample_df = create_dtime_features(sample_df)\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "_cell_id": "Z6AhwVFwv1yCou3S"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sales</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>cos_weekday</th>\n",
       "      <th>sin_weekday</th>\n",
       "      <th>is_leap_year</th>\n",
       "      <th>dayofyear</th>\n",
       "      <th>cos_day</th>\n",
       "      <th>sin_day</th>\n",
       "      <th>quarter</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>weekofyear</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>is_month_start</th>\n",
       "      <th>is_month_end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-01-01</th>\n",
       "      <td>100</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>0.974928</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999852</td>\n",
       "      <td>0.017213</td>\n",
       "      <td>1</td>\n",
       "      <td>2025</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-02</th>\n",
       "      <td>120</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>0.433884</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>0.999407</td>\n",
       "      <td>0.034422</td>\n",
       "      <td>1</td>\n",
       "      <td>2025</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-03</th>\n",
       "      <td>90</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>-0.433884</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>0.998667</td>\n",
       "      <td>0.051620</td>\n",
       "      <td>1</td>\n",
       "      <td>2025</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-04</th>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>-0.974928</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>0.997630</td>\n",
       "      <td>0.068802</td>\n",
       "      <td>1</td>\n",
       "      <td>2025</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-05</th>\n",
       "      <td>130</td>\n",
       "      <td>6</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>-0.781831</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "      <td>0.996298</td>\n",
       "      <td>0.085965</td>\n",
       "      <td>1</td>\n",
       "      <td>2025</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            sales  dayofweek  cos_weekday  sin_weekday  is_leap_year  \\\n",
       "2025-01-01    100          2    -0.222521     0.974928         False   \n",
       "2025-01-02    120          3    -0.900969     0.433884         False   \n",
       "2025-01-03     90          4    -0.900969    -0.433884         False   \n",
       "2025-01-04    110          5    -0.222521    -0.974928         False   \n",
       "2025-01-05    130          6     0.623490    -0.781831         False   \n",
       "\n",
       "            dayofyear   cos_day   sin_day  quarter  year  month  day  \\\n",
       "2025-01-01          1  0.999852  0.017213        1  2025      1    1   \n",
       "2025-01-02          2  0.999407  0.034422        1  2025      1    2   \n",
       "2025-01-03          3  0.998667  0.051620        1  2025      1    3   \n",
       "2025-01-04          4  0.997630  0.068802        1  2025      1    4   \n",
       "2025-01-05          5  0.996298  0.085965        1  2025      1    5   \n",
       "\n",
       "            weekofyear  is_weekend  is_month_start  is_month_end  \n",
       "2025-01-01           1           0            True         False  \n",
       "2025-01-02           1           0           False         False  \n",
       "2025-01-03           1           0           False         False  \n",
       "2025-01-04           1           1           False         False  \n",
       "2025-01-05           1           1           False         False  "
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# –í—ã–∑—ã–≤–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é –∏ –ø–æ—Å–º–æ—Ç—Ä–∏–º, —á—Ç–æ —É –Ω–∞—Å —Å—Ç–∞–ª–æ —Å —Ç–∞–±–ª–∏—á–∫–æ–π\n",
    "sample_df = create_dtime_features(sample_df)\n",
    "\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü—É—Ñ! –ò —É –Ω–∞—Å —É–∂–µ 15 –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–∫–æ–ª–æ–Ω–æ–∫) üòé –î–∞–ª—å—à–µ –±–æ–ª—å—à–µ! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢–µ–ø–µ—Ä—å –¥–æ–±–∞–≤–∏–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –æ –ø—Ä–∞–∑–¥–Ω–∏–∫–∞—Ö. –ï—Å—Ç—å —Ä–∞–¥–æ—Å—Ç–Ω–∞—è –Ω–æ–≤–æ—Å—Ç—å, –ø—Ä–∞–∑–¥–Ω–∏–∫–∏ –Ω–µ –Ω—É–∂–Ω–æ –¥–æ–±–∞–≤–ª—è—Ç—å —Ä—É–∫–∞–º–∏, –∑–∞ –Ω–∞—Å —ç—Ç–æ —É–∂–µ —Å–¥–µ–ª–∞–ª–∏ –¥—Ä—É–≥–∏–µ –ª—é–¥–∏ –≤ –±–∏–±–ª–∏–æ—Ç–µ–∫–µ `Prophet`. \n",
    "\n",
    "–î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —É–∫–∞–∑–∞—Ç—å —Å—Ç—Ä–∞–Ω—É –∏ –¥–∏–∞–ø–∞–∑–æ–Ω –ª–µ—Ç. –ò –ø–æ–ª—É—á–∏–º –∏—Å—á–µ—Ä–ø—ã–≤–∞—é—â–∏–π —Å–ø–∏—Å–æ–∫ –ø—Ä–∞–∑–¥–Ω–∏–∫–æ–≤.\n",
    "\n",
    "–î–ª—è —É–¥–æ–±—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏ –∑–∞–∫–æ–¥–∏—Ä—É–µ–º –±–∏–Ω–∞—Ä–Ω—ã–º –∑–Ω–∞—á–µ–Ω–∏–µ–º –∫–∞–∂–¥—ã–π –ø—Ä–∞–∑–¥–Ω–∏–∫."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "_cell_id": "UUHVIi1P7f0ScHQP"
   },
   "outputs": [],
   "source": [
    "def get_holiday_df(country_name: str) -> pd.DataFrame:\n",
    "    holidays_df = make_holidays_df(year_list=range(2010, 2017), country=country_name)\n",
    "\n",
    "    holidays_df[\"is_holiday\"] = 1\n",
    "    holidays_df = holidays_df.rename(columns={\"ds\": \"timestamp\"})\n",
    "    holidays_df = holidays_df.set_index(\"timestamp\")\n",
    "\n",
    "    return holidays_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "_cell_id": "EraTrMeXINgykJyS"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>holiday</th>\n",
       "      <th>is_holiday</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-01</th>\n",
       "      <td>New Year's Day</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-08</th>\n",
       "      <td>Chinese New Year</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-09</th>\n",
       "      <td>Chinese New Year</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-07-06</th>\n",
       "      <td>Eid al-Fitr</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-09-12</th>\n",
       "      <td>Eid al-Adha</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     holiday  is_holiday\n",
       "timestamp                               \n",
       "2016-01-01    New Year's Day           1\n",
       "2016-02-08  Chinese New Year           1\n",
       "2016-02-09  Chinese New Year           1\n",
       "2016-07-06       Eid al-Fitr           1\n",
       "2016-09-12       Eid al-Adha           1"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_holidays_df = get_holiday_df(SAMPLE_COUNTRY)\n",
    "\n",
    "sample_holidays_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ß—Ç–æ–±—ã —Ä–∞—Å—à–∏—Ä–∏—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏, –¥–æ–±–∞–≤–∏–º –≤ —Ç–∞–±–ª–∏—Ü—É –ª–∞–≥–∏ –ø–æ—Å–ª–µ –ø—Ä–∞–∑–¥–Ω–∏—á–Ω–æ–≥–æ –¥–Ω—è (1 –∏ 2 –¥–Ω—è –ø–æ—Å–ª–µ).\n",
    "\n",
    "–î–ª—è —É–¥–æ–±—Å—Ç–≤–∞ –æ—Ñ–æ—Ä–º–∏–º –≤—Å—ë —ç—Ç–æ –≤ –æ—Ç–¥–µ–ª—å–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "_cell_id": "Fbch5Yxhmw5JqjJK"
   },
   "outputs": [],
   "source": [
    "def create_holiday_features(df: pd.DataFrame, holidays_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Create holiday features.\"\"\"\n",
    "    df_new = df.copy()\n",
    "    df_new = df_new.merge(holidays_df[[\"is_holiday\"]], left_index=True, right_index=True, how=\"outer\")\n",
    "    df_new[\"is_holiday\"] = df_new[\"is_holiday\"].fillna(0)\n",
    "    df_new[\"holiday_lag_1\"] = df_new[\"is_holiday\"].shift(1)\n",
    "    df_new[\"holiday_lag_2\"] = df_new[\"is_holiday\"].shift(2)\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "_cell_id": "li7Ufz34S97aEnPK"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sales</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>cos_weekday</th>\n",
       "      <th>sin_weekday</th>\n",
       "      <th>is_leap_year</th>\n",
       "      <th>dayofyear</th>\n",
       "      <th>cos_day</th>\n",
       "      <th>sin_day</th>\n",
       "      <th>quarter</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>weekofyear</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>is_month_start</th>\n",
       "      <th>is_month_end</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>holiday_lag_1</th>\n",
       "      <th>holiday_lag_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-02-14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-02-15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-02-16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-04-02</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            sales  dayofweek  cos_weekday  sin_weekday is_leap_year  \\\n",
       "2010-01-01    NaN        NaN          NaN          NaN          NaN   \n",
       "2010-02-14    NaN        NaN          NaN          NaN          NaN   \n",
       "2010-02-15    NaN        NaN          NaN          NaN          NaN   \n",
       "2010-02-16    NaN        NaN          NaN          NaN          NaN   \n",
       "2010-04-02    NaN        NaN          NaN          NaN          NaN   \n",
       "\n",
       "            dayofyear  cos_day  sin_day  quarter  year  month  day  \\\n",
       "2010-01-01        NaN      NaN      NaN      NaN   NaN    NaN  NaN   \n",
       "2010-02-14        NaN      NaN      NaN      NaN   NaN    NaN  NaN   \n",
       "2010-02-15        NaN      NaN      NaN      NaN   NaN    NaN  NaN   \n",
       "2010-02-16        NaN      NaN      NaN      NaN   NaN    NaN  NaN   \n",
       "2010-04-02        NaN      NaN      NaN      NaN   NaN    NaN  NaN   \n",
       "\n",
       "            weekofyear  is_weekend is_month_start is_month_end  is_holiday  \\\n",
       "2010-01-01         NaN         NaN            NaN          NaN         1.0   \n",
       "2010-02-14         NaN         NaN            NaN          NaN         1.0   \n",
       "2010-02-15         NaN         NaN            NaN          NaN         1.0   \n",
       "2010-02-16         NaN         NaN            NaN          NaN         1.0   \n",
       "2010-04-02         NaN         NaN            NaN          NaN         1.0   \n",
       "\n",
       "            holiday_lag_1  holiday_lag_2  \n",
       "2010-01-01            NaN            NaN  \n",
       "2010-02-14            1.0            NaN  \n",
       "2010-02-15            1.0            1.0  \n",
       "2010-02-16            1.0            1.0  \n",
       "2010-04-02            1.0            1.0  "
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df = create_holiday_features(sample_df, sample_holidays_df)\n",
    "\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ï—â—ë +3 –ø—Ä–∏–∑–Ω–∞–∫–∞. –ò—Ç–æ–≥–æ —É–∂–µ 18 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏.\n",
    "\n",
    "–ü–µ—Ä–µ—Ö–æ–¥–∏–º –∫ \"–ª–∞–≥–Ω—É—Ç—ã–º\" –ø—Ä–∏–∑–Ω–∞–∫–∞–º. –°–≥–µ–Ω–µ—Ä–∏–º –ª–∞–≥–∏ –Ω–∞ 7, 10 –∏ 15 –¥–Ω–µ–π –Ω–∞–∑–∞–¥. \n",
    "\n",
    "–ü–æ–º–∏–º–æ —ç—Ç–æ–≥–æ –¥–æ–±–∞–≤–∏–º –µ—â—ë –Ω–µ–º–Ω–æ–≥–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –ª–∞–≥–∞–º: —Å–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ —Å –æ–∫–Ω–æ–º 14 –∏ 7 –¥–Ω–µ–π. \n",
    "\n",
    "–û–ø—è—Ç—å –∂–µ –∫–∞–∫–∏–µ –ª–∞–≥–∏ –≤—ã–±—Ä–∞—Ç—å, —á—Ç–æ–±—ã –º–æ–¥–µ–ª–∏ –ø–æ–º–æ—á—å, –Ω–∏–∫—Ç–æ –Ω–µ —Å–∫–∞–∂–µ—Ç. –¢—É—Ç –Ω—É–∂–Ω–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –∏ –æ—Ç—Ç–∞–ª–∫–∏–≤–∞—Ç—å—Å—è –æ—Ç –∑–∞–¥–∞—á–∏ –∏ –¥–∞–Ω–Ω—ã—Ö.\n",
    "\n",
    "–î–ª—è —É–¥–æ–±—Å—Ç–≤–∞ –æ–±–µ—Ä–Ω—ë–º –≤—Å—ë –≤ —Ñ—É–Ω–∫—Ü–∏—é."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_id": "5tJxPbjUiwcwgzIS"
   },
   "outputs": [],
   "source": [
    "def create_lag_features(df: pd.DataFrame, horizon: int) -> pd.DataFrame:\n",
    "    \"\"\"Create lag features for time series.\"\"\"\n",
    "    df_new = df.copy()\n",
    "    \n",
    "    \n",
    "    if 'target' not in df_new.columns:\n",
    "       \n",
    "        possible_targets = ['num_sold', 'sales', 'value']  \n",
    "        target_col = None\n",
    "        for col in possible_targets:\n",
    "            if col in df_new.columns:\n",
    "                target_col = col\n",
    "                break\n",
    "        \n",
    "        if target_col is None:\n",
    "            \n",
    "            numeric_cols = df_new.select_dtypes(include=[np.number]).columns\n",
    "            if len(numeric_cols) > 0:\n",
    "                target_col = numeric_cols[0]\n",
    "            else:\n",
    "                raise KeyError(\"Êâæ‰∏çÂà∞ÁõÆÊ†áÂàóÔºåËØ∑Á°Æ‰øùÊï∞ÊçÆÂåÖÂê´Êï∞ÂÄºÂûãÁõÆÊ†áÂèòÈáè\")\n",
    "        \n",
    "       \n",
    "        df_new = df_new.rename(columns={target_col: 'target'})\n",
    "    \n",
    "    \n",
    "    for lag in range(1, horizon + 1):\n",
    "        df_new[f\"lag_{lag}\"] = df_new[\"target\"].shift(lag)\n",
    "    \n",
    "   \n",
    "    for window in [7, 14, 30]:\n",
    "        df_new[f\"rolling_mean_{window}\"] = df_new[\"target\"].shift(1).rolling(window=window).mean()\n",
    "        df_new[f\"rolling_std_{window}\"] = df_new[\"target\"].shift(1).rolling(window=window).std()\n",
    "        df_new[f\"rolling_min_{window}\"] = df_new[\"target\"].shift(1).rolling(window=window).min()\n",
    "        df_new[f\"rolling_max_{window}\"] = df_new[\"target\"].shift(1).rolling(window=window).max()\n",
    "    \n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "_cell_id": "8PfQgMXAoHVfEbF4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>cos_weekday</th>\n",
       "      <th>sin_weekday</th>\n",
       "      <th>is_leap_year</th>\n",
       "      <th>dayofyear</th>\n",
       "      <th>cos_day</th>\n",
       "      <th>sin_day</th>\n",
       "      <th>quarter</th>\n",
       "      <th>year</th>\n",
       "      <th>...</th>\n",
       "      <th>rolling_min_7</th>\n",
       "      <th>rolling_max_7</th>\n",
       "      <th>rolling_mean_14</th>\n",
       "      <th>rolling_std_14</th>\n",
       "      <th>rolling_min_14</th>\n",
       "      <th>rolling_max_14</th>\n",
       "      <th>rolling_mean_30</th>\n",
       "      <th>rolling_std_30</th>\n",
       "      <th>rolling_min_30</th>\n",
       "      <th>rolling_max_30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-02-14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-02-15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-02-16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-04-02</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            target  dayofweek  cos_weekday  sin_weekday is_leap_year  \\\n",
       "2010-01-01     NaN        NaN          NaN          NaN          NaN   \n",
       "2010-02-14     NaN        NaN          NaN          NaN          NaN   \n",
       "2010-02-15     NaN        NaN          NaN          NaN          NaN   \n",
       "2010-02-16     NaN        NaN          NaN          NaN          NaN   \n",
       "2010-04-02     NaN        NaN          NaN          NaN          NaN   \n",
       "\n",
       "            dayofyear  cos_day  sin_day  quarter  year  ...  rolling_min_7  \\\n",
       "2010-01-01        NaN      NaN      NaN      NaN   NaN  ...            NaN   \n",
       "2010-02-14        NaN      NaN      NaN      NaN   NaN  ...            NaN   \n",
       "2010-02-15        NaN      NaN      NaN      NaN   NaN  ...            NaN   \n",
       "2010-02-16        NaN      NaN      NaN      NaN   NaN  ...            NaN   \n",
       "2010-04-02        NaN      NaN      NaN      NaN   NaN  ...            NaN   \n",
       "\n",
       "            rolling_max_7  rolling_mean_14  rolling_std_14 rolling_min_14  \\\n",
       "2010-01-01            NaN              NaN             NaN            NaN   \n",
       "2010-02-14            NaN              NaN             NaN            NaN   \n",
       "2010-02-15            NaN              NaN             NaN            NaN   \n",
       "2010-02-16            NaN              NaN             NaN            NaN   \n",
       "2010-04-02            NaN              NaN             NaN            NaN   \n",
       "\n",
       "           rolling_max_14  rolling_mean_30  rolling_std_30  rolling_min_30  \\\n",
       "2010-01-01            NaN              NaN             NaN             NaN   \n",
       "2010-02-14            NaN              NaN             NaN             NaN   \n",
       "2010-02-15            NaN              NaN             NaN             NaN   \n",
       "2010-02-16            NaN              NaN             NaN             NaN   \n",
       "2010-04-02            NaN              NaN             NaN             NaN   \n",
       "\n",
       "            rolling_max_30  \n",
       "2010-01-01             NaN  \n",
       "2010-02-14             NaN  \n",
       "2010-02-15             NaN  \n",
       "2010-02-16             NaN  \n",
       "2010-04-02             NaN  \n",
       "\n",
       "[5 rows x 62 columns]"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df = create_lag_features(sample_df, horizon=HORIZON)\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "_cell_id": "gYPKE7SESV6wdUJ9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>cos_weekday</th>\n",
       "      <th>sin_weekday</th>\n",
       "      <th>is_leap_year</th>\n",
       "      <th>dayofyear</th>\n",
       "      <th>cos_day</th>\n",
       "      <th>sin_day</th>\n",
       "      <th>quarter</th>\n",
       "      <th>year</th>\n",
       "      <th>...</th>\n",
       "      <th>rolling_min_7</th>\n",
       "      <th>rolling_max_7</th>\n",
       "      <th>rolling_mean_14</th>\n",
       "      <th>rolling_std_14</th>\n",
       "      <th>rolling_min_14</th>\n",
       "      <th>rolling_max_14</th>\n",
       "      <th>rolling_mean_30</th>\n",
       "      <th>rolling_std_30</th>\n",
       "      <th>rolling_min_30</th>\n",
       "      <th>rolling_max_30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2025-01-01</th>\n",
       "      <td>100.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>0.974928</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999852</td>\n",
       "      <td>0.017213</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-02</th>\n",
       "      <td>120.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>0.433884</td>\n",
       "      <td>False</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.999407</td>\n",
       "      <td>0.034422</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-03</th>\n",
       "      <td>90.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>-0.433884</td>\n",
       "      <td>False</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.998667</td>\n",
       "      <td>0.051620</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-04</th>\n",
       "      <td>110.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>-0.974928</td>\n",
       "      <td>False</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.997630</td>\n",
       "      <td>0.068802</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-01-05</th>\n",
       "      <td>130.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>-0.781831</td>\n",
       "      <td>False</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.996298</td>\n",
       "      <td>0.085965</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            target  dayofweek  cos_weekday  sin_weekday is_leap_year  \\\n",
       "2025-01-01   100.0        2.0    -0.222521     0.974928        False   \n",
       "2025-01-02   120.0        3.0    -0.900969     0.433884        False   \n",
       "2025-01-03    90.0        4.0    -0.900969    -0.433884        False   \n",
       "2025-01-04   110.0        5.0    -0.222521    -0.974928        False   \n",
       "2025-01-05   130.0        6.0     0.623490    -0.781831        False   \n",
       "\n",
       "            dayofyear   cos_day   sin_day  quarter    year  ...  \\\n",
       "2025-01-01        1.0  0.999852  0.017213      1.0  2025.0  ...   \n",
       "2025-01-02        2.0  0.999407  0.034422      1.0  2025.0  ...   \n",
       "2025-01-03        3.0  0.998667  0.051620      1.0  2025.0  ...   \n",
       "2025-01-04        4.0  0.997630  0.068802      1.0  2025.0  ...   \n",
       "2025-01-05        5.0  0.996298  0.085965      1.0  2025.0  ...   \n",
       "\n",
       "            rolling_min_7  rolling_max_7  rolling_mean_14  rolling_std_14  \\\n",
       "2025-01-01            NaN            NaN              NaN             NaN   \n",
       "2025-01-02            NaN            NaN              NaN             NaN   \n",
       "2025-01-03            NaN            NaN              NaN             NaN   \n",
       "2025-01-04            NaN            NaN              NaN             NaN   \n",
       "2025-01-05            NaN            NaN              NaN             NaN   \n",
       "\n",
       "           rolling_min_14 rolling_max_14  rolling_mean_30  rolling_std_30  \\\n",
       "2025-01-01            NaN            NaN              NaN             NaN   \n",
       "2025-01-02            NaN            NaN              NaN             NaN   \n",
       "2025-01-03            NaN            NaN              NaN             NaN   \n",
       "2025-01-04            NaN            NaN              NaN             NaN   \n",
       "2025-01-05            NaN            NaN              NaN             NaN   \n",
       "\n",
       "            rolling_min_30  rolling_max_30  \n",
       "2025-01-01             NaN             NaN  \n",
       "2025-01-02             NaN             NaN  \n",
       "2025-01-03             NaN             NaN  \n",
       "2025-01-04             NaN             NaN  \n",
       "2025-01-05             NaN             NaN  \n",
       "\n",
       "[5 rows x 62 columns]"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í–∞–∂–Ω–æ –æ—Ç–º–µ—Ç–∏—Ç—å, —á—Ç–æ –≤ –Ω–∞—á–∞–ª–µ –Ω–∞—à–µ–π –≤—ã–±–æ—Ä–∫–∏ —á–∞—Å—Ç—å –ª–∞–≥–Ω—É—Ç—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å–æ–¥–µ—Ä–∂–∏—Ç `NaN` –≤ —è—á–µ–π–∫–∞—Ö. –≠—Ç–æ —Å–ª—É—á–∏–ª–æ—Å—å –ø–æ—Ç–æ–º—É, —á—Ç–æ —ç—Ç–æ –Ω–∞—á–∞–ª–æ –∏—Å—Ç–æ—Ä–∏–∏ –∏ –ª–∞–≥–∏ –ø—Ä–æ—Å—Ç–æ –Ω–µ–æ—Ç–∫—É–¥–∞ –≤–∑—è—Ç—å. –ü—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ —ç—Ç–∏ —Å—Ç—Ä–æ–∫–∏ –ø—Ä–∏–¥—ë—Ç—Å—è –¥—Ä–æ–ø–Ω—É—Ç—å.\n",
    "\n",
    "–ù–æ —Ç–∞–∫ –∫–∞–∫ —ç—Ç–æ –Ω–∞—á–∞–ª–æ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏, —Ç–æ –Ω–µ —Å—Ç—Ä–∞—à–Ω–æ. –¢–µ–º –±–æ–ª–µ–µ —É–π–¥—ë—Ç –≤—Å–µ–≥–æ –æ–∫–æ–ª–æ 30 —Å—Ç—Ä–æ—á–µ–∫, —á—Ç–æ –º–µ–Ω—å—à–µ 1% –ø—Ä–æ—Ü–µ–Ω—Ç–∞ –Ω–∞—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "_cell_id": "JvRTY3U6kgoJbG80"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96, 62)"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ò—Ç–∞–∫ ... –ª—ë–≥–∫–∏–º –¥–≤–∏–∂–µ–Ω–∏–º —Ä—É–∫–∏ —É –Ω–∞—Å —É–∂–µ 63 –ø—Ä–∏–∑–Ω–∞–∫–∞. –ê –ø–æ–º–Ω–∏—Ç–µ —Å —á–µ–≥–æ –º—ã –Ω–∞—á–∏–Ω–∞–ª–∏? 1 –∫–æ–ª–æ–Ω–∫–∞ —Å –¥–Ω—ë–º –∏ 1 —Ü–µ–ª–µ–≤–∞—è –∫–æ–ª–æ–Ω–∫–∞.  \n",
    "\n",
    "–ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –∏ –±–æ–ª—å—à–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –Ω–æ –ø–æ–∫–∞ –æ—Å—Ç–∞–Ω–æ–≤–∏–º—Å—è –Ω–∞ —ç—Ç–æ–º.\n",
    "\n",
    "–¢—É—Ç –∫–∞–∫ –∏ –≤–µ–∑–¥–µ –≤–∞–∂–Ω–æ **–Ω–µ –ø–µ—Ä–µ–±–æ—Ä—â–∏—Ç—å**. –ö–æ–≥–¥–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –æ—á–µ–Ω—å –º–Ω–æ–≥–æ (–±–æ–ª—å—à–µ 100), —Ç–æ –º–æ–¥–µ–ª—è–º —É–∂–µ —Å–ª–æ–∂–Ω–æ —Ä–∞–∑–¥–µ–ª–∏—Ç—å –∏—Ö –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è —Å–Ω–∏–∂–∞–µ—Ç—Å—è. \n",
    "\n",
    "–ï—Å–ª–∏ –±—ã —É –Ω–∞—Å –±—ã–ª –≤—Å–µ–≥–æ 1 —Ä—è–¥, –º—ã –±—ã –ø–æ–ª—É—á–µ–Ω–Ω—É—é —Ç–∞–±–ª–∏—Ü—É —Ä–∞–∑–¥–µ–ª–∏–ª–∏ –æ–±—Ä–∞—Ç–Ω–æ –Ω–∞ –¥–≤–µ –≤—ã–±–æ—Ä–∫–∏ –∏ –æ–±—É—á–∏–ª–∏ –±—ã 1 –º–æ–¥–µ–ª—å. \n",
    "\n",
    "–ù–æ —Ç–∞–∫ –∫–∞–∫ —É –Ω–∞—Å –µ—Å—Ç—å –µ—â—ë 87 —Ä—è–¥–æ–≤, —Ç–æ –¥–∞–≤–∞–π—Ç–µ –Ω–∞–ø–∏—à–µ–º —Ü–∏–∫–ª, –≤ –∫–æ—Ç–æ—Ä–æ–º –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ä—è–¥–∞ –ø–æ–≤—Ç–æ—Ä–∏–º —ç—Ç–∏ —à–∞–≥–∏, –æ–±—É—á–∏–º –º–æ–¥–µ–ª—å –∏ –ø–æ–ª—É—á–∏–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "_cell_id": "w4pox5yQPNNi3X1u"
   },
   "outputs": [],
   "source": [
    "# —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π, —á—Ç–æ–±—ã —Ñ–æ—Ä–º–∞—Ç —Ç–∞–±–ª–∏—Ü –±—ã–ª —Ç–∞–∫–æ–π –∂–µ –∫–∞–∫ —É ETNA \n",
    "def create_preds_df(y_test, y_pred, country, store, product):\n",
    "    y_test = y_test.rename(columns={\"target\": \"y_test\"})\n",
    "\n",
    "    y_test[\"y_pred\"] = y_pred\n",
    "    # –æ–∫—Ä—É–≥–ª—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è, –ø—ã—à–∫–∏ –ø–æ —á–∞—Å—Ç—è–º –º—ã –Ω–µ –ø—Ä–æ–¥–∞—ë–º\n",
    "    y_test[\"y_pred\"] = np.ceil(y_test[\"y_pred\"])\n",
    "\n",
    "    y_test[\"country\"] = country\n",
    "    y_test[\"store\"] = store\n",
    "    y_test[\"product\"] = product\n",
    "    preds_df = y_test.reset_index()\n",
    "    preds_df = preds_df[[\"timestamp\", \"y_test\", \"y_pred\", \"country\", \"store\", \"product\"]]\n",
    "\n",
    "    return preds_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°–æ–±–∏—Ä–∞–µ–º –≤—Å—ë –≤ –±–æ–ª—å—à—É—é –∏ –∫—Ä–∞—Å–∏–≤—É—é –∫—É—á–∫—É."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "_cell_id": "bYR8FTZ6pN4UAC3k"
   },
   "outputs": [],
   "source": [
    "def run_pipeline(model) -> pd.DataFrame:\n",
    "    # –≤—ã–¥–µ–ª—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å—Ç—Ä–∞–Ω—ã –≤ –¥–∞–Ω–Ω—ã—Ö \n",
    "    countries = df[\"country\"].unique()\n",
    "\n",
    "    # –≤—ã–¥–µ–ª—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –ø—ã—à–µ—á–Ω—ã–µ –≤ –¥–∞–Ω–Ω—ã—Ö\n",
    "    stores = df[\"store\"].unique()\n",
    "\n",
    "    # –≤—ã–¥–µ–ª—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –≤–∫—É—Å—ã –ø—ã—à–µ–∫ –≤ –¥–∞–Ω–Ω—ã—Ö\n",
    "    products = df[\"product\"].unique()\n",
    "\n",
    "    # –∫–æ–ª-–≤–æ –∫–æ–º–±–∏–Ω–∞—Ü–∏–π, —á–∏—Å—Ç–æ –¥–ª—è –∫—Ä–∞—Å–∏–≤—ã—Ö –ª–æ–≥–æ–≤ \n",
    "    iter_n = len(countries) * len(stores) * len(products)\n",
    "\n",
    "    missing_predictions = []\n",
    "    predictions = []\n",
    "\n",
    "    iter_counter = 1\n",
    "\n",
    "    # –∏—Ç–µ—Ä–∏—Ä—É–µ–º—Å—è –ø–æ –≤—Å–µ–º –∫–æ–º–±–∏–Ω–∞—Ü–∏—è–º —Å—Ç—Ä–∞–Ω–∞ - –ø—ã—à–µ—á–Ω–∞—è - –≤–∫—É—Å \n",
    "    for country in countries:\n",
    "        for store in stores:\n",
    "            for product in products:\n",
    "                print(f\"{iter_counter}/{iter_n} {country!r} - {store!r} - {product!r} processing ...\")\n",
    "\n",
    "                # –≤—ã–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ 1 –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Ä—è–¥\n",
    "                sample_df = df[\n",
    "                    (df[\"country\"] == country) \n",
    "                    & (df[\"store\"] == store)\n",
    "                    & (df[\"product\"] == product)\n",
    "                ]\n",
    "                nan_target_index = sample_df[sample_df[\"target\"].isna()].index\n",
    "                sample_df = sample_df[[\"target\"]].reset_index()\n",
    "\n",
    "                if sample_df.empty:\n",
    "                    missing_predictions.append((country, store, product))\n",
    "                    continue\n",
    "                \n",
    "                sample_df = sample_df.resample(\"1D\", on=\"timestamp\").sum()\n",
    "                sample_df.index = pd.to_datetime(sample_df.index)\n",
    "\n",
    "                # –≥–µ–Ω–µ—Ä–∏–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
    "                sample_df = create_dtime_features(sample_df)\n",
    "\n",
    "                # –≥–µ–Ω–µ—Ä–∏–º –ø—Ä–∞–∑–¥–Ω–∏—á–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ \n",
    "                sample_holidays_df = get_holiday_df(country)\n",
    "                sample_df = create_holiday_features(sample_df, sample_holidays_df)\n",
    "                \n",
    "                # –≥–µ–Ω–µ—Ä–∏–º –ª–∞–≥–Ω—É—Ç—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ \n",
    "                sample_df = create_lag_features(sample_df, horizon=HORIZON)\n",
    "\n",
    "                # –¥–µ–ª–∏–º –¥–∞–Ω–Ω—ã–µ –æ–±—Ä–∞—Ç–Ω–æ –Ω–∞ 2 –≤—ã–±–æ—Ä–∫–∏: train –∏ test\n",
    "\n",
    "                # –æ–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞\n",
    "                sample_train_df = sample_df.loc[: SPLIT_DATE - timedelta(days=1)]\n",
    "                # –¥—Ä–æ–ø–Ω–µ–º —Å—Ç—Ä–æ–∫–∏, –≥–¥–µ –µ—Å—Ç—å –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è \n",
    "                sample_train_df = sample_train_df.dropna()\n",
    "\n",
    "                # —Ç–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞\n",
    "                sample_test_df = sample_df.loc[SPLIT_DATE:]\n",
    "\n",
    "                # —Ä–∞–∑–¥–µ–ª—è–µ–º –≤—ã–±–æ—Ä–∫–∏ –Ω–∞ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏ —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é\n",
    "                X_train = sample_train_df.drop(columns=[\"target\"])\n",
    "                y_train = sample_train_df[\"target\"]\n",
    "\n",
    "                X_test = sample_test_df.drop(columns=[\"target\"])\n",
    "                # X_test = X_test.fillna(method=\"bfill\").fillna(method=\"ffill\")\n",
    "\n",
    "                y_test = sample_test_df[[\"target\"]]\n",
    "                nan_y_test_index = set(y_test.index) & set(nan_target_index)\n",
    "                y_test.loc[nan_y_test_index, \"target\"] = None\n",
    "\n",
    "                iter_counter += 1\n",
    "\n",
    "                # –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π –ø—Ä–æ–≤–µ—Ä—è–µ–º –¥–∞–Ω–Ω—ã–µ –Ω–∞ \"–ø—É—Å—Ç–æ—Ç—É\"\n",
    "                if X_train.empty or X_test.empty:\n",
    "                    missing_predictions.append((country, store, product))\n",
    "                    continue\n",
    "\n",
    "                # –æ–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å\n",
    "                model.fit(X_train, y_train)\n",
    "                # –ø–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
    "                y_pred = model.predict(X_test)\n",
    "\n",
    "                # —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤ —É–¥–æ–±–Ω—ã–π –Ω–∞–º —Ñ–æ—Ä–º–∞—Ç\n",
    "                preds_df = create_preds_df(y_test, y_pred, country, store, product)\n",
    "                predictions.append(preds_df)\n",
    "\n",
    "    # –æ–±—ä–µ–¥–∏–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –æ—Ç–æ –≤—Å–µ—Ö —Ä—è–¥–æ–≤ –≤ –µ–¥–∏–Ω—É—é —Ç–∞–±–ª–∏—Ü—É\n",
    "    predictions_df = pd.concat(predictions)\n",
    "    return predictions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í—Å–µ –ø—Ä–∏–≥–æ—Ç–æ–≤–ª–µ–Ω–∏—è —Å–¥–µ–ª–∞–Ω—ã, –ø–æ–≥–Ω–∞–ª–∏ —É—á–∏—Ç—å –º–æ–¥–µ–ª—å–∫–∏! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –õ–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–µ—Ä–≤–æ–π –±—É–¥–µ—Ç —Å—Ç–∞—Ä–∞—è –¥–æ–±—Ä–∞—è –º–æ–¥–µ–ª—å –ª–∏–Ω–µ–π–Ω–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "_cell_id": "lDWrpfZbnQsTc5x8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/90 'Canada' - 'Donots' - 'Chocolate' processing ...\n",
      "2/90 'Canada' - 'Donots' - 'Coffee' processing ...\n",
      "3/90 'Canada' - 'Donots' - 'Cream' processing ...\n",
      "4/90 'Canada' - 'Donots' - 'Strawberry Jam' processing ...\n",
      "5/90 'Canada' - 'Donots' - 'Sugar Powder' processing ...\n",
      "6/90 'Canada' - 'Fluffy Bounce' - 'Chocolate' processing ...\n",
      "6/90 'Canada' - 'Fluffy Bounce' - 'Coffee' processing ...\n",
      "7/90 'Canada' - 'Fluffy Bounce' - 'Cream' processing ...\n",
      "8/90 'Canada' - 'Fluffy Bounce' - 'Strawberry Jam' processing ...\n",
      "9/90 'Canada' - 'Fluffy Bounce' - 'Sugar Powder' processing ...\n",
      "10/90 'Canada' - 'Pyshka' - 'Chocolate' processing ...\n",
      "11/90 'Canada' - 'Pyshka' - 'Coffee' processing ...\n",
      "12/90 'Canada' - 'Pyshka' - 'Cream' processing ...\n",
      "13/90 'Canada' - 'Pyshka' - 'Strawberry Jam' processing ...\n",
      "14/90 'Canada' - 'Pyshka' - 'Sugar Powder' processing ...\n",
      "15/90 'Finland' - 'Donots' - 'Chocolate' processing ...\n",
      "16/90 'Finland' - 'Donots' - 'Coffee' processing ...\n",
      "17/90 'Finland' - 'Donots' - 'Cream' processing ...\n",
      "18/90 'Finland' - 'Donots' - 'Strawberry Jam' processing ...\n",
      "19/90 'Finland' - 'Donots' - 'Sugar Powder' processing ...\n",
      "20/90 'Finland' - 'Fluffy Bounce' - 'Chocolate' processing ...\n",
      "21/90 'Finland' - 'Fluffy Bounce' - 'Coffee' processing ...\n",
      "22/90 'Finland' - 'Fluffy Bounce' - 'Cream' processing ...\n",
      "23/90 'Finland' - 'Fluffy Bounce' - 'Strawberry Jam' processing ...\n",
      "24/90 'Finland' - 'Fluffy Bounce' - 'Sugar Powder' processing ...\n",
      "25/90 'Finland' - 'Pyshka' - 'Chocolate' processing ...\n",
      "26/90 'Finland' - 'Pyshka' - 'Coffee' processing ...\n",
      "27/90 'Finland' - 'Pyshka' - 'Cream' processing ...\n",
      "28/90 'Finland' - 'Pyshka' - 'Strawberry Jam' processing ...\n",
      "29/90 'Finland' - 'Pyshka' - 'Sugar Powder' processing ...\n",
      "30/90 'Italy' - 'Donots' - 'Chocolate' processing ...\n",
      "31/90 'Italy' - 'Donots' - 'Coffee' processing ...\n",
      "32/90 'Italy' - 'Donots' - 'Cream' processing ...\n",
      "33/90 'Italy' - 'Donots' - 'Strawberry Jam' processing ...\n",
      "34/90 'Italy' - 'Donots' - 'Sugar Powder' processing ...\n",
      "35/90 'Italy' - 'Fluffy Bounce' - 'Chocolate' processing ...\n",
      "36/90 'Italy' - 'Fluffy Bounce' - 'Coffee' processing ...\n",
      "37/90 'Italy' - 'Fluffy Bounce' - 'Cream' processing ...\n",
      "38/90 'Italy' - 'Fluffy Bounce' - 'Strawberry Jam' processing ...\n",
      "39/90 'Italy' - 'Fluffy Bounce' - 'Sugar Powder' processing ...\n",
      "40/90 'Italy' - 'Pyshka' - 'Chocolate' processing ...\n",
      "41/90 'Italy' - 'Pyshka' - 'Coffee' processing ...\n",
      "42/90 'Italy' - 'Pyshka' - 'Cream' processing ...\n",
      "43/90 'Italy' - 'Pyshka' - 'Strawberry Jam' processing ...\n",
      "44/90 'Italy' - 'Pyshka' - 'Sugar Powder' processing ...\n",
      "45/90 'Kenya' - 'Donots' - 'Chocolate' processing ...\n",
      "46/90 'Kenya' - 'Donots' - 'Coffee' processing ...\n",
      "47/90 'Kenya' - 'Donots' - 'Cream' processing ...\n",
      "48/90 'Kenya' - 'Donots' - 'Strawberry Jam' processing ...\n",
      "49/90 'Kenya' - 'Donots' - 'Sugar Powder' processing ...\n",
      "50/90 'Kenya' - 'Fluffy Bounce' - 'Chocolate' processing ...\n",
      "50/90 'Kenya' - 'Fluffy Bounce' - 'Coffee' processing ...\n",
      "51/90 'Kenya' - 'Fluffy Bounce' - 'Cream' processing ...\n",
      "52/90 'Kenya' - 'Fluffy Bounce' - 'Strawberry Jam' processing ...\n",
      "53/90 'Kenya' - 'Fluffy Bounce' - 'Sugar Powder' processing ...\n",
      "54/90 'Kenya' - 'Pyshka' - 'Chocolate' processing ...\n",
      "55/90 'Kenya' - 'Pyshka' - 'Coffee' processing ...\n",
      "56/90 'Kenya' - 'Pyshka' - 'Cream' processing ...\n",
      "57/90 'Kenya' - 'Pyshka' - 'Strawberry Jam' processing ...\n",
      "58/90 'Kenya' - 'Pyshka' - 'Sugar Powder' processing ...\n",
      "59/90 'Norway' - 'Donots' - 'Chocolate' processing ...\n",
      "60/90 'Norway' - 'Donots' - 'Coffee' processing ...\n",
      "61/90 'Norway' - 'Donots' - 'Cream' processing ...\n",
      "62/90 'Norway' - 'Donots' - 'Strawberry Jam' processing ...\n",
      "63/90 'Norway' - 'Donots' - 'Sugar Powder' processing ...\n",
      "64/90 'Norway' - 'Fluffy Bounce' - 'Chocolate' processing ...\n",
      "65/90 'Norway' - 'Fluffy Bounce' - 'Coffee' processing ...\n",
      "66/90 'Norway' - 'Fluffy Bounce' - 'Cream' processing ...\n",
      "67/90 'Norway' - 'Fluffy Bounce' - 'Strawberry Jam' processing ...\n",
      "68/90 'Norway' - 'Fluffy Bounce' - 'Sugar Powder' processing ...\n",
      "69/90 'Norway' - 'Pyshka' - 'Chocolate' processing ...\n",
      "70/90 'Norway' - 'Pyshka' - 'Coffee' processing ...\n",
      "71/90 'Norway' - 'Pyshka' - 'Cream' processing ...\n",
      "72/90 'Norway' - 'Pyshka' - 'Strawberry Jam' processing ...\n",
      "73/90 'Norway' - 'Pyshka' - 'Sugar Powder' processing ...\n",
      "74/90 'Singapore' - 'Donots' - 'Chocolate' processing ...\n",
      "75/90 'Singapore' - 'Donots' - 'Coffee' processing ...\n",
      "76/90 'Singapore' - 'Donots' - 'Cream' processing ...\n",
      "77/90 'Singapore' - 'Donots' - 'Strawberry Jam' processing ...\n",
      "78/90 'Singapore' - 'Donots' - 'Sugar Powder' processing ...\n",
      "79/90 'Singapore' - 'Fluffy Bounce' - 'Chocolate' processing ...\n",
      "80/90 'Singapore' - 'Fluffy Bounce' - 'Coffee' processing ...\n",
      "81/90 'Singapore' - 'Fluffy Bounce' - 'Cream' processing ...\n",
      "82/90 'Singapore' - 'Fluffy Bounce' - 'Strawberry Jam' processing ...\n",
      "83/90 'Singapore' - 'Fluffy Bounce' - 'Sugar Powder' processing ...\n",
      "84/90 'Singapore' - 'Pyshka' - 'Chocolate' processing ...\n",
      "85/90 'Singapore' - 'Pyshka' - 'Coffee' processing ...\n",
      "86/90 'Singapore' - 'Pyshka' - 'Cream' processing ...\n",
      "87/90 'Singapore' - 'Pyshka' - 'Strawberry Jam' processing ...\n",
      "88/90 'Singapore' - 'Pyshka' - 'Sugar Powder' processing ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>y_test</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>country</th>\n",
       "      <th>store</th>\n",
       "      <th>product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>220.0</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Donots</td>\n",
       "      <td>Chocolate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-12-02</td>\n",
       "      <td>212.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Donots</td>\n",
       "      <td>Chocolate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-12-03</td>\n",
       "      <td>210.0</td>\n",
       "      <td>208.0</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Donots</td>\n",
       "      <td>Chocolate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-12-04</td>\n",
       "      <td>231.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Donots</td>\n",
       "      <td>Chocolate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-12-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>228.0</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Donots</td>\n",
       "      <td>Chocolate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   timestamp  y_test  y_pred country   store    product\n",
       "0 2016-12-01     NaN   220.0  Canada  Donots  Chocolate\n",
       "1 2016-12-02   212.0   173.0  Canada  Donots  Chocolate\n",
       "2 2016-12-03   210.0   208.0  Canada  Donots  Chocolate\n",
       "3 2016-12-04   231.0   245.0  Canada  Donots  Chocolate\n",
       "4 2016-12-05     NaN   228.0  Canada  Donots  Chocolate"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linreg_model = LinearRegression()\n",
    "linreg_predictions = run_pipeline(linreg_model)\n",
    "\n",
    "linreg_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "_cell_id": "KwaNMN2Xv4t8MOwo"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2728, 6)"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linreg_predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢–∞–∫–∂–µ –∫–∞–∫ —Å –º–æ–¥–µ–ª—å—é ETNA –æ—Ç—Ä–∏—Å—É–µ–º –Ω–∞ –≥—Ä–∞—Ñ–∏–∫–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ü–µ–ª–µ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—å—é."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "_cell_id": "1YEdqMWugyI8levc"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83d4f1f22c8f42b0adf9bf8163b73331",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='country', options=('Canada', 'Finland', 'Italy', 'Kenya', 'Norway'‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@widgets.interact(\n",
    "    country=widgets.Dropdown(options=linreg_predictions[\"country\"].unique()),\n",
    "    store=widgets.Dropdown(options=linreg_predictions[\"store\"].unique()),\n",
    "    products=widgets.Dropdown(options=linreg_predictions[\"product\"].unique()),\n",
    "    start_date=widgets.DatePicker(value=linreg_predictions[\"timestamp\"].min()),\n",
    ")\n",
    "def show_linreg_predictions(country: str, store: str, products: str, start_date: date):  # noqa: D103\n",
    "    start_date = pd.Timestamp(start_date)\n",
    "    plot_df = linreg_predictions[\n",
    "        (linreg_predictions[\"country\"] == country)\n",
    "        & (linreg_predictions[\"store\"] == store)\n",
    "        & (linreg_predictions[\"product\"] == products)\n",
    "        & (linreg_predictions[\"timestamp\"].dt.date >= start_date.date())\n",
    "    ]\n",
    "\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=1, figsize=(15, 4))\n",
    "\n",
    "    axs.plot(plot_df[\"timestamp\"], plot_df[\"y_pred\"], marker=\"o\", label=\"pred\")\n",
    "    # —É–±–µ—Ä—ë–º –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è, —á—Ç–æ–±—ã —Ç–æ—á–∫–∏ —Å–æ–µ–¥–∏–Ω–∏–ª–∏—Å—å –º–µ–∂–¥—É —Å–æ–±–æ–π - —á–∏—Å—Ç–æ –¥–ª—è –∫—Ä–∞—Å–æ—Ç—ã\n",
    "    plot_df = plot_df.dropna()\n",
    "    axs.plot(plot_df[\"timestamp\"], plot_df[\"y_test\"], marker=\"o\", label=\"true\")\n",
    "    axs.set_xlabel(\"–î–∞—Ç–∞\")\n",
    "    axs.set_ylabel(\"–ö–æ–ª-–≤–æ –ø—Ä–æ–¥–∞–Ω–Ω—ã—Ö –ø—ã—à–µ–∫\")\n",
    "    axs.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ò —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–∞–π–ª, —á—Ç–æ–±—ã –Ω–µ –ø–æ—Ç–µ—Ä—è—Ç—å."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "_cell_id": "0pFIP4bxZOrZYfV2"
   },
   "outputs": [],
   "source": [
    "pred_dpath = Path(\"forecast_predictions\")\n",
    "pred_dpath.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pred_fpath = pred_dpath / \"linear_regression_predictions.csv\"\n",
    "linreg_predictions.to_csv(pred_fpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGBM : –ë—É—Å—Ç–∏–Ω–≥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –æ–±—É—á–∏–º –º–æ–¥–µ–ª—å –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ –±—É—Å—Ç–∏–Ω–≥–∞."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "_cell_id": "3syPU8Y7N2LfAeGa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/90 'Canada' - 'Donots' - 'Chocolate' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001280 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11460\n",
      "[LightGBM] [Info] Number of data points in the train set: 2495, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 223.419258\n",
      "2/90 'Canada' - 'Donots' - 'Coffee' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001066 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11464\n",
      "[LightGBM] [Info] Number of data points in the train set: 2495, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 655.612425\n",
      "3/90 'Canada' - 'Donots' - 'Cream' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000952 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11474\n",
      "[LightGBM] [Info] Number of data points in the train set: 2495, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 1450.294990\n",
      "4/90 'Canada' - 'Donots' - 'Strawberry Jam' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000826 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11496\n",
      "[LightGBM] [Info] Number of data points in the train set: 2495, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 763.559118\n",
      "5/90 'Canada' - 'Donots' - 'Sugar Powder' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000895 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11508\n",
      "[LightGBM] [Info] Number of data points in the train set: 2495, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 1199.611623\n",
      "6/90 'Canada' - 'Fluffy Bounce' - 'Chocolate' processing ...\n",
      "6/90 'Canada' - 'Fluffy Bounce' - 'Coffee' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000984 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10974\n",
      "[LightGBM] [Info] Number of data points in the train set: 2495, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 323.811677\n",
      "7/90 'Canada' - 'Fluffy Bounce' - 'Cream' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000916 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11318\n",
      "[LightGBM] [Info] Number of data points in the train set: 2495, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 716.215230\n",
      "8/90 'Canada' - 'Fluffy Bounce' - 'Strawberry Jam' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001114 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11284\n",
      "[LightGBM] [Info] Number of data points in the train set: 2495, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 378.063327\n",
      "9/90 'Canada' - 'Fluffy Bounce' - 'Sugar Powder' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000985 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11261\n",
      "[LightGBM] [Info] Number of data points in the train set: 2495, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 592.812024\n",
      "10/90 'Canada' - 'Pyshka' - 'Chocolate' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001034 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10903\n",
      "[LightGBM] [Info] Number of data points in the train set: 2495, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 243.734051\n",
      "11/90 'Canada' - 'Pyshka' - 'Coffee' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001156 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11482\n",
      "[LightGBM] [Info] Number of data points in the train set: 2495, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 773.450501\n",
      "12/90 'Canada' - 'Pyshka' - 'Cream' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000977 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11505\n",
      "[LightGBM] [Info] Number of data points in the train set: 2495, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 1713.759920\n",
      "13/90 'Canada' - 'Pyshka' - 'Strawberry Jam' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000971 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11561\n",
      "[LightGBM] [Info] Number of data points in the train set: 2495, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 904.809218\n",
      "14/90 'Canada' - 'Pyshka' - 'Sugar Powder' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001019 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11514\n",
      "[LightGBM] [Info] Number of data points in the train set: 2495, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 1418.996794\n",
      "15/90 'Finland' - 'Donots' - 'Chocolate' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000852 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7562\n",
      "[LightGBM] [Info] Number of data points in the train set: 2495, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 198.335471\n",
      "16/90 'Finland' - 'Donots' - 'Coffee' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000972 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11400\n",
      "[LightGBM] [Info] Number of data points in the train set: 2495, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 642.069339\n",
      "17/90 'Finland' - 'Donots' - 'Cream' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001013 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11467\n",
      "[LightGBM] [Info] Number of data points in the train set: 2495, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 1424.219639\n",
      "18/90 'Finland' - 'Donots' - 'Strawberry Jam' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000966 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11515\n",
      "[LightGBM] [Info] Number of data points in the train set: 2495, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 751.976754\n",
      "19/90 'Finland' - 'Donots' - 'Sugar Powder' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000866 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11456\n",
      "[LightGBM] [Info] Number of data points in the train set: 2495, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 1175.059319\n",
      "20/90 'Finland' - 'Fluffy Bounce' - 'Chocolate' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000611 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5353\n",
      "[LightGBM] [Info] Number of data points in the train set: 2495, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 97.711423\n",
      "21/90 'Finland' - 'Fluffy Bounce' - 'Coffee' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000856 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10649\n",
      "[LightGBM] [Info] Number of data points in the train set: 2495, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 317.241283\n",
      "22/90 'Finland' - 'Fluffy Bounce' - 'Cream' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000914 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11261\n",
      "[LightGBM] [Info] Number of data points in the train set: 2495, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 703.318236\n",
      "23/90 'Finland' - 'Fluffy Bounce' - 'Strawberry Jam' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001029 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11300\n",
      "[LightGBM] [Info] Number of data points in the train set: 2495, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 371.974749\n",
      "24/90 'Finland' - 'Fluffy Bounce' - 'Sugar Powder' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000904 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11199\n",
      "[LightGBM] [Info] Number of data points in the train set: 2495, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 581.398798\n",
      "25/90 'Finland' - 'Pyshka' - 'Chocolate' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000888 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8339\n",
      "[LightGBM] [Info] Number of data points in the train set: 2495, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 234.484569\n",
      "26/90 'Finland' - 'Pyshka' - 'Coffee' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000955 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11524\n",
      "[LightGBM] [Info] Number of data points in the train set: 2495, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 758.547094\n",
      "27/90 'Finland' - 'Pyshka' - 'Cream' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000961 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11491\n",
      "[LightGBM] [Info] Number of data points in the train set: 2495, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 1682.818838\n",
      "28/90 'Finland' - 'Pyshka' - 'Strawberry Jam' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001042 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11539\n",
      "[LightGBM] [Info] Number of data points in the train set: 2495, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 890.837675\n",
      "29/90 'Finland' - 'Pyshka' - 'Sugar Powder' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000930 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11453\n",
      "[LightGBM] [Info] Number of data points in the train set: 2495, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 1387.829259\n",
      "30/90 'Italy' - 'Donots' - 'Chocolate' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000713 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6504\n",
      "[LightGBM] [Info] Number of data points in the train set: 2496, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 143.469952\n",
      "31/90 'Italy' - 'Donots' - 'Coffee' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001145 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11347\n",
      "[LightGBM] [Info] Number of data points in the train set: 2496, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 465.557692\n",
      "32/90 'Italy' - 'Donots' - 'Cream' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000945 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11369\n",
      "[LightGBM] [Info] Number of data points in the train set: 2496, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 1031.300080\n",
      "33/90 'Italy' - 'Donots' - 'Strawberry Jam' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000938 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11456\n",
      "[LightGBM] [Info] Number of data points in the train set: 2496, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 544.416266\n",
      "34/90 'Italy' - 'Donots' - 'Sugar Powder' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000835 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11376\n",
      "[LightGBM] [Info] Number of data points in the train set: 2496, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 851.569311\n",
      "35/90 'Italy' - 'Fluffy Bounce' - 'Chocolate' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000623 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4760\n",
      "[LightGBM] [Info] Number of data points in the train set: 2496, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 70.579728\n",
      "36/90 'Italy' - 'Fluffy Bounce' - 'Coffee' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000959 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8666\n",
      "[LightGBM] [Info] Number of data points in the train set: 2496, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 229.924279\n",
      "37/90 'Italy' - 'Fluffy Bounce' - 'Cream' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001030 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11141\n",
      "[LightGBM] [Info] Number of data points in the train set: 2496, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 509.451122\n",
      "38/90 'Italy' - 'Fluffy Bounce' - 'Strawberry Jam' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000908 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10766\n",
      "[LightGBM] [Info] Number of data points in the train set: 2496, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 268.834936\n",
      "39/90 'Italy' - 'Fluffy Bounce' - 'Sugar Powder' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001066 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10934\n",
      "[LightGBM] [Info] Number of data points in the train set: 2496, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 421.511218\n",
      "40/90 'Italy' - 'Pyshka' - 'Chocolate' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000800 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7125\n",
      "[LightGBM] [Info] Number of data points in the train set: 2496, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 169.731170\n",
      "41/90 'Italy' - 'Pyshka' - 'Coffee' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000911 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11409\n",
      "[LightGBM] [Info] Number of data points in the train set: 2496, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 550.173478\n",
      "42/90 'Italy' - 'Pyshka' - 'Cream' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001018 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11473\n",
      "[LightGBM] [Info] Number of data points in the train set: 2496, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 1219.608974\n",
      "43/90 'Italy' - 'Pyshka' - 'Strawberry Jam' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001013 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11483\n",
      "[LightGBM] [Info] Number of data points in the train set: 2496, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 644.127804\n",
      "44/90 'Italy' - 'Pyshka' - 'Sugar Powder' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000949 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11435\n",
      "[LightGBM] [Info] Number of data points in the train set: 2496, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 1007.698718\n",
      "45/90 'Kenya' - 'Donots' - 'Chocolate' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000912 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11489\n",
      "[LightGBM] [Info] Number of data points in the train set: 2495, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 5.229682\n",
      "46/90 'Kenya' - 'Donots' - 'Coffee' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000529 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2987\n",
      "[LightGBM] [Info] Number of data points in the train set: 2495, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 15.619639\n",
      "47/90 'Kenya' - 'Donots' - 'Cream' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000614 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3628\n",
      "[LightGBM] [Info] Number of data points in the train set: 2495, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 35.227655\n",
      "48/90 'Kenya' - 'Donots' - 'Strawberry Jam' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000545 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3158\n",
      "[LightGBM] [Info] Number of data points in the train set: 2495, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 18.392786\n",
      "49/90 'Kenya' - 'Donots' - 'Sugar Powder' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000563 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3434\n",
      "[LightGBM] [Info] Number of data points in the train set: 2495, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 29.002806\n",
      "50/90 'Kenya' - 'Fluffy Bounce' - 'Chocolate' processing ...\n",
      "50/90 'Kenya' - 'Fluffy Bounce' - 'Coffee' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000487 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3161\n",
      "[LightGBM] [Info] Number of data points in the train set: 2495, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 7.500511\n",
      "51/90 'Kenya' - 'Fluffy Bounce' - 'Cream' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000591 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2800\n",
      "[LightGBM] [Info] Number of data points in the train set: 2495, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 17.127856\n",
      "52/90 'Kenya' - 'Fluffy Bounce' - 'Strawberry Jam' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000588 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2585\n",
      "[LightGBM] [Info] Number of data points in the train set: 2495, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 8.808564\n",
      "53/90 'Kenya' - 'Fluffy Bounce' - 'Sugar Powder' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000426 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2723\n",
      "[LightGBM] [Info] Number of data points in the train set: 2495, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 14.097796\n",
      "54/90 'Kenya' - 'Pyshka' - 'Chocolate' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001016 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10706\n",
      "[LightGBM] [Info] Number of data points in the train set: 2495, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 5.701106\n",
      "55/90 'Kenya' - 'Pyshka' - 'Coffee' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000454 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3133\n",
      "[LightGBM] [Info] Number of data points in the train set: 2495, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 18.570741\n",
      "56/90 'Kenya' - 'Pyshka' - 'Cream' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000635 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3839\n",
      "[LightGBM] [Info] Number of data points in the train set: 2495, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 41.658116\n",
      "57/90 'Kenya' - 'Pyshka' - 'Strawberry Jam' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000573 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3349\n",
      "[LightGBM] [Info] Number of data points in the train set: 2495, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 21.803206\n",
      "58/90 'Kenya' - 'Pyshka' - 'Sugar Powder' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000355 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3605\n",
      "[LightGBM] [Info] Number of data points in the train set: 2495, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 34.433667\n",
      "59/90 'Norway' - 'Donots' - 'Chocolate' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001042 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11284\n",
      "[LightGBM] [Info] Number of data points in the train set: 2496, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 378.623798\n",
      "60/90 'Norway' - 'Donots' - 'Coffee' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001012 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11582\n",
      "[LightGBM] [Info] Number of data points in the train set: 2496, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 1226.609375\n",
      "61/90 'Norway' - 'Donots' - 'Cream' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001074 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11594\n",
      "[LightGBM] [Info] Number of data points in the train set: 2496, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 2711.946715\n",
      "62/90 'Norway' - 'Donots' - 'Strawberry Jam' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000903 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11648\n",
      "[LightGBM] [Info] Number of data points in the train set: 2496, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 1436.268830\n",
      "63/90 'Norway' - 'Donots' - 'Sugar Powder' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000863 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11592\n",
      "[LightGBM] [Info] Number of data points in the train set: 2496, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 2241.607372\n",
      "64/90 'Norway' - 'Fluffy Bounce' - 'Chocolate' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000989 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8158\n",
      "[LightGBM] [Info] Number of data points in the train set: 2496, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 186.622997\n",
      "65/90 'Norway' - 'Fluffy Bounce' - 'Coffee' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000922 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11461\n",
      "[LightGBM] [Info] Number of data points in the train set: 2496, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 604.735978\n",
      "66/90 'Norway' - 'Fluffy Bounce' - 'Cream' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000897 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11501\n",
      "[LightGBM] [Info] Number of data points in the train set: 2496, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 1342.220353\n",
      "67/90 'Norway' - 'Fluffy Bounce' - 'Strawberry Jam' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001168 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11540\n",
      "[LightGBM] [Info] Number of data points in the train set: 2496, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 709.036458\n",
      "68/90 'Norway' - 'Fluffy Bounce' - 'Sugar Powder' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000881 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11516\n",
      "[LightGBM] [Info] Number of data points in the train set: 2496, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 1107.911058\n",
      "69/90 'Norway' - 'Pyshka' - 'Chocolate' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001013 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11355\n",
      "[LightGBM] [Info] Number of data points in the train set: 2496, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 446.913462\n",
      "70/90 'Norway' - 'Pyshka' - 'Coffee' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000948 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11595\n",
      "[LightGBM] [Info] Number of data points in the train set: 2496, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 1445.232372\n",
      "71/90 'Norway' - 'Pyshka' - 'Cream' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000949 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11603\n",
      "[LightGBM] [Info] Number of data points in the train set: 2496, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 3205.653846\n",
      "72/90 'Norway' - 'Pyshka' - 'Strawberry Jam' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001040 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11640\n",
      "[LightGBM] [Info] Number of data points in the train set: 2496, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 1693.763221\n",
      "73/90 'Norway' - 'Pyshka' - 'Sugar Powder' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000960 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11588\n",
      "[LightGBM] [Info] Number of data points in the train set: 2496, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 2648.067708\n",
      "74/90 'Singapore' - 'Donots' - 'Chocolate' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000960 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8408\n",
      "[LightGBM] [Info] Number of data points in the train set: 2495, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 228.888978\n",
      "75/90 'Singapore' - 'Donots' - 'Coffee' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000987 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11470\n",
      "[LightGBM] [Info] Number of data points in the train set: 2495, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 741.565932\n",
      "76/90 'Singapore' - 'Donots' - 'Cream' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000969 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11472\n",
      "[LightGBM] [Info] Number of data points in the train set: 2495, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 1643.381162\n",
      "77/90 'Singapore' - 'Donots' - 'Strawberry Jam' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000961 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11526\n",
      "[LightGBM] [Info] Number of data points in the train set: 2495, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 867.420040\n",
      "78/90 'Singapore' - 'Donots' - 'Sugar Powder' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001025 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11467\n",
      "[LightGBM] [Info] Number of data points in the train set: 2495, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 1357.215230\n",
      "79/90 'Singapore' - 'Fluffy Bounce' - 'Chocolate' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000560 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5770\n",
      "[LightGBM] [Info] Number of data points in the train set: 2495, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 112.810822\n",
      "80/90 'Singapore' - 'Fluffy Bounce' - 'Coffee' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000893 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11192\n",
      "[LightGBM] [Info] Number of data points in the train set: 2495, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 366.355511\n",
      "81/90 'Singapore' - 'Fluffy Bounce' - 'Cream' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001068 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11256\n",
      "[LightGBM] [Info] Number of data points in the train set: 2495, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 814.090581\n",
      "82/90 'Singapore' - 'Fluffy Bounce' - 'Strawberry Jam' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000895 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11367\n",
      "[LightGBM] [Info] Number of data points in the train set: 2495, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 428.290982\n",
      "83/90 'Singapore' - 'Fluffy Bounce' - 'Sugar Powder' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000862 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11285\n",
      "[LightGBM] [Info] Number of data points in the train set: 2495, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 671.574749\n",
      "84/90 'Singapore' - 'Pyshka' - 'Chocolate' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000751 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9022\n",
      "[LightGBM] [Info] Number of data points in the train set: 2495, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 270.780762\n",
      "85/90 'Singapore' - 'Pyshka' - 'Coffee' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000924 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11491\n",
      "[LightGBM] [Info] Number of data points in the train set: 2495, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 876.056914\n",
      "86/90 'Singapore' - 'Pyshka' - 'Cream' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000979 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11504\n",
      "[LightGBM] [Info] Number of data points in the train set: 2495, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 1937.478156\n",
      "87/90 'Singapore' - 'Pyshka' - 'Strawberry Jam' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000757 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11572\n",
      "[LightGBM] [Info] Number of data points in the train set: 2495, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 1025.044489\n",
      "88/90 'Singapore' - 'Pyshka' - 'Sugar Powder' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000815 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11508\n",
      "[LightGBM] [Info] Number of data points in the train set: 2495, number of used features: 61\n",
      "[LightGBM] [Info] Start training from score 1604.571142\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>y_test</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>country</th>\n",
       "      <th>store</th>\n",
       "      <th>product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>215.0</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Donots</td>\n",
       "      <td>Chocolate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-12-02</td>\n",
       "      <td>212.0</td>\n",
       "      <td>221.0</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Donots</td>\n",
       "      <td>Chocolate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-12-03</td>\n",
       "      <td>210.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Donots</td>\n",
       "      <td>Chocolate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-12-04</td>\n",
       "      <td>231.0</td>\n",
       "      <td>247.0</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Donots</td>\n",
       "      <td>Chocolate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-12-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>219.0</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Donots</td>\n",
       "      <td>Chocolate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   timestamp  y_test  y_pred country   store    product\n",
       "0 2016-12-01     NaN   215.0  Canada  Donots  Chocolate\n",
       "1 2016-12-02   212.0   221.0  Canada  Donots  Chocolate\n",
       "2 2016-12-03   210.0   226.0  Canada  Donots  Chocolate\n",
       "3 2016-12-04   231.0   247.0  Canada  Donots  Chocolate\n",
       "4 2016-12-05     NaN   219.0  Canada  Donots  Chocolate"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm_model = LGBMRegressor(random_state=42)\n",
    "lgbm_predictions = run_pipeline(lgbm_model)\n",
    "\n",
    "lgbm_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "_cell_id": "3peG9LfBQeIMKQJC"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2728, 6)"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm_predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–£–≥–∞–¥–∞–π—Ç–µ, —á—Ç–æ? –û—Ç—Ä–∏—Å—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "_cell_id": "2HScDF5S2npQiTpY"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf78f1057eb24c13955a6f419246faa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='country', options=('Canada', 'Finland', 'Italy', 'Kenya', 'Norway'‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@widgets.interact(\n",
    "    country=widgets.Dropdown(options=lgbm_predictions[\"country\"].unique()),\n",
    "    store=widgets.Dropdown(options=lgbm_predictions[\"store\"].unique()),\n",
    "    products=widgets.Dropdown(options=lgbm_predictions[\"product\"].unique()),\n",
    "    start_date=widgets.DatePicker(value=lgbm_predictions[\"timestamp\"].min()),\n",
    ")\n",
    "def show_lgbm_predictions(country: str, store: str, products: str, start_date: date):  # noqa: D103\n",
    "    start_date = pd.Timestamp(start_date)\n",
    "    plot_df = lgbm_predictions[\n",
    "        (lgbm_predictions[\"country\"] == country)\n",
    "        & (lgbm_predictions[\"store\"] == store)\n",
    "        & (lgbm_predictions[\"product\"] == products)\n",
    "        & (lgbm_predictions[\"timestamp\"].dt.date >= start_date.date())\n",
    "    ]\n",
    "\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=1, figsize=(15, 4))\n",
    "\n",
    "    axs.plot(plot_df[\"timestamp\"], plot_df[\"y_pred\"], marker=\"o\", label=\"pred\")\n",
    "    # —É–±–µ—Ä—ë–º –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è, —á—Ç–æ–±—ã —Ç–æ—á–∫–∏ —Å–æ–µ–¥–∏–Ω–∏–ª–∏—Å—å –º–µ–∂–¥—É —Å–æ–±–æ–π - —á–∏—Å—Ç–æ –¥–ª—è –∫—Ä–∞—Å–æ—Ç—ã\n",
    "    plot_df = plot_df.dropna()\n",
    "    axs.plot(plot_df[\"timestamp\"], plot_df[\"y_test\"], marker=\"o\", label=\"true\")\n",
    "    axs.set_xlabel(\"–î–∞—Ç–∞\")\n",
    "    axs.set_ylabel(\"–ö–æ–ª-–≤–æ –ø—Ä–æ–¥–∞–Ω–Ω—ã—Ö –ø—ã—à–µ–∫\")\n",
    "    axs.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ñ–∞–π–ª."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "_cell_id": "hQzkCxpBtiSin6Et"
   },
   "outputs": [],
   "source": [
    "pred_dpath = Path(\"forecast_predictions\")\n",
    "pred_dpath.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pred_fpath = pred_dpath / \"lgbm_predictions.csv\"\n",
    "lgbm_predictions.to_csv(pred_fpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–£—Ä–∞! –ú–æ–¥–µ–ª–∏ –æ–±—É—á–µ–Ω—ã, —Ç–µ–ø–µ—Ä—å –æ—Å—Ç–∞–ª–æ—Å—å —Å–∞–º–æ–µ –≤–∫—É—Å–Ω–æ–µ - –ø–æ—Å—á–∏—Ç–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏, —Å—Ä–∞–≤–Ω–∏—Ç—å –º–æ–¥–µ–ª–∏ –∏ —Å–¥–µ–ª–∞—Ç—å –≤—ã–≤–æ–¥—ã. \n",
    "\n",
    "–≠—Ç–∏–º –∑–∞–π–º—ë–º—Å—è –≤ —Å–ª–µ–¥—É—é—â–µ–º –Ω–æ—É—Ç–±—É–∫–µ. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ó–∞–∫–ª—é—á–µ–Ω–∏–µ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í —ç—Ç–æ–º –Ω–æ—É—Ç–±—É–∫–µ –≤—ã –ø–æ–∑–Ω–∞–∫–æ–º–∏–ª–∏—Å—å —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–π –º–æ–¥–µ–ª—å—é —Å –ø–∞—Ñ–æ—Å–Ω—ã–º –Ω–∞–∑–≤–∞–Ω–∏–µ–º –•–æ–ª—å—Ç–∞-–í–∏–Ω—Ç–µ—Ä—Å–∞. \n",
    "\n",
    "–ü–æ—Å–º–æ—Ç—Ä–µ–ª–∏ –∫–∞–∫ –Ω–∞—Ç—è–Ω—É—Ç—å –∑–Ω–∞–∫–æ–º—ã–µ –º–æ–¥–µ–ª–∏ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –Ω–∞ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ. \n",
    "\n",
    "–ò –ø–æ–≥—Ä—É–∑–∏–ª–∏—Å—å –≤ —É–¥–∏–≤–∏—Ç–µ–ª—å–Ω—ã–π –º–∏—Ä –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –í–æ–ø—Ä–æ—Å—ã –¥–ª—è –∑–∞–∫—Ä–µ–ø–ª–µ–Ω–∏—è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. –í —á—ë–º —Ä–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É —Ç—Ä–µ–Ω–¥–æ–º –∏ —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç—å—é? \n",
    "2. –ú–æ–∂–µ—Ç –ª–∏ –±—ã—Ç—å —Ç–∞–∫, —á—Ç–æ –≤ —Ä—è–¥—É –µ—Å—Ç—å —Ç—Ä–µ–Ω–¥, –Ω–æ –Ω–µ—Ç —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç–∏? \n",
    "3. –ú–æ–∂–Ω–æ –ª–∏ –æ–±—É—á–∏—Ç—å —Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –±–µ–∑ –ª–∞–≥–Ω—É—Ç—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤? \n",
    "4. –ö–∞–∫ –æ–±—É—á–∏—Ç—å —Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—É—é –º–æ–¥–µ–ª—å, –µ—Å–ª–∏ –≤ –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤? \n",
    "5. –ß—Ç–æ –Ω—É–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å, —á—Ç–æ–±—ã –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –•–æ–ª—å—Ç–∞-–í–∏–Ω—Ç–µ—Ä—Å–∞, –µ—Å–ª–∏ –≤ –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
