{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ. –ú–æ–¥–µ–ª–∏."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> üöÄ –í —ç—Ç–æ–π –ø—Ä–∞–∫—Ç–∏–∫–µ –Ω–∞–º –ø–æ–Ω–∞–¥–æ–±—è—Ç—Å—è: `etna==2.10.0, numpy==1.26.4, pandas==1.5.3, matplotlib==3.10.3, seaborn==0.13.2, lightgbm==4.6.0, prophet==1.1.6` \n",
    "\n",
    "> üöÄ –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –≤—ã –∏—Ö –º–æ–∂–µ—Ç–µ —Å –ø–æ–º–æ—â—å—é –∫–æ–º–∞–Ω–¥—ã: `%pip install etna==2.10.0 numpy==1.26.4 pandas==1.5.3 matplotlib==3.10.3 seaborn==0.13.2 lightgbm==4.6.0 prophet==1.1.6` \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ\n",
    "\n",
    "* [–ó–∞–≥—Ä—É–∑–∫–∞ –≤—ã–±–æ—Ä–æ–∫](#–ó–∞–≥—Ä—É–∑–∫–∞-–≤—ã–±–æ—Ä–æ–∫)\n",
    "* [–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –º–æ–¥–µ–ª–∏](#–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ-–º–æ–¥–µ–ª–∏)\n",
    "* [–ú–æ–¥–µ–ª–∏ —Ä–µ–≥—Ä–µ—Å—Å–æ—Ä—ã](#–ú–æ–¥–µ–ª–∏-—Ä–µ–≥—Ä–µ—Å—Å–æ—Ä—ã)\n",
    "  * [–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤](#–ì–µ–Ω–µ—Ä–∞—Ü–∏—è-–ø—Ä–∏–∑–Ω–∞–∫–æ–≤)\n",
    "  * [–õ–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è](#–õ–∏–Ω–µ–π–Ω–∞—è-—Ä–µ–≥—Ä–µ—Å—Å–∏—è)\n",
    "  * [LGBM : –ë—É—Å—Ç–∏–Ω–≥](#LGBM-:-–ë—É—Å—Ç–∏–Ω–≥)\n",
    "* [–ó–∞–∫–ª—é—á–µ–Ω–∏–µ](#–ó–∞–∫–ª—é—á–µ–Ω–∏–µ)\n",
    "* [–í–æ–ø—Ä–æ—Å—ã –¥–ª—è –∑–∞–∫—Ä–µ–ø–ª–µ–Ω–∏—è](#–í–æ–ø—Ä–æ—Å—ã-–¥–ª—è-–∑–∞–∫—Ä–µ–ø–ª–µ–Ω–∏—è)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü—Ä–∏–≤–µ—Ç! –í –ø—Ä–µ–¥—ã–¥—É—â–µ–º –Ω–æ—É—Ç–±—É–∫–µ –º—ã –ø–æ–¥–≥–æ—Ç–æ–≤–∏–ª–∏ –≤—ã–±–æ—Ä–∫–∏, –Ω–∞ –∫–æ—Ç–æ—Ä—ã—Ö —Ç–µ–ø–µ—Ä—å –±—É–¥–µ–º —É—á–∏—Ç—å –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥–µ–ª–∏. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "_cell_id": "hazT9XSLxttiqRH6"
   },
   "outputs": [],
   "source": [
    "from datetime import date, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "from etna.datasets.tsdataset import TSDataset\n",
    "from etna.models import HoltWintersModel\n",
    "from etna.pipeline import Pipeline\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from prophet.make_holidays import make_holidays_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ó–∞–≥—Ä—É–∑–∫–∞ –≤—ã–±–æ—Ä–æ–∫"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–ª—è –Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏–º —Ä–∞–Ω–µ–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã–µ –≤—ã–±–æ—Ä–∫–∏."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "_cell_id": "BPY0vaQ3gzUo2r8u"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((222288, 5), (2728, 5))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpath = Path().cwd() / \"ts_datasets\"\n",
    "\n",
    "train_fpath = dpath / \"train.csv\"\n",
    "train_df = pd.read_csv(train_fpath, parse_dates=[\"timestamp\"], index_col=0)\n",
    "\n",
    "test_fpath = dpath / \"test.csv\"\n",
    "test_df = pd.read_csv(test_fpath, parse_dates=[\"timestamp\"], index_col=0)\n",
    "\n",
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í –¥–∞–ª—å–Ω–µ–π—à–µ–º –Ω–∞–º —Ç–∞–∫–∂–µ –ø–æ–Ω–∞–¥–æ–±–∏—Ç—Å—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ: \n",
    "- –≥–æ—Ä–∏–∑–æ–Ω—Ç–µ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è (–≤ –Ω–∞—à–µ–º —Å–ª—É—á–∞–µ —Ö–æ—Ç–∏–º –Ω–∞ 31 –¥–µ–Ω—å);\n",
    "- –¥–∞—Ç–∞ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö (1 –¥–µ–∫–∞–±—Ä—è 2016–≥).\n",
    "\n",
    "–í—ã–Ω–µ—Å–µ–º –∏—Ö –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ, —á—Ç–æ–±—ã –º–µ–Ω—å—à–µ —Ö–∞—Ä–¥–∫–æ–¥–∏—Ç—å. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "_cell_id": "ZIjDI0y4MJ4tijz9"
   },
   "outputs": [],
   "source": [
    "HORIZON = 31\n",
    "SPLIT_DATE = date(2016, 12, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –º–æ–¥–µ–ª–∏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ù–∞—á–Ω—ë–º —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π. –¢–∞–∫–∏–º –º–æ–¥–µ–ª—è–º –ø–æ —Å—É—Ç–∏ –Ω–µ –Ω—É–∂–Ω–æ –æ–±—É—á–µ–Ω–∏–µ, —Ç.–∫. –æ–Ω–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç —á–∏—Å—Ç–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏, –Ω–∞ –±–∞–∑–µ –∫–æ—Ç–æ—Ä—ã—Ö –≥–µ–Ω–µ—Ä–∏—Ä—É—é—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤ –±—É–¥—É—â–µ–µ. \n",
    "\n",
    "–í —ç—Ç–æ—Ç —Ä–∞–∑ –º—ã –ø–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –º–æ–¥–µ–ª—å –•–æ–ª—å—Ç–∞-–£–∏–Ω—Ç–µ—Ä—Å–∞. –ó–≤—É—á–∏—Ç –ø–∞—Ñ–æ—Å–Ω–æ, –Ω–æ –ø–æ–¥ –∫—Ä–∞—Å–∏–≤—ã–º –Ω–∞–∑–≤–∞–Ω–∏—è–º —Å–∫—Ä—ã–≤–∞–µ—Ç—Å—è —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–µ —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ. \n",
    "\n",
    "–≠—Ç–∞ –º–æ–¥–µ–ª—å —É–∂–µ –µ—Å—Ç—å –≤ ETNA, –¥–∞–≤–∞–π—Ç–µ –µ—é –∏ –≤–æ—Å–ø–æ–ª—å–∑—É–µ–º—Å—è."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢–∞–∫ –∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ–º —Å ETNA, —Å–Ω–æ–≤–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Ç–∞–±–ª–∏—Ü—ã –≤ ETNA-–¥–∞—Ç–∞—Å–µ—Ç—ã –∏ –∏–º–∏—Ç–∏—Ä—É–µ–º `segment` –¥–ª—è –Ω–∞—à–µ–≥–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä—è–¥–∞. –°–¥–µ–ª–∞–µ–º —ç—Ç–æ –∫–∞–∫ –≤ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ, —Ç–∞–∫ –∏ –≤ —Ç–µ—Å—Ç–æ–≤–æ–π. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "_cell_id": "rNViH4toETaFg82e"
   },
   "outputs": [],
   "source": [
    "train_ts_df = train_df.copy(deep=True)\n",
    "test_ts_df = test_df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "_cell_id": "omJrza0g6VBbXkD5"
   },
   "outputs": [],
   "source": [
    "# TODO - Ê∑ªÂä†segmentÂàó\n",
    "train_ts_df[\"segment\"] = train_ts_df[\"country\"] + \"_\" + train_ts_df[\"store\"] + \"_\" + train_ts_df[\"product\"]\n",
    "test_ts_df[\"segment\"] = test_ts_df[\"country\"] + \"_\" + test_ts_df[\"store\"] + \"_\" + test_ts_df[\"product\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "_cell_id": "wvWD9eXlxA3ZPHjN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–¢–µ—Å—Ç—ã –ø—Ä–æ—à–ª–∏! –í—Å—ë —Ö–æ—Ä–æ—à–æ!\n"
     ]
    }
   ],
   "source": [
    "assert \"segment\" in train_ts_df.columns, \"–ö–æ–ª–æ–Ω–∫–∞ `segment` –Ω–µ –Ω–∞–π–¥–µ–Ω–∞\"\n",
    "assert \"segment\" in test_ts_df.columns, \"–ö–æ–ª–æ–Ω–∫–∞ `segment` –Ω–µ –Ω–∞–π–¥–µ–Ω–∞\"\n",
    "\n",
    "print(\"–¢–µ—Å—Ç—ã –ø—Ä–æ—à–ª–∏! –í—Å—ë —Ö–æ—Ä–æ—à–æ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "_cell_id": "9Ph9XDREyRMvm1ji"
   },
   "outputs": [],
   "source": [
    "# TODO - Âà†Èô§Âàócountry, storeÂíåproduct\n",
    "train_ts_df = train_ts_df.drop(columns=[\"country\", \"store\", \"product\"])\n",
    "test_ts_df = test_ts_df.drop(columns=[\"country\", \"store\", \"product\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "_cell_id": "j5IQqT6WUKVG6kly"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–¢–µ—Å—Ç—ã –ø—Ä–æ—à–ª–∏! –í—Å—ë —Ö–æ—Ä–æ—à–æ!\n"
     ]
    }
   ],
   "source": [
    "train_cols = train_ts_df.columns\n",
    "test_cols = test_ts_df.columns \n",
    "\n",
    "for col_name in [\"country\", \"store\", \"product\"]:\n",
    "    assert col_name not in train_cols, f\"–í –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ –≤—Å—ë –µ—â—ë –µ—Å—Ç—å –∫–æ–ª–æ–Ω–∫–∞ {col_name}\"\n",
    "    assert col_name not in test_cols, f\"–í —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ –≤—Å—ë –µ—â—ë –µ—Å—Ç—å –∫–æ–ª–æ–Ω–∫–∞ {col_name}\"\n",
    "\n",
    "print(\"–¢–µ—Å—Ç—ã –ø—Ä–æ—à–ª–∏! –í—Å—ë —Ö–æ—Ä–æ—à–æ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "_cell_id": "RVADwWBdvReoDuMT"
   },
   "outputs": [],
   "source": [
    "# TODO - ËΩ¨Êç¢pandasË°®Ê†º‰∏∫ETNA Dataset\n",
    "train_ts_df = TSDataset(df=train_ts_df, freq=\"D\")\n",
    "test_ts_df = TSDataset(df=test_ts_df, freq=\"D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "_cell_id": "QEvt6fBnkhrIZfu8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–¢–µ—Å—Ç—ã –ø—Ä–æ—à–ª–∏! –í—Å—ë —Ö–æ—Ä–æ—à–æ!\n"
     ]
    }
   ],
   "source": [
    "assert isinstance(train_ts_df, TSDataset), f\"–ù–µ–≤–µ—Ä–Ω—ã–π —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö: {type(train_ts_df)}\"\n",
    "assert isinstance(test_ts_df, TSDataset), f\"–ù–µ–≤–µ—Ä–Ω—ã–π —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö: {type(test_ts_df)}\"\n",
    "\n",
    "print(\"–¢–µ—Å—Ç—ã –ø—Ä–æ—à–ª–∏! –í—Å—ë —Ö–æ—Ä–æ—à–æ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–∞—Ç–∞—Å–µ—Ç—ã —Å–∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã, –º–æ–∂–Ω–æ \"—É—á–∏—Ç—å\" –º–æ–¥–µ–ª—å. \n",
    "\n",
    "–ù–æ –Ω–∞ –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–∏ –ª–µ–∂–∏—Ç –≤–æ–ø—Ä–æ—Å: \"–ê –Ω–∞ —á—ë–º —É—á–∏—Ç—å, –µ—Å–ª–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤-—Ç–æ –Ω–µ—Ç?\". –ò —ç—Ç–æ —Ö–æ—Ä–æ—à–∏–π –≤–æ–ø—Ä–æ—Å. ETNA –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ñ–∏—á–∏ –ø—Ä—è–º–æ —á–µ—Ä–µ–∑ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏.\n",
    "\n",
    "–í –Ω–∞—à–µ–º —Å–ª—É—á–∞–µ –¥–æ–±–∞–≤–∏–º: \n",
    "- –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç—Ä–µ–Ω–¥–µ –≤ –¥–∞–Ω–Ω—ã—Ö;\n",
    "- –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–µ–∑–æ–Ω–Ω–æ–π —Å–æ—Å—Ç–∞–≤–ª—è—é—â–µ–π.\n",
    "\n",
    "<details>\n",
    "    <summary>ü§ì –ù–∞–ø–æ–º–∏–Ω–∞–ª–∫–∞ –ø—Ä–æ —Ç—Ä–µ–Ω–¥—ã –∏ —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç—å [–ù–∞–∂–º–∏ –Ω–∞ –º–µ–Ω—è]</summary>\n",
    "\n",
    "**–¢—Ä–µ–Ω–¥** - —ç—Ç–æ –∫–∞–∫–∞—è-—Ç–æ –æ–±—â–∞—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å, —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω–∞—è –¥–ª—è –≤—Å–µ–≥–æ —Ä—è–¥–∞ —Ü–µ–ª–∏–∫–æ–º. **–°–µ–∑–æ–Ω–Ω–æ—Å—Ç—å** - —ç—Ç–æ –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ –ø–æ–≤—Ç–æ—Ä—è—é—â–∞—è—Å—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞. –û–±–µ —ç—Ç–∏ —Å–æ—Å—Ç–∞–≤–ª—è—é—â–∏–µ —á–∞—Å—Ç–æ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤ —Ä—è–¥–µ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ. \n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "_cell_id": "9l0ZLMQ9JrH6ND1i"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(model = HoltWintersModel(trend = 'add', damped_trend = False, seasonal = 'add', seasonal_periods = 7, initialization_method = 'estimated', initial_level = None, initial_trend = None, initial_seasonal = None, use_boxcox = False, bounds = None, dates = None, freq = None, missing = 'none', smoothing_level = None, smoothing_trend = None, smoothing_seasonal = None, damping_trend = None, ), transforms = (), horizon = 31, )"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = HoltWintersModel(\n",
    "    trend=\"add\", \n",
    "    seasonal=\"add\", \n",
    "    seasonal_periods=7,\n",
    ")\n",
    "pipeline = Pipeline(model=model, horizon=HORIZON)\n",
    "\n",
    "pipeline.fit(train_ts_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ú–æ–¥–µ–ª—å –æ–±—É—á–∏–ª–∏, —Ç–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø–æ–ª—É—á–∏—Ç—å. \n",
    "\n",
    "–ù–µ–º–Ω–æ–≥–æ –ø–æ–∏–≥—Ä–∞–µ–º—Å—è —Å –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–æ–∫, —á—Ç–æ–±—ã –¥–∞–ª—å—à–µ –±—ã–ª–æ —É–¥–æ–±–Ω–µ–µ —Å—á–∏—Ç–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "_cell_id": "Tz46UzUDK7aFsxT1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>y_test</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>country</th>\n",
       "      <th>store</th>\n",
       "      <th>product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>215.0</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Donots</td>\n",
       "      <td>Chocolate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-12-02</td>\n",
       "      <td>212.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Donots</td>\n",
       "      <td>Chocolate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-12-03</td>\n",
       "      <td>210.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Donots</td>\n",
       "      <td>Chocolate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-12-04</td>\n",
       "      <td>231.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Donots</td>\n",
       "      <td>Chocolate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-12-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>215.0</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Donots</td>\n",
       "      <td>Chocolate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   timestamp  y_test  y_pred country   store    product\n",
       "0 2016-12-01     NaN   215.0  Canada  Donots  Chocolate\n",
       "1 2016-12-02   212.0   219.0  Canada  Donots  Chocolate\n",
       "2 2016-12-03   210.0   213.0  Canada  Donots  Chocolate\n",
       "3 2016-12-04   231.0   233.0  Canada  Donots  Chocolate\n",
       "4 2016-12-05     NaN   215.0  Canada  Donots  Chocolate"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# –ø–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è \n",
    "ts_forecast_df = pipeline.forecast()\n",
    "\n",
    "# –≤ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –ø–µ—Ä–µ–∏–º–µ–Ω—É–µ–º –≤ y_test (—ç—Ç–æ true –∑–Ω–∞—á–µ–Ω–∏—è)\n",
    "forecast_test_df = test_ts_df.to_pandas(flatten=True).rename(columns={\"target\": \"y_test\"})\n",
    "\n",
    "# –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –ø–µ—Ä–µ–∏–º–µ–Ω—É–µ–º –≤ y_pred (—ç—Ç–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è)\n",
    "forecast_df = ts_forecast_df.to_pandas(flatten=True).rename(columns={\"target\": \"y_pred\"})\n",
    "\n",
    "# —Å–ª–µ–ø–∏–º —Ç–∞–±–ª–∏—Ü—ã, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å –æ–±—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
    "hotl_winters_predictions = forecast_test_df.merge(\n",
    "    forecast_df, \n",
    "    on=[\"timestamp\", \"segment\"], \n",
    "    how=\"outer\"\n",
    ")\n",
    "\n",
    "# —Ä–∞–∑–¥–µ–ª—è–µ–º segment –æ–±—Ä–∞—Ç–Ω–æ –Ω–∞ 3 —Å–æ—Å—Ç–∞–≤–ª—è—é—â–∏–µ \n",
    "hotl_winters_predictions[\"country\"] = hotl_winters_predictions[\"segment\"].str.split(\"_\").str[0]\n",
    "hotl_winters_predictions[\"store\"] = hotl_winters_predictions[\"segment\"].str.split(\"_\").str[1]\n",
    "hotl_winters_predictions[\"product\"] = hotl_winters_predictions[\"segment\"].str.split(\"_\").str[2]\n",
    "hotl_winters_predictions = hotl_winters_predictions.drop(columns=[\"segment\"])\n",
    "\n",
    "# –æ–∫—Ä—É–≥–ª—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –æ—Ç –º–æ–¥–µ–ª–∏, —Ç.–∫. –ø—Ä–æ–¥–∞–≤–∞—Ç—å —á–∞—Å—Ç—å –ø—ã—à–∫–∏ - –Ω–µ –≤–∞—Ä–∏–∞–Ω—Ç \n",
    "hotl_winters_predictions[\"y_pred\"] = np.ceil(hotl_winters_predictions[\"y_pred\"])#.astype(int)\n",
    "\n",
    "hotl_winters_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "_cell_id": "QqZeEiFioPOi9QkK"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2728, 6)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hotl_winters_predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û—Ç—Ä–∏—Å—É–µ–º –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤ —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏ —Å –∏—Å—Ç–∏–Ω–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –Ω–∞ –≥—Ä–∞—Ñ–∏–∫–µ. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "_cell_id": "zSDu4flZ0fOjahoc"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "649035a8eadc4e76aa6ff8ea253f7a1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='country', options=('Canada', 'Finland', 'Italy', 'Kenya', 'Norway'‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@widgets.interact(\n",
    "    country=widgets.Dropdown(options=hotl_winters_predictions[\"country\"].unique()),\n",
    "    store=widgets.Dropdown(options=hotl_winters_predictions[\"store\"].unique()),\n",
    "    products=widgets.Dropdown(options=hotl_winters_predictions[\"product\"].unique()),\n",
    "    start_date=widgets.DatePicker(value=hotl_winters_predictions[\"timestamp\"].min()),\n",
    ")\n",
    "def show_holt_winters_predictions(country: str, store: str, products: str, start_date: date):\n",
    "    start_date = pd.Timestamp(start_date)\n",
    "    plot_df = hotl_winters_predictions[\n",
    "        (hotl_winters_predictions[\"country\"] == country)\n",
    "        & (hotl_winters_predictions[\"store\"] == store)\n",
    "        & (hotl_winters_predictions[\"product\"] == products)\n",
    "        & (hotl_winters_predictions[\"timestamp\"].dt.date >= start_date.date())\n",
    "    ]\n",
    "\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=1, figsize=(15, 4))\n",
    "\n",
    "    axs.plot(plot_df[\"timestamp\"], plot_df[\"y_pred\"], marker=\"o\", label=\"pred\")\n",
    "    # —É–±–µ—Ä—ë–º –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è, —á—Ç–æ–±—ã —Ç–æ—á–∫–∏ —Å–æ–µ–¥–∏–Ω–∏–ª–∏—Å—å –º–µ–∂–¥—É —Å–æ–±–æ–π - –≤–æ –∏–º—è —Å—É–±—ä–∫—Ç–∏–≤–Ω–æ–≥–æ –ø—Ä–µ–∫—Ä–∞—Å–Ω–æ–≥–æ \n",
    "    plot_df = plot_df.dropna()\n",
    "    axs.plot(plot_df[\"timestamp\"], plot_df[\"y_test\"], marker=\"o\", label=\"true\")\n",
    "    axs.set_xlabel(\"–î–∞—Ç–∞\")\n",
    "    axs.set_ylabel(\"–ö–æ–ª-–≤–æ –ø—Ä–æ–¥–∞–Ω–Ω—ã—Ö –ø—ã—à–µ–∫\")\n",
    "    axs.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ß—Ç–æ–±—ã –Ω–µ –ø–æ—Ç–µ—Ä—è—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ä–∞–±–æ—Ç—ã –º–æ–¥–µ–ª–∏, —Å–æ—Ö—Ä–∞–Ω–∏–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤ —Ñ–∞–π–ª."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "_cell_id": "E9dwNOeU0MXmB9vk"
   },
   "outputs": [],
   "source": [
    "pred_dpath = Path(\"forecast_predictions\")\n",
    "pred_dpath.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pred_fpath = pred_dpath / \"holt_winters_predictions.csv\"\n",
    "hotl_winters_predictions.to_csv(pred_fpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ú–æ–¥–µ–ª–∏ —Ä–µ–≥—Ä–µ—Å—Å–æ—Ä—ã"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–µ –∂–µ –º–æ–¥–µ–ª–∏, —á—Ç–æ –∏ –≤ \"—Å—ã—Ä–æ–π\" —Ä–µ–≥—Ä–µ—Å—Å–∏–∏. –ù–∞–ø—Ä–∏–º–µ—Ä, –ª–∏–Ω–µ–π–Ω—É—é —Ä–µ–≥—Ä–µ—Å—Å–∏—é –∏–ª–∏ LGBM. –ò—Ö –º—ã –∏ –ø–æ–≤–∞—Ä–∏–º –¥–ª—è –Ω–∞—à–∏—Ö –ø—ã—à–µ–∫.\n",
    "\n",
    "–ù–æ –µ—Å—Ç—å –±–æ–ª—å—à–æ–µ –∂–∏—Ä–Ω–æ–µ **–ù–û**. –ó–¥–µ—Å—å –º—ã —É–∂–µ –Ω–µ —Å–º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å ETNA, –∏ –≤—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –Ω—É–∂–Ω–æ –±—É–¥–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ä—É—á–∫–∞–º–∏. –ß—Ç–æ –∂, –ø–æ–≥–Ω–∞–ª–∏! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü—Ä–∏–∑–Ω–∞–∫–∏ –Ω—É–∂–Ω—ã –±—É–¥—É—Ç –∫–∞–∫ –¥–ª—è –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏, —Ç–∞–∫ –∏ –¥–ª—è —Ç–µ—Å—Ç–æ–≤–æ–π. –ü–æ—ç—Ç–æ–º—É –æ–±—ä–µ–¥–∏–Ω–∏–º –∏—Ö –æ–±—Ä–∞—Ç–Ω–æ –≤ –µ–¥–∏–Ω—É—é —Ç–∞–±–ª–∏—Ü—É, —Å–≥–µ–Ω–µ—Ä–∏–º –ø—Ä–∏–∑–Ω–∞–∫–∏, –∞ –ø–æ—Ç–æ–º —Å–Ω–æ–≤–∞ —Ä–∞–∑–¥–µ–ª–∏–º –ø–æ —Ç–æ–π –∂–µ –¥–∞—Ç–µ 1 –¥–µ–∫–∞–±—Ä—è 2016. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "_cell_id": "khbD51V9lK79jOnU"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>store</th>\n",
       "      <th>product</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>Canada</td>\n",
       "      <td>Donots</td>\n",
       "      <td>Chocolate</td>\n",
       "      <td>300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-02</th>\n",
       "      <td>Canada</td>\n",
       "      <td>Donots</td>\n",
       "      <td>Chocolate</td>\n",
       "      <td>281.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-03</th>\n",
       "      <td>Canada</td>\n",
       "      <td>Donots</td>\n",
       "      <td>Chocolate</td>\n",
       "      <td>297.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-04</th>\n",
       "      <td>Canada</td>\n",
       "      <td>Donots</td>\n",
       "      <td>Chocolate</td>\n",
       "      <td>235.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-05</th>\n",
       "      <td>Canada</td>\n",
       "      <td>Donots</td>\n",
       "      <td>Chocolate</td>\n",
       "      <td>240.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           country   store    product  target\n",
       "timestamp                                    \n",
       "2010-01-01  Canada  Donots  Chocolate   300.0\n",
       "2010-01-02  Canada  Donots  Chocolate   281.0\n",
       "2010-01-03  Canada  Donots  Chocolate   297.0\n",
       "2010-01-04  Canada  Donots  Chocolate   235.0\n",
       "2010-01-05  Canada  Donots  Chocolate   240.0"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat((train_df, test_df)).set_index(\"timestamp\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ù–µ—Ç –µ–¥–∏–Ω–æ–≥–æ –≤–µ—Ä–Ω–æ–≥–æ —Å–ø–æ—Å–æ–±–∞ –∫–∞–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –≥–µ–Ω–µ—Ä–∏—Ç—å –∏ –∫–∞–∫, —Ç—É—Ç –≤–∞—Å –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç —Ç–æ–ª—å–∫–æ –≤–∞—à–∞ —Ñ–∞–Ω—Ç–∞–∑–∏—è –∏ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ. \n",
    "\n",
    "–ú—ã —Å–æ–∑–¥–∞–¥–∏–º —Ç—Ä–∏ —Ç–∏–ø–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: \n",
    "- –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –¥–Ω—è—Ö;\n",
    "- –ø—Ä–∏–∑–Ω–∞–∫–∏ –æ –ø—Ä–∞–∑–¥–Ω–∏–∫–∞—Ö;\n",
    "- –ø—Ä–∏–∑–Ω–∞–∫–∏ –æ \"–ª–∞–≥–∞—Ö\".\n",
    "\n",
    "<details>\n",
    "    <summary>ü§ì –ß—Ç–æ —Ç–∞–∫–æ–µ –ª–∞–≥–∏? [–ù–∞–∂–º–∏ –Ω–∞ –º–µ–Ω—è]</summary>\n",
    "\n",
    "**–õ–∞–≥–∏** - —ç—Ç–æ –∫–∞–∫–∏–µ-—Ç–æ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä—è–¥–∞. –ù–∞–ø—Ä–∏–º–µ—Ä, –ø–µ—Ä–≤—ã–π –ª–∞–≥ - —ç—Ç–æ –≤—á–µ—Ä–∞—à–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ, –∞ –ø—è—Ç—ã–π –ª–∞–≥ - —ç—Ç–æ –∑–Ω–∞—á–µ–Ω–∏–µ 5 –¥–Ω–µ–π –Ω–∞–∑–∞–¥ –∏ —Ç.–¥. –¢–∞–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –æ—á–µ–Ω—å –≤–∞–∂–Ω—ã –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, —Ç.–∫. –∏–º –Ω—É–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ—à–ª–æ–º —Ä—è–¥–∞. \n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ì–µ–Ω–µ—Ä–∏—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏ (—Ñ–∏—á–∏) –Ω—É–∂–Ω–æ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ä—è–¥–∞ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ—Å—Ç–∏, —Ç.–∫. –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ä—è–¥–∞ –º—ã –±—É–¥–µ–º —É—á–∏—Ç—å —Å–≤–æ—é —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å. –í –∏—Ç–æ–≥–µ —É –Ω–∞—Å –ø–æ–ª—É—á–∏—Ç—Å—è 90 –º–æ–¥–µ–ª–µ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ (–Ω–∞ —Å–∞–º–æ–º –¥–µ–ª–µ 88, —Ç.–∫. 2 –ø—É—Å—Ç—ã—Ö —Ä—è–¥–∞ –º—ã –≤—ã–∫–∏–Ω—É–ª–∏). –ê –Ω–µ –æ–¥–Ω–∞, –∫–∞–∫ –≤ ETNA. \n",
    "\n",
    "–î–∞–≤–∞–π—Ç–µ –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞ –ø–æ—Å–º–æ—Ç—Ä–∏–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ä—è–¥–∞, –∞ –ø–æ—Ç–æ–º –Ω–∞–ø–∏—à–µ–º —Ü–∏–∫–ª (–ø–∞–π–ø–ª–∞–π–Ω), –∫–æ—Ç–æ—Ä—ã–π –±—É–¥–µ—Ç –ø—Ä–æ—Ö–æ–¥–∏—Ç—å –ø–æ –∫–∞–∂–¥–æ–º—É —Ä—è–¥—É. \n",
    "\n",
    "–î–æ–ø—É—Å—Ç–∏–º –≤—ã–±–µ—Ä–µ–º –∫–æ—Ñ–µ–π–Ω—É—é –ø—ã—à–∫—É –≤ –ø—ã—à–µ—á–Ω–æ–π Pyshka –≤ –°–∏–Ω–≥–∞–ø—É—Ä–µ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "_cell_id": "XLwNTbWclFhGhAM8"
   },
   "outputs": [],
   "source": [
    "SAMPLE_COUNTRY = \"Singapore\"\n",
    "SAMPLE_STORE = \"Pyshka\"\n",
    "SAMPLE_PRODUCT = \"Coffee\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "_cell_id": "k5tjwLcdc2gIijec"
   },
   "outputs": [],
   "source": [
    "# TODO - ËøáÊª§Âè™ÈíàÂØπ‰∏Ä‰∏™Â∫èÂàóÁöÑÊï∞ÊçÆ\n",
    "sample_df = df[\n",
    "    (df[\"country\"] == SAMPLE_COUNTRY) \n",
    "    & (df[\"store\"] == SAMPLE_STORE) \n",
    "    & (df[\"product\"] == SAMPLE_PRODUCT)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "_cell_id": "ASQLA05dVwVNDRGh"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>store</th>\n",
       "      <th>product</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>Singapore</td>\n",
       "      <td>Pyshka</td>\n",
       "      <td>Coffee</td>\n",
       "      <td>1045.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-02</th>\n",
       "      <td>Singapore</td>\n",
       "      <td>Pyshka</td>\n",
       "      <td>Coffee</td>\n",
       "      <td>1010.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-03</th>\n",
       "      <td>Singapore</td>\n",
       "      <td>Pyshka</td>\n",
       "      <td>Coffee</td>\n",
       "      <td>1040.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-04</th>\n",
       "      <td>Singapore</td>\n",
       "      <td>Pyshka</td>\n",
       "      <td>Coffee</td>\n",
       "      <td>846.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-05</th>\n",
       "      <td>Singapore</td>\n",
       "      <td>Pyshka</td>\n",
       "      <td>Coffee</td>\n",
       "      <td>770.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              country   store product  target\n",
       "timestamp                                    \n",
       "2010-01-01  Singapore  Pyshka  Coffee  1045.0\n",
       "2010-01-02  Singapore  Pyshka  Coffee  1010.0\n",
       "2010-01-03  Singapore  Pyshka  Coffee  1040.0\n",
       "2010-01-04  Singapore  Pyshka  Coffee   846.0\n",
       "2010-01-05  Singapore  Pyshka  Coffee   770.0"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "_cell_id": "qsABHet5TnNxGx60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–¢–µ—Å—Ç—ã –ø—Ä–æ—à–ª–∏! –í—Å—ë —Ö–æ—Ä–æ—à–æ!\n"
     ]
    }
   ],
   "source": [
    "assert set(sample_df[\"country\"].unique()) == set([SAMPLE_COUNTRY]), \"–î–æ–ª–∂–µ–Ω –±—ã—Ç—å —Ç–æ–ª—å–∫–æ –°–∏–Ω–≥–∞–ø—É—Ä –≤ –¥–∞–Ω–Ω—ã—Ö\"\n",
    "assert set(sample_df[\"store\"].unique()) == set([SAMPLE_STORE]), \"–î–æ–ª–∂–Ω–∞ –±—ã—Ç—å —Ç–æ–ª—å–∫–æ –æ–¥–Ω–∞ –ø—ã—à–µ—á–Ω–∞—è –≤ –¥–∞–Ω–Ω—ã—Ö\"\n",
    "assert set(sample_df[\"product\"].unique()) == set([SAMPLE_PRODUCT]), \"–î–æ–ª–∂–Ω–∞ –±—ã—Ç—å —Ç–æ–ª—å–∫–æ –∫–æ—Ñ–µ–π–Ω–∞—è –ø—ã—à–∫–∞ –≤ –¥–∞–Ω–Ω—ã—Ö\"\n",
    "\n",
    "print(\"–¢–µ—Å—Ç—ã –ø—Ä–æ—à–ª–∏! –í—Å—ë —Ö–æ—Ä–æ—à–æ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û—Å—Ç–∞–≤–∏–º —Ç–æ–ª—å–∫–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–∞—Ç–µ –∏ —Ü–µ–ª–µ–≤—É—é –∫–æ–ª–æ–Ω–∫—É. –û—Å—Ç–∞–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ —Å–æ–¥–µ—Ä–∂–∞—Ç –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –∑–Ω–∞—á–µ–Ω–∏—è –∏ –º–æ–¥–µ–ª–∏ –æ–Ω–∏ –Ω–∏—á–µ–º –Ω–µ –ø–æ–º–æ–≥—É—Ç."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "_cell_id": "1Bdi0w7O3pB2ZyHb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>1045.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-02</th>\n",
       "      <td>1010.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-03</th>\n",
       "      <td>1040.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-04</th>\n",
       "      <td>846.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-05</th>\n",
       "      <td>770.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            target\n",
       "timestamp         \n",
       "2010-01-01  1045.0\n",
       "2010-01-02  1010.0\n",
       "2010-01-03  1040.0\n",
       "2010-01-04   846.0\n",
       "2010-01-05   770.0"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df = sample_df[[\"target\"]]\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ù–∞—á–Ω—ë–º —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –¥–Ω—è—Ö. –°—é–¥–∞ –æ—Ç–Ω–æ—Å–∏—Ç—Å—è —Ç–∞–∫–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∫–∞–∫ –¥–µ–Ω—å –º–µ—Å—è—Ü–∞, –¥–µ–Ω—å –Ω–µ–¥–µ–ª–∏, —á–µ—Ç–≤–µ—Ä—Ç—å –≥–æ–¥–∞, –∫ –∫–æ—Ç–æ—Ä–æ–º—É –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∏—Ç —Ç–µ–∫—É—â–∏–π timestamp, –ø–æ—Ä—è–¥–∫–æ–≤—ã–π –¥–µ–Ω—å –≥–æ–¥–∞, –ø–æ—Ä—è–¥–∫–æ–≤–∞—è –Ω–µ–¥–µ–ª—è –≥–æ–¥–∞ –∏ —Ç.–¥ –∏ —Ç.–ø. –í—Å—ë, —á—Ç–æ –≤—ã —Ç–æ–ª—å–∫–æ –º–æ–∂–µ—Ç–µ –ø—Ä–∏–¥—É–º–∞—Ç—å.  \n",
    "\n",
    "–î–∞–≤–∞–π—Ç–µ —Å–¥–µ–ª–∞–µ–º –æ—Ç–¥–µ–ª—å–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–∞–∫–∏—Ö —Ñ–∏—á, –æ–Ω–∞ –Ω–∞–º –ø—Ä–∏–≥–æ–¥–∏—Ç—Å—è –µ—â—ë –≤ –ø–∞–π–ø–ª–∞–π–Ω–µ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "_cell_id": "9DviDMiQRWW8il0a"
   },
   "outputs": [],
   "source": [
    "def create_dtime_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Create day-time features.\"\"\"\n",
    "    df_new = df.copy()\n",
    "\n",
    "    df_new[\"dayofweek\"] = df_new.index.dayofweek.tolist()\n",
    "    df_new[\"cos_weekday\"] = np.cos(df_new[\"dayofweek\"] / 7 * 2 * np.pi)\n",
    "    df_new[\"sin_weekday\"] = np.sin(df_new[\"dayofweek\"] / 7 * 2 * np.pi)\n",
    "\n",
    "    df_new[\"is_leap_year\"] = df_new.index.is_leap_year.tolist()\n",
    "    df_new[\"dayofyear\"] = df_new.index.dayofyear.tolist()\n",
    "    df_new[\"cos_doy\"] = np.cos(df_new[\"dayofyear\"] / (365 + df_new[\"is_leap_year\"]) * 2 * np.pi)\n",
    "    df_new[\"sin_doy\"] = np.sin(df_new[\"dayofyear\"] / (365 + df_new[\"is_leap_year\"]) * 2 * np.pi)\n",
    "\n",
    "    df_new[\"quarter\"] = df_new.index.quarter.tolist()\n",
    "    df_new[\"year\"] = df_new.index.year.tolist()\n",
    "    df_new[\"month\"] = df_new.index.month.tolist()\n",
    "    df_new[\"day\"] = df_new.index.day.tolist()\n",
    "    df_new[\"weekofyear\"] = df_new.index.isocalendar().week.tolist()\n",
    "\n",
    "    df_new[\"is_weekend\"] = 0\n",
    "    df_new.loc[df_new[\"dayofweek\"].isin([5, 6]), \"is_weekend\"] = 1\n",
    "\n",
    "    df_new[\"is_month_start\"] = df_new.index.is_month_start.tolist()\n",
    "    df_new[\"is_month_end\"] = df_new.index.is_month_end.tolist()\n",
    "\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "_cell_id": "Z6AhwVFwv1yCou3S"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>cos_weekday</th>\n",
       "      <th>sin_weekday</th>\n",
       "      <th>is_leap_year</th>\n",
       "      <th>dayofyear</th>\n",
       "      <th>cos_doy</th>\n",
       "      <th>sin_doy</th>\n",
       "      <th>quarter</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>weekofyear</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>is_month_start</th>\n",
       "      <th>is_month_end</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>1045.0</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>-0.433884</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999852</td>\n",
       "      <td>0.017213</td>\n",
       "      <td>1</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-02</th>\n",
       "      <td>1010.0</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>-0.974928</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>0.999407</td>\n",
       "      <td>0.034422</td>\n",
       "      <td>1</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-03</th>\n",
       "      <td>1040.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>-0.781831</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>0.998667</td>\n",
       "      <td>0.051620</td>\n",
       "      <td>1</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-04</th>\n",
       "      <td>846.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>0.997630</td>\n",
       "      <td>0.068802</td>\n",
       "      <td>1</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-05</th>\n",
       "      <td>770.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>0.781831</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "      <td>0.996298</td>\n",
       "      <td>0.085965</td>\n",
       "      <td>1</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            target  dayofweek  cos_weekday  sin_weekday  is_leap_year  \\\n",
       "timestamp                                                               \n",
       "2010-01-01  1045.0          4    -0.900969    -0.433884         False   \n",
       "2010-01-02  1010.0          5    -0.222521    -0.974928         False   \n",
       "2010-01-03  1040.0          6     0.623490    -0.781831         False   \n",
       "2010-01-04   846.0          0     1.000000     0.000000         False   \n",
       "2010-01-05   770.0          1     0.623490     0.781831         False   \n",
       "\n",
       "            dayofyear   cos_doy   sin_doy  quarter  year  month  day  \\\n",
       "timestamp                                                              \n",
       "2010-01-01          1  0.999852  0.017213        1  2010      1    1   \n",
       "2010-01-02          2  0.999407  0.034422        1  2010      1    2   \n",
       "2010-01-03          3  0.998667  0.051620        1  2010      1    3   \n",
       "2010-01-04          4  0.997630  0.068802        1  2010      1    4   \n",
       "2010-01-05          5  0.996298  0.085965        1  2010      1    5   \n",
       "\n",
       "            weekofyear  is_weekend  is_month_start  is_month_end  \n",
       "timestamp                                                         \n",
       "2010-01-01          53           0            True         False  \n",
       "2010-01-02          53           1           False         False  \n",
       "2010-01-03          53           1           False         False  \n",
       "2010-01-04           1           0           False         False  \n",
       "2010-01-05           1           0           False         False  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# –í—ã–∑—ã–≤–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é –∏ –ø–æ—Å–º–æ—Ç—Ä–∏–º, —á—Ç–æ —É –Ω–∞—Å —Å—Ç–∞–ª–æ —Å —Ç–∞–±–ª–∏—á–∫–æ–π\n",
    "sample_df = create_dtime_features(sample_df)\n",
    "\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü—É—Ñ! –ò —É –Ω–∞—Å —É–∂–µ 15 –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–∫–æ–ª–æ–Ω–æ–∫) üòé –î–∞–ª—å—à–µ –±–æ–ª—å—à–µ! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢–µ–ø–µ—Ä—å –¥–æ–±–∞–≤–∏–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –æ –ø—Ä–∞–∑–¥–Ω–∏–∫–∞—Ö. –ï—Å—Ç—å —Ä–∞–¥–æ—Å—Ç–Ω–∞—è –Ω–æ–≤–æ—Å—Ç—å, –ø—Ä–∞–∑–¥–Ω–∏–∫–∏ –Ω–µ –Ω—É–∂–Ω–æ –¥–æ–±–∞–≤–ª—è—Ç—å —Ä—É–∫–∞–º–∏, –∑–∞ –Ω–∞—Å —ç—Ç–æ —É–∂–µ —Å–¥–µ–ª–∞–ª–∏ –¥—Ä—É–≥–∏–µ –ª—é–¥–∏ –≤ –±–∏–±–ª–∏–æ—Ç–µ–∫–µ `Prophet`. \n",
    "\n",
    "–î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —É–∫–∞–∑–∞—Ç—å —Å—Ç—Ä–∞–Ω—É –∏ –¥–∏–∞–ø–∞–∑–æ–Ω –ª–µ—Ç. –ò –ø–æ–ª—É—á–∏–º –∏—Å—á–µ—Ä–ø—ã–≤–∞—é—â–∏–π —Å–ø–∏—Å–æ–∫ –ø—Ä–∞–∑–¥–Ω–∏–∫–æ–≤.\n",
    "\n",
    "–î–ª—è —É–¥–æ–±—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏ –∑–∞–∫–æ–¥–∏—Ä—É–µ–º –±–∏–Ω–∞—Ä–Ω—ã–º –∑–Ω–∞—á–µ–Ω–∏–µ–º –∫–∞–∂–¥—ã–π –ø—Ä–∞–∑–¥–Ω–∏–∫."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "_cell_id": "UUHVIi1P7f0ScHQP"
   },
   "outputs": [],
   "source": [
    "def get_holiday_df(country_name: str) -> pd.DataFrame:\n",
    "    holidays_df = make_holidays_df(year_list=range(2010, 2017), country=country_name)\n",
    "\n",
    "    holidays_df[\"is_holiday\"] = 1\n",
    "    holidays_df = holidays_df.rename(columns={\"ds\": \"timestamp\"})\n",
    "    holidays_df = holidays_df.set_index(\"timestamp\")\n",
    "\n",
    "    return holidays_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "_cell_id": "EraTrMeXINgykJyS"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>holiday</th>\n",
       "      <th>is_holiday</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-01</th>\n",
       "      <td>New Year's Day</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-08</th>\n",
       "      <td>Chinese New Year</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-02-09</th>\n",
       "      <td>Chinese New Year</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-07-06</th>\n",
       "      <td>Eid al-Fitr</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-09-12</th>\n",
       "      <td>Eid al-Adha</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     holiday  is_holiday\n",
       "timestamp                               \n",
       "2016-01-01    New Year's Day           1\n",
       "2016-02-08  Chinese New Year           1\n",
       "2016-02-09  Chinese New Year           1\n",
       "2016-07-06       Eid al-Fitr           1\n",
       "2016-09-12       Eid al-Adha           1"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_holidays_df = get_holiday_df(SAMPLE_COUNTRY)\n",
    "\n",
    "sample_holidays_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ß—Ç–æ–±—ã —Ä–∞—Å—à–∏—Ä–∏—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏, –¥–æ–±–∞–≤–∏–º –≤ —Ç–∞–±–ª–∏—Ü—É –ª–∞–≥–∏ –ø–æ—Å–ª–µ –ø—Ä–∞–∑–¥–Ω–∏—á–Ω–æ–≥–æ –¥–Ω—è (1 –∏ 2 –¥–Ω—è –ø–æ—Å–ª–µ).\n",
    "\n",
    "–î–ª—è —É–¥–æ–±—Å—Ç–≤–∞ –æ—Ñ–æ—Ä–º–∏–º –≤—Å—ë —ç—Ç–æ –≤ –æ—Ç–¥–µ–ª—å–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "_cell_id": "Fbch5Yxhmw5JqjJK"
   },
   "outputs": [],
   "source": [
    "def create_holiday_features(df: pd.DataFrame, holidays_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Create holiday features.\"\"\"\n",
    "    df_new = df.copy()\n",
    "    df_new = df_new.merge(holidays_df[[\"is_holiday\"]], left_index=True, right_index=True, how=\"outer\")\n",
    "    df_new[\"is_holiday\"] = df_new[\"is_holiday\"].fillna(0)\n",
    "    df_new[\"holiday_lag_1\"] = df_new[\"is_holiday\"].shift(1)\n",
    "    df_new[\"holiday_lag_2\"] = df_new[\"is_holiday\"].shift(2)\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "_cell_id": "li7Ufz34S97aEnPK"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>cos_weekday</th>\n",
       "      <th>sin_weekday</th>\n",
       "      <th>is_leap_year</th>\n",
       "      <th>dayofyear</th>\n",
       "      <th>cos_doy</th>\n",
       "      <th>sin_doy</th>\n",
       "      <th>quarter</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>weekofyear</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>is_month_start</th>\n",
       "      <th>is_month_end</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>holiday_lag_1</th>\n",
       "      <th>holiday_lag_2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>1045.0</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>-0.433884</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999852</td>\n",
       "      <td>0.017213</td>\n",
       "      <td>1</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-02</th>\n",
       "      <td>1010.0</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>-0.974928</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>0.999407</td>\n",
       "      <td>0.034422</td>\n",
       "      <td>1</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-03</th>\n",
       "      <td>1040.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>-0.781831</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>0.998667</td>\n",
       "      <td>0.051620</td>\n",
       "      <td>1</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-04</th>\n",
       "      <td>846.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>0.997630</td>\n",
       "      <td>0.068802</td>\n",
       "      <td>1</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-05</th>\n",
       "      <td>770.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>0.781831</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "      <td>0.996298</td>\n",
       "      <td>0.085965</td>\n",
       "      <td>1</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            target  dayofweek  cos_weekday  sin_weekday  is_leap_year  \\\n",
       "timestamp                                                               \n",
       "2010-01-01  1045.0          4    -0.900969    -0.433884         False   \n",
       "2010-01-02  1010.0          5    -0.222521    -0.974928         False   \n",
       "2010-01-03  1040.0          6     0.623490    -0.781831         False   \n",
       "2010-01-04   846.0          0     1.000000     0.000000         False   \n",
       "2010-01-05   770.0          1     0.623490     0.781831         False   \n",
       "\n",
       "            dayofyear   cos_doy   sin_doy  quarter  year  month  day  \\\n",
       "timestamp                                                              \n",
       "2010-01-01          1  0.999852  0.017213        1  2010      1    1   \n",
       "2010-01-02          2  0.999407  0.034422        1  2010      1    2   \n",
       "2010-01-03          3  0.998667  0.051620        1  2010      1    3   \n",
       "2010-01-04          4  0.997630  0.068802        1  2010      1    4   \n",
       "2010-01-05          5  0.996298  0.085965        1  2010      1    5   \n",
       "\n",
       "            weekofyear  is_weekend  is_month_start  is_month_end  is_holiday  \\\n",
       "timestamp                                                                      \n",
       "2010-01-01          53           0            True         False         1.0   \n",
       "2010-01-02          53           1           False         False         0.0   \n",
       "2010-01-03          53           1           False         False         0.0   \n",
       "2010-01-04           1           0           False         False         0.0   \n",
       "2010-01-05           1           0           False         False         0.0   \n",
       "\n",
       "            holiday_lag_1  holiday_lag_2  \n",
       "timestamp                                 \n",
       "2010-01-01            NaN            NaN  \n",
       "2010-01-02            1.0            NaN  \n",
       "2010-01-03            0.0            1.0  \n",
       "2010-01-04            0.0            0.0  \n",
       "2010-01-05            0.0            0.0  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df = create_holiday_features(sample_df, sample_holidays_df)\n",
    "\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ï—â—ë +3 –ø—Ä–∏–∑–Ω–∞–∫–∞. –ò—Ç–æ–≥–æ —É–∂–µ 18 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏.\n",
    "\n",
    "–ü–µ—Ä–µ—Ö–æ–¥–∏–º –∫ \"–ª–∞–≥–Ω—É—Ç—ã–º\" –ø—Ä–∏–∑–Ω–∞–∫–∞–º. –°–≥–µ–Ω–µ—Ä–∏–º –ª–∞–≥–∏ –Ω–∞ 7, 10 –∏ 15 –¥–Ω–µ–π –Ω–∞–∑–∞–¥. \n",
    "\n",
    "–ü–æ–º–∏–º–æ —ç—Ç–æ–≥–æ –¥–æ–±–∞–≤–∏–º –µ—â—ë –Ω–µ–º–Ω–æ–≥–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –ª–∞–≥–∞–º: —Å–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ —Å –æ–∫–Ω–æ–º 14 –∏ 7 –¥–Ω–µ–π. \n",
    "\n",
    "–û–ø—è—Ç—å –∂–µ –∫–∞–∫–∏–µ –ª–∞–≥–∏ –≤—ã–±—Ä–∞—Ç—å, —á—Ç–æ–±—ã –º–æ–¥–µ–ª–∏ –ø–æ–º–æ—á—å, –Ω–∏–∫—Ç–æ –Ω–µ —Å–∫–∞–∂–µ—Ç. –¢—É—Ç –Ω—É–∂–Ω–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –∏ –æ—Ç—Ç–∞–ª–∫–∏–≤–∞—Ç—å—Å—è –æ—Ç –∑–∞–¥–∞—á–∏ –∏ –¥–∞–Ω–Ω—ã—Ö.\n",
    "\n",
    "–î–ª—è —É–¥–æ–±—Å—Ç–≤–∞ –æ–±–µ—Ä–Ω—ë–º –≤—Å—ë –≤ —Ñ—É–Ω–∫—Ü–∏—é."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "_cell_id": "5tJxPbjUiwcwgzIS"
   },
   "outputs": [],
   "source": [
    "def create_lag_features(df: pd.DataFrame, horizon) -> pd.DataFrame:\n",
    "    lags = [7, 10, 15]\n",
    "    df_new = df.copy()\n",
    "    \n",
    "    for lag in lags: \n",
    "        for lag_value in range(horizon, horizon + lag):\n",
    "            feature_name = f\"lag_{lag_value}\"\n",
    "            df_new[feature_name] = df_new[\"target\"].shift(lag)\n",
    "\n",
    "            df_new[f\"{feature_name}_rolling_mean_30\"] = df_new[feature_name].rolling(14).mean() \n",
    "            df_new[f\"{feature_name}_rolling_mean_7\"] = df_new[feature_name].rolling(7).mean()\n",
    "\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "_cell_id": "8PfQgMXAoHVfEbF4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>cos_weekday</th>\n",
       "      <th>sin_weekday</th>\n",
       "      <th>is_leap_year</th>\n",
       "      <th>dayofyear</th>\n",
       "      <th>cos_doy</th>\n",
       "      <th>sin_doy</th>\n",
       "      <th>quarter</th>\n",
       "      <th>year</th>\n",
       "      <th>...</th>\n",
       "      <th>lag_42_rolling_mean_7</th>\n",
       "      <th>lag_43</th>\n",
       "      <th>lag_43_rolling_mean_30</th>\n",
       "      <th>lag_43_rolling_mean_7</th>\n",
       "      <th>lag_44</th>\n",
       "      <th>lag_44_rolling_mean_30</th>\n",
       "      <th>lag_44_rolling_mean_7</th>\n",
       "      <th>lag_45</th>\n",
       "      <th>lag_45_rolling_mean_30</th>\n",
       "      <th>lag_45_rolling_mean_7</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>1045.0</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>-0.433884</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999852</td>\n",
       "      <td>0.017213</td>\n",
       "      <td>1</td>\n",
       "      <td>2010</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-02</th>\n",
       "      <td>1010.0</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>-0.974928</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>0.999407</td>\n",
       "      <td>0.034422</td>\n",
       "      <td>1</td>\n",
       "      <td>2010</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-03</th>\n",
       "      <td>1040.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>-0.781831</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>0.998667</td>\n",
       "      <td>0.051620</td>\n",
       "      <td>1</td>\n",
       "      <td>2010</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-04</th>\n",
       "      <td>846.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>0.997630</td>\n",
       "      <td>0.068802</td>\n",
       "      <td>1</td>\n",
       "      <td>2010</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-05</th>\n",
       "      <td>770.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>0.781831</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "      <td>0.996298</td>\n",
       "      <td>0.085965</td>\n",
       "      <td>1</td>\n",
       "      <td>2010</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            target  dayofweek  cos_weekday  sin_weekday  is_leap_year  \\\n",
       "timestamp                                                               \n",
       "2010-01-01  1045.0          4    -0.900969    -0.433884         False   \n",
       "2010-01-02  1010.0          5    -0.222521    -0.974928         False   \n",
       "2010-01-03  1040.0          6     0.623490    -0.781831         False   \n",
       "2010-01-04   846.0          0     1.000000     0.000000         False   \n",
       "2010-01-05   770.0          1     0.623490     0.781831         False   \n",
       "\n",
       "            dayofyear   cos_doy   sin_doy  quarter  year  ...  \\\n",
       "timestamp                                                 ...   \n",
       "2010-01-01          1  0.999852  0.017213        1  2010  ...   \n",
       "2010-01-02          2  0.999407  0.034422        1  2010  ...   \n",
       "2010-01-03          3  0.998667  0.051620        1  2010  ...   \n",
       "2010-01-04          4  0.997630  0.068802        1  2010  ...   \n",
       "2010-01-05          5  0.996298  0.085965        1  2010  ...   \n",
       "\n",
       "            lag_42_rolling_mean_7  lag_43  lag_43_rolling_mean_30  \\\n",
       "timestamp                                                           \n",
       "2010-01-01                    NaN     NaN                     NaN   \n",
       "2010-01-02                    NaN     NaN                     NaN   \n",
       "2010-01-03                    NaN     NaN                     NaN   \n",
       "2010-01-04                    NaN     NaN                     NaN   \n",
       "2010-01-05                    NaN     NaN                     NaN   \n",
       "\n",
       "            lag_43_rolling_mean_7  lag_44  lag_44_rolling_mean_30  \\\n",
       "timestamp                                                           \n",
       "2010-01-01                    NaN     NaN                     NaN   \n",
       "2010-01-02                    NaN     NaN                     NaN   \n",
       "2010-01-03                    NaN     NaN                     NaN   \n",
       "2010-01-04                    NaN     NaN                     NaN   \n",
       "2010-01-05                    NaN     NaN                     NaN   \n",
       "\n",
       "            lag_44_rolling_mean_7  lag_45  lag_45_rolling_mean_30  \\\n",
       "timestamp                                                           \n",
       "2010-01-01                    NaN     NaN                     NaN   \n",
       "2010-01-02                    NaN     NaN                     NaN   \n",
       "2010-01-03                    NaN     NaN                     NaN   \n",
       "2010-01-04                    NaN     NaN                     NaN   \n",
       "2010-01-05                    NaN     NaN                     NaN   \n",
       "\n",
       "            lag_45_rolling_mean_7  \n",
       "timestamp                          \n",
       "2010-01-01                    NaN  \n",
       "2010-01-02                    NaN  \n",
       "2010-01-03                    NaN  \n",
       "2010-01-04                    NaN  \n",
       "2010-01-05                    NaN  \n",
       "\n",
       "[5 rows x 64 columns]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df = create_lag_features(sample_df, horizon=HORIZON)\n",
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "_cell_id": "gYPKE7SESV6wdUJ9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>cos_weekday</th>\n",
       "      <th>sin_weekday</th>\n",
       "      <th>is_leap_year</th>\n",
       "      <th>dayofyear</th>\n",
       "      <th>cos_doy</th>\n",
       "      <th>sin_doy</th>\n",
       "      <th>quarter</th>\n",
       "      <th>year</th>\n",
       "      <th>...</th>\n",
       "      <th>lag_42_rolling_mean_7</th>\n",
       "      <th>lag_43</th>\n",
       "      <th>lag_43_rolling_mean_30</th>\n",
       "      <th>lag_43_rolling_mean_7</th>\n",
       "      <th>lag_44</th>\n",
       "      <th>lag_44_rolling_mean_30</th>\n",
       "      <th>lag_44_rolling_mean_7</th>\n",
       "      <th>lag_45</th>\n",
       "      <th>lag_45_rolling_mean_30</th>\n",
       "      <th>lag_45_rolling_mean_7</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-12-27</th>\n",
       "      <td>1079.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>0.781831</td>\n",
       "      <td>True</td>\n",
       "      <td>362</td>\n",
       "      <td>0.997643</td>\n",
       "      <td>-6.861474e-02</td>\n",
       "      <td>4</td>\n",
       "      <td>2016</td>\n",
       "      <td>...</td>\n",
       "      <td>978.571429</td>\n",
       "      <td>895.0</td>\n",
       "      <td>987.928571</td>\n",
       "      <td>978.571429</td>\n",
       "      <td>895.0</td>\n",
       "      <td>987.928571</td>\n",
       "      <td>978.571429</td>\n",
       "      <td>895.0</td>\n",
       "      <td>987.928571</td>\n",
       "      <td>978.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-28</th>\n",
       "      <td>1050.0</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>0.974928</td>\n",
       "      <td>True</td>\n",
       "      <td>363</td>\n",
       "      <td>0.998674</td>\n",
       "      <td>-5.147875e-02</td>\n",
       "      <td>4</td>\n",
       "      <td>2016</td>\n",
       "      <td>...</td>\n",
       "      <td>956.571429</td>\n",
       "      <td>796.0</td>\n",
       "      <td>976.642857</td>\n",
       "      <td>956.571429</td>\n",
       "      <td>796.0</td>\n",
       "      <td>976.642857</td>\n",
       "      <td>956.571429</td>\n",
       "      <td>796.0</td>\n",
       "      <td>976.642857</td>\n",
       "      <td>956.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-29</th>\n",
       "      <td>1133.0</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>0.433884</td>\n",
       "      <td>True</td>\n",
       "      <td>364</td>\n",
       "      <td>0.999411</td>\n",
       "      <td>-3.432760e-02</td>\n",
       "      <td>4</td>\n",
       "      <td>2016</td>\n",
       "      <td>...</td>\n",
       "      <td>962.142857</td>\n",
       "      <td>958.0</td>\n",
       "      <td>976.142857</td>\n",
       "      <td>962.142857</td>\n",
       "      <td>958.0</td>\n",
       "      <td>976.142857</td>\n",
       "      <td>962.142857</td>\n",
       "      <td>958.0</td>\n",
       "      <td>976.142857</td>\n",
       "      <td>962.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-30</th>\n",
       "      <td>1253.0</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>-0.433884</td>\n",
       "      <td>True</td>\n",
       "      <td>365</td>\n",
       "      <td>0.999853</td>\n",
       "      <td>-1.716633e-02</td>\n",
       "      <td>4</td>\n",
       "      <td>2016</td>\n",
       "      <td>...</td>\n",
       "      <td>942.571429</td>\n",
       "      <td>858.0</td>\n",
       "      <td>965.642857</td>\n",
       "      <td>942.571429</td>\n",
       "      <td>858.0</td>\n",
       "      <td>965.642857</td>\n",
       "      <td>942.571429</td>\n",
       "      <td>858.0</td>\n",
       "      <td>965.642857</td>\n",
       "      <td>942.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-31</th>\n",
       "      <td>1242.0</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>-0.974928</td>\n",
       "      <td>True</td>\n",
       "      <td>366</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-2.449294e-16</td>\n",
       "      <td>4</td>\n",
       "      <td>2016</td>\n",
       "      <td>...</td>\n",
       "      <td>925.857143</td>\n",
       "      <td>937.0</td>\n",
       "      <td>962.071429</td>\n",
       "      <td>925.857143</td>\n",
       "      <td>937.0</td>\n",
       "      <td>962.071429</td>\n",
       "      <td>925.857143</td>\n",
       "      <td>937.0</td>\n",
       "      <td>962.071429</td>\n",
       "      <td>925.857143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            target  dayofweek  cos_weekday  sin_weekday  is_leap_year  \\\n",
       "timestamp                                                               \n",
       "2016-12-27  1079.0          1     0.623490     0.781831          True   \n",
       "2016-12-28  1050.0          2    -0.222521     0.974928          True   \n",
       "2016-12-29  1133.0          3    -0.900969     0.433884          True   \n",
       "2016-12-30  1253.0          4    -0.900969    -0.433884          True   \n",
       "2016-12-31  1242.0          5    -0.222521    -0.974928          True   \n",
       "\n",
       "            dayofyear   cos_doy       sin_doy  quarter  year  ...  \\\n",
       "timestamp                                                     ...   \n",
       "2016-12-27        362  0.997643 -6.861474e-02        4  2016  ...   \n",
       "2016-12-28        363  0.998674 -5.147875e-02        4  2016  ...   \n",
       "2016-12-29        364  0.999411 -3.432760e-02        4  2016  ...   \n",
       "2016-12-30        365  0.999853 -1.716633e-02        4  2016  ...   \n",
       "2016-12-31        366  1.000000 -2.449294e-16        4  2016  ...   \n",
       "\n",
       "            lag_42_rolling_mean_7  lag_43  lag_43_rolling_mean_30  \\\n",
       "timestamp                                                           \n",
       "2016-12-27             978.571429   895.0              987.928571   \n",
       "2016-12-28             956.571429   796.0              976.642857   \n",
       "2016-12-29             962.142857   958.0              976.142857   \n",
       "2016-12-30             942.571429   858.0              965.642857   \n",
       "2016-12-31             925.857143   937.0              962.071429   \n",
       "\n",
       "            lag_43_rolling_mean_7  lag_44  lag_44_rolling_mean_30  \\\n",
       "timestamp                                                           \n",
       "2016-12-27             978.571429   895.0              987.928571   \n",
       "2016-12-28             956.571429   796.0              976.642857   \n",
       "2016-12-29             962.142857   958.0              976.142857   \n",
       "2016-12-30             942.571429   858.0              965.642857   \n",
       "2016-12-31             925.857143   937.0              962.071429   \n",
       "\n",
       "            lag_44_rolling_mean_7  lag_45  lag_45_rolling_mean_30  \\\n",
       "timestamp                                                           \n",
       "2016-12-27             978.571429   895.0              987.928571   \n",
       "2016-12-28             956.571429   796.0              976.642857   \n",
       "2016-12-29             962.142857   958.0              976.142857   \n",
       "2016-12-30             942.571429   858.0              965.642857   \n",
       "2016-12-31             925.857143   937.0              962.071429   \n",
       "\n",
       "            lag_45_rolling_mean_7  \n",
       "timestamp                          \n",
       "2016-12-27             978.571429  \n",
       "2016-12-28             956.571429  \n",
       "2016-12-29             962.142857  \n",
       "2016-12-30             942.571429  \n",
       "2016-12-31             925.857143  \n",
       "\n",
       "[5 rows x 64 columns]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í–∞–∂–Ω–æ –æ—Ç–º–µ—Ç–∏—Ç—å, —á—Ç–æ –≤ –Ω–∞—á–∞–ª–µ –Ω–∞—à–µ–π –≤—ã–±–æ—Ä–∫–∏ —á–∞—Å—Ç—å –ª–∞–≥–Ω—É—Ç—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å–æ–¥–µ—Ä–∂–∏—Ç `NaN` –≤ —è—á–µ–π–∫–∞—Ö. –≠—Ç–æ —Å–ª—É—á–∏–ª–æ—Å—å –ø–æ—Ç–æ–º—É, —á—Ç–æ —ç—Ç–æ –Ω–∞—á–∞–ª–æ –∏—Å—Ç–æ—Ä–∏–∏ –∏ –ª–∞–≥–∏ –ø—Ä–æ—Å—Ç–æ –Ω–µ–æ—Ç–∫—É–¥–∞ –≤–∑—è—Ç—å. –ü—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ —ç—Ç–∏ —Å—Ç—Ä–æ–∫–∏ –ø—Ä–∏–¥—ë—Ç—Å—è –¥—Ä–æ–ø–Ω—É—Ç—å.\n",
    "\n",
    "–ù–æ —Ç–∞–∫ –∫–∞–∫ —ç—Ç–æ –Ω–∞—á–∞–ª–æ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏, —Ç–æ –Ω–µ —Å—Ç—Ä–∞—à–Ω–æ. –¢–µ–º –±–æ–ª–µ–µ —É–π–¥—ë—Ç –≤—Å–µ–≥–æ –æ–∫–æ–ª–æ 30 —Å—Ç—Ä–æ—á–µ–∫, —á—Ç–æ –º–µ–Ω—å—à–µ 1% –ø—Ä–æ—Ü–µ–Ω—Ç–∞ –Ω–∞—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "_cell_id": "JvRTY3U6kgoJbG80"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2557, 64)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ò—Ç–∞–∫ ... –ª—ë–≥–∫–∏–º –¥–≤–∏–∂–µ–Ω–∏–º —Ä—É–∫–∏ —É –Ω–∞—Å —É–∂–µ 63 –ø—Ä–∏–∑–Ω–∞–∫–∞. –ê –ø–æ–º–Ω–∏—Ç–µ —Å —á–µ–≥–æ –º—ã –Ω–∞—á–∏–Ω–∞–ª–∏? 1 –∫–æ–ª–æ–Ω–∫–∞ —Å –¥–Ω—ë–º –∏ 1 —Ü–µ–ª–µ–≤–∞—è –∫–æ–ª–æ–Ω–∫–∞.  \n",
    "\n",
    "–ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –∏ –±–æ–ª—å—à–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –Ω–æ –ø–æ–∫–∞ –æ—Å—Ç–∞–Ω–æ–≤–∏–º—Å—è –Ω–∞ —ç—Ç–æ–º.\n",
    "\n",
    "–¢—É—Ç –∫–∞–∫ –∏ –≤–µ–∑–¥–µ –≤–∞–∂–Ω–æ **–Ω–µ –ø–µ—Ä–µ–±–æ—Ä—â–∏—Ç—å**. –ö–æ–≥–¥–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –æ—á–µ–Ω—å –º–Ω–æ–≥–æ (–±–æ–ª—å—à–µ 100), —Ç–æ –º–æ–¥–µ–ª—è–º —É–∂–µ —Å–ª–æ–∂–Ω–æ —Ä–∞–∑–¥–µ–ª–∏—Ç—å –∏—Ö –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è —Å–Ω–∏–∂–∞–µ—Ç—Å—è. \n",
    "\n",
    "–ï—Å–ª–∏ –±—ã —É –Ω–∞—Å –±—ã–ª –≤—Å–µ–≥–æ 1 —Ä—è–¥, –º—ã –±—ã –ø–æ–ª—É—á–µ–Ω–Ω—É—é —Ç–∞–±–ª–∏—Ü—É —Ä–∞–∑–¥–µ–ª–∏–ª–∏ –æ–±—Ä–∞—Ç–Ω–æ –Ω–∞ –¥–≤–µ –≤—ã–±–æ—Ä–∫–∏ –∏ –æ–±—É—á–∏–ª–∏ –±—ã 1 –º–æ–¥–µ–ª—å. \n",
    "\n",
    "–ù–æ —Ç–∞–∫ –∫–∞–∫ —É –Ω–∞—Å –µ—Å—Ç—å –µ—â—ë 87 —Ä—è–¥–æ–≤, —Ç–æ –¥–∞–≤–∞–π—Ç–µ –Ω–∞–ø–∏—à–µ–º —Ü–∏–∫–ª, –≤ –∫–æ—Ç–æ—Ä–æ–º –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ä—è–¥–∞ –ø–æ–≤—Ç–æ—Ä–∏–º —ç—Ç–∏ —à–∞–≥–∏, –æ–±—É—á–∏–º –º–æ–¥–µ–ª—å –∏ –ø–æ–ª—É—á–∏–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "_cell_id": "w4pox5yQPNNi3X1u"
   },
   "outputs": [],
   "source": [
    "# —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π, —á—Ç–æ–±—ã —Ñ–æ—Ä–º–∞—Ç —Ç–∞–±–ª–∏—Ü –±—ã–ª —Ç–∞–∫–æ–π –∂–µ –∫–∞–∫ —É ETNA \n",
    "def create_preds_df(y_test, y_pred, country, store, product):\n",
    "    y_test = y_test.rename(columns={\"target\": \"y_test\"})\n",
    "\n",
    "    y_test[\"y_pred\"] = y_pred\n",
    "    # –æ–∫—Ä—É–≥–ª—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è, –ø—ã—à–∫–∏ –ø–æ —á–∞—Å—Ç—è–º –º—ã –Ω–µ –ø—Ä–æ–¥–∞—ë–º\n",
    "    y_test[\"y_pred\"] = np.ceil(y_test[\"y_pred\"])\n",
    "\n",
    "    y_test[\"country\"] = country\n",
    "    y_test[\"store\"] = store\n",
    "    y_test[\"product\"] = product\n",
    "    preds_df = y_test.reset_index()\n",
    "    preds_df = preds_df[[\"timestamp\", \"y_test\", \"y_pred\", \"country\", \"store\", \"product\"]]\n",
    "\n",
    "    return preds_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°–æ–±–∏—Ä–∞–µ–º –≤—Å—ë –≤ –±–æ–ª—å—à—É—é –∏ –∫—Ä–∞—Å–∏–≤—É—é –∫—É—á–∫—É."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "_cell_id": "bYR8FTZ6pN4UAC3k"
   },
   "outputs": [],
   "source": [
    "def run_pipeline(model) -> pd.DataFrame:\n",
    "    # –≤—ã–¥–µ–ª—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å—Ç—Ä–∞–Ω—ã –≤ –¥–∞–Ω–Ω—ã—Ö \n",
    "    countries = df[\"country\"].unique()\n",
    "\n",
    "    # –≤—ã–¥–µ–ª—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –ø—ã—à–µ—á–Ω—ã–µ –≤ –¥–∞–Ω–Ω—ã—Ö\n",
    "    stores = df[\"store\"].unique()\n",
    "\n",
    "    # –≤—ã–¥–µ–ª—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –≤–∫—É—Å—ã –ø—ã—à–µ–∫ –≤ –¥–∞–Ω–Ω—ã—Ö\n",
    "    products = df[\"product\"].unique()\n",
    "\n",
    "    # –∫–æ–ª-–≤–æ –∫–æ–º–±–∏–Ω–∞—Ü–∏–π, —á–∏—Å—Ç–æ –¥–ª—è –∫—Ä–∞—Å–∏–≤—ã—Ö –ª–æ–≥–æ–≤ \n",
    "    iter_n = len(countries) * len(stores) * len(products)\n",
    "\n",
    "    missing_predictions = []\n",
    "    predictions = []\n",
    "\n",
    "    iter_counter = 1\n",
    "\n",
    "    # –∏—Ç–µ—Ä–∏—Ä—É–µ–º—Å—è –ø–æ –≤—Å–µ–º –∫–æ–º–±–∏–Ω–∞—Ü–∏—è–º —Å—Ç—Ä–∞–Ω–∞ - –ø—ã—à–µ—á–Ω–∞—è - –≤–∫—É—Å \n",
    "    for country in countries:\n",
    "        for store in stores:\n",
    "            for product in products:\n",
    "                print(f\"{iter_counter}/{iter_n} {country!r} - {store!r} - {product!r} processing ...\")\n",
    "\n",
    "                # –≤—ã–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ 1 –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Ä—è–¥\n",
    "                sample_df = df[\n",
    "                    (df[\"country\"] == country) \n",
    "                    & (df[\"store\"] == store)\n",
    "                    & (df[\"product\"] == product)\n",
    "                ]\n",
    "                nan_target_index = sample_df[sample_df[\"target\"].isna()].index\n",
    "                sample_df = sample_df[[\"target\"]].reset_index()\n",
    "\n",
    "                if sample_df.empty:\n",
    "                    missing_predictions.append((country, store, product))\n",
    "                    continue\n",
    "                \n",
    "                sample_df = sample_df.resample(\"1D\", on=\"timestamp\").sum()\n",
    "                sample_df.index = pd.to_datetime(sample_df.index)\n",
    "\n",
    "                # –≥–µ–Ω–µ—Ä–∏–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
    "                sample_df = create_dtime_features(sample_df)\n",
    "\n",
    "                # –≥–µ–Ω–µ—Ä–∏–º –ø—Ä–∞–∑–¥–Ω–∏—á–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ \n",
    "                sample_holidays_df = get_holiday_df(country)\n",
    "                sample_df = create_holiday_features(sample_df, sample_holidays_df)\n",
    "                \n",
    "                # –≥–µ–Ω–µ—Ä–∏–º –ª–∞–≥–Ω—É—Ç—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ \n",
    "                sample_df = create_lag_features(sample_df, horizon=HORIZON)\n",
    "\n",
    "                # –¥–µ–ª–∏–º –¥–∞–Ω–Ω—ã–µ –æ–±—Ä–∞—Ç–Ω–æ –Ω–∞ 2 –≤—ã–±–æ—Ä–∫–∏: train –∏ test\n",
    "\n",
    "                # –æ–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞\n",
    "                sample_train_df = sample_df.loc[: SPLIT_DATE - timedelta(days=1)]\n",
    "                # –¥—Ä–æ–ø–Ω–µ–º —Å—Ç—Ä–æ–∫–∏, –≥–¥–µ –µ—Å—Ç—å –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è \n",
    "                sample_train_df = sample_train_df.dropna()\n",
    "\n",
    "                # —Ç–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞\n",
    "                sample_test_df = sample_df.loc[SPLIT_DATE:]\n",
    "\n",
    "                # —Ä–∞–∑–¥–µ–ª—è–µ–º –≤—ã–±–æ—Ä–∫–∏ –Ω–∞ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏ —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é\n",
    "                X_train = sample_train_df.drop(columns=[\"target\"])\n",
    "                y_train = sample_train_df[\"target\"]\n",
    "\n",
    "                X_test = sample_test_df.drop(columns=[\"target\"])\n",
    "                # X_test = X_test.fillna(method=\"bfill\").fillna(method=\"ffill\")\n",
    "\n",
    "                y_test = sample_test_df[[\"target\"]]\n",
    "                nan_y_test_index = set(y_test.index) & set(nan_target_index)\n",
    "                y_test.loc[nan_y_test_index, \"target\"] = None\n",
    "\n",
    "                iter_counter += 1\n",
    "\n",
    "                # –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π –ø—Ä–æ–≤–µ—Ä—è–µ–º –¥–∞–Ω–Ω—ã–µ –Ω–∞ \"–ø—É—Å—Ç–æ—Ç—É\"\n",
    "                if X_train.empty or X_test.empty:\n",
    "                    missing_predictions.append((country, store, product))\n",
    "                    continue\n",
    "\n",
    "                # –æ–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å\n",
    "                model.fit(X_train, y_train)\n",
    "                # –ø–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
    "                y_pred = model.predict(X_test)\n",
    "\n",
    "                # —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –≤ —É–¥–æ–±–Ω—ã–π –Ω–∞–º —Ñ–æ—Ä–º–∞—Ç\n",
    "                preds_df = create_preds_df(y_test, y_pred, country, store, product)\n",
    "                predictions.append(preds_df)\n",
    "\n",
    "    # –æ–±—ä–µ–¥–∏–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –æ—Ç–æ –≤—Å–µ—Ö —Ä—è–¥–æ–≤ –≤ –µ–¥–∏–Ω—É—é —Ç–∞–±–ª–∏—Ü—É\n",
    "    predictions_df = pd.concat(predictions)\n",
    "    return predictions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í—Å–µ –ø—Ä–∏–≥–æ—Ç–æ–≤–ª–µ–Ω–∏—è —Å–¥–µ–ª–∞–Ω—ã, –ø–æ–≥–Ω–∞–ª–∏ —É—á–∏—Ç—å –º–æ–¥–µ–ª—å–∫–∏! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –õ–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–µ—Ä–≤–æ–π –±—É–¥–µ—Ç —Å—Ç–∞—Ä–∞—è –¥–æ–±—Ä–∞—è –º–æ–¥–µ–ª—å –ª–∏–Ω–µ–π–Ω–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "_cell_id": "lDWrpfZbnQsTc5x8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/90 'Canada' - 'Donots' - 'Chocolate' processing ...\n",
      "2/90 'Canada' - 'Donots' - 'Coffee' processing ...\n",
      "3/90 'Canada' - 'Donots' - 'Cream' processing ...\n",
      "4/90 'Canada' - 'Donots' - 'Strawberry Jam' processing ...\n",
      "5/90 'Canada' - 'Donots' - 'Sugar Powder' processing ...\n",
      "6/90 'Canada' - 'Fluffy Bounce' - 'Chocolate' processing ...\n",
      "6/90 'Canada' - 'Fluffy Bounce' - 'Coffee' processing ...\n",
      "7/90 'Canada' - 'Fluffy Bounce' - 'Cream' processing ...\n",
      "8/90 'Canada' - 'Fluffy Bounce' - 'Strawberry Jam' processing ...\n",
      "9/90 'Canada' - 'Fluffy Bounce' - 'Sugar Powder' processing ...\n",
      "10/90 'Canada' - 'Pyshka' - 'Chocolate' processing ...\n",
      "11/90 'Canada' - 'Pyshka' - 'Coffee' processing ...\n",
      "12/90 'Canada' - 'Pyshka' - 'Cream' processing ...\n",
      "13/90 'Canada' - 'Pyshka' - 'Strawberry Jam' processing ...\n",
      "14/90 'Canada' - 'Pyshka' - 'Sugar Powder' processing ...\n",
      "15/90 'Finland' - 'Donots' - 'Chocolate' processing ...\n",
      "16/90 'Finland' - 'Donots' - 'Coffee' processing ...\n",
      "17/90 'Finland' - 'Donots' - 'Cream' processing ...\n",
      "18/90 'Finland' - 'Donots' - 'Strawberry Jam' processing ...\n",
      "19/90 'Finland' - 'Donots' - 'Sugar Powder' processing ...\n",
      "20/90 'Finland' - 'Fluffy Bounce' - 'Chocolate' processing ...\n",
      "21/90 'Finland' - 'Fluffy Bounce' - 'Coffee' processing ...\n",
      "22/90 'Finland' - 'Fluffy Bounce' - 'Cream' processing ...\n",
      "23/90 'Finland' - 'Fluffy Bounce' - 'Strawberry Jam' processing ...\n",
      "24/90 'Finland' - 'Fluffy Bounce' - 'Sugar Powder' processing ...\n",
      "25/90 'Finland' - 'Pyshka' - 'Chocolate' processing ...\n",
      "26/90 'Finland' - 'Pyshka' - 'Coffee' processing ...\n",
      "27/90 'Finland' - 'Pyshka' - 'Cream' processing ...\n",
      "28/90 'Finland' - 'Pyshka' - 'Strawberry Jam' processing ...\n",
      "29/90 'Finland' - 'Pyshka' - 'Sugar Powder' processing ...\n",
      "30/90 'Italy' - 'Donots' - 'Chocolate' processing ...\n",
      "31/90 'Italy' - 'Donots' - 'Coffee' processing ...\n",
      "32/90 'Italy' - 'Donots' - 'Cream' processing ...\n",
      "33/90 'Italy' - 'Donots' - 'Strawberry Jam' processing ...\n",
      "34/90 'Italy' - 'Donots' - 'Sugar Powder' processing ...\n",
      "35/90 'Italy' - 'Fluffy Bounce' - 'Chocolate' processing ...\n",
      "36/90 'Italy' - 'Fluffy Bounce' - 'Coffee' processing ...\n",
      "37/90 'Italy' - 'Fluffy Bounce' - 'Cream' processing ...\n",
      "38/90 'Italy' - 'Fluffy Bounce' - 'Strawberry Jam' processing ...\n",
      "39/90 'Italy' - 'Fluffy Bounce' - 'Sugar Powder' processing ...\n",
      "40/90 'Italy' - 'Pyshka' - 'Chocolate' processing ...\n",
      "41/90 'Italy' - 'Pyshka' - 'Coffee' processing ...\n",
      "42/90 'Italy' - 'Pyshka' - 'Cream' processing ...\n",
      "43/90 'Italy' - 'Pyshka' - 'Strawberry Jam' processing ...\n",
      "44/90 'Italy' - 'Pyshka' - 'Sugar Powder' processing ...\n",
      "45/90 'Kenya' - 'Donots' - 'Chocolate' processing ...\n",
      "46/90 'Kenya' - 'Donots' - 'Coffee' processing ...\n",
      "47/90 'Kenya' - 'Donots' - 'Cream' processing ...\n",
      "48/90 'Kenya' - 'Donots' - 'Strawberry Jam' processing ...\n",
      "49/90 'Kenya' - 'Donots' - 'Sugar Powder' processing ...\n",
      "50/90 'Kenya' - 'Fluffy Bounce' - 'Chocolate' processing ...\n",
      "50/90 'Kenya' - 'Fluffy Bounce' - 'Coffee' processing ...\n",
      "51/90 'Kenya' - 'Fluffy Bounce' - 'Cream' processing ...\n",
      "52/90 'Kenya' - 'Fluffy Bounce' - 'Strawberry Jam' processing ...\n",
      "53/90 'Kenya' - 'Fluffy Bounce' - 'Sugar Powder' processing ...\n",
      "54/90 'Kenya' - 'Pyshka' - 'Chocolate' processing ...\n",
      "55/90 'Kenya' - 'Pyshka' - 'Coffee' processing ...\n",
      "56/90 'Kenya' - 'Pyshka' - 'Cream' processing ...\n",
      "57/90 'Kenya' - 'Pyshka' - 'Strawberry Jam' processing ...\n",
      "58/90 'Kenya' - 'Pyshka' - 'Sugar Powder' processing ...\n",
      "59/90 'Norway' - 'Donots' - 'Chocolate' processing ...\n",
      "60/90 'Norway' - 'Donots' - 'Coffee' processing ...\n",
      "61/90 'Norway' - 'Donots' - 'Cream' processing ...\n",
      "62/90 'Norway' - 'Donots' - 'Strawberry Jam' processing ...\n",
      "63/90 'Norway' - 'Donots' - 'Sugar Powder' processing ...\n",
      "64/90 'Norway' - 'Fluffy Bounce' - 'Chocolate' processing ...\n",
      "65/90 'Norway' - 'Fluffy Bounce' - 'Coffee' processing ...\n",
      "66/90 'Norway' - 'Fluffy Bounce' - 'Cream' processing ...\n",
      "67/90 'Norway' - 'Fluffy Bounce' - 'Strawberry Jam' processing ...\n",
      "68/90 'Norway' - 'Fluffy Bounce' - 'Sugar Powder' processing ...\n",
      "69/90 'Norway' - 'Pyshka' - 'Chocolate' processing ...\n",
      "70/90 'Norway' - 'Pyshka' - 'Coffee' processing ...\n",
      "71/90 'Norway' - 'Pyshka' - 'Cream' processing ...\n",
      "72/90 'Norway' - 'Pyshka' - 'Strawberry Jam' processing ...\n",
      "73/90 'Norway' - 'Pyshka' - 'Sugar Powder' processing ...\n",
      "74/90 'Singapore' - 'Donots' - 'Chocolate' processing ...\n",
      "75/90 'Singapore' - 'Donots' - 'Coffee' processing ...\n",
      "76/90 'Singapore' - 'Donots' - 'Cream' processing ...\n",
      "77/90 'Singapore' - 'Donots' - 'Strawberry Jam' processing ...\n",
      "78/90 'Singapore' - 'Donots' - 'Sugar Powder' processing ...\n",
      "79/90 'Singapore' - 'Fluffy Bounce' - 'Chocolate' processing ...\n",
      "80/90 'Singapore' - 'Fluffy Bounce' - 'Coffee' processing ...\n",
      "81/90 'Singapore' - 'Fluffy Bounce' - 'Cream' processing ...\n",
      "82/90 'Singapore' - 'Fluffy Bounce' - 'Strawberry Jam' processing ...\n",
      "83/90 'Singapore' - 'Fluffy Bounce' - 'Sugar Powder' processing ...\n",
      "84/90 'Singapore' - 'Pyshka' - 'Chocolate' processing ...\n",
      "85/90 'Singapore' - 'Pyshka' - 'Coffee' processing ...\n",
      "86/90 'Singapore' - 'Pyshka' - 'Cream' processing ...\n",
      "87/90 'Singapore' - 'Pyshka' - 'Strawberry Jam' processing ...\n",
      "88/90 'Singapore' - 'Pyshka' - 'Sugar Powder' processing ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>y_test</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>country</th>\n",
       "      <th>store</th>\n",
       "      <th>product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>224.0</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Donots</td>\n",
       "      <td>Chocolate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-12-02</td>\n",
       "      <td>212.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Donots</td>\n",
       "      <td>Chocolate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-12-03</td>\n",
       "      <td>210.0</td>\n",
       "      <td>231.0</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Donots</td>\n",
       "      <td>Chocolate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-12-04</td>\n",
       "      <td>231.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Donots</td>\n",
       "      <td>Chocolate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-12-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>225.0</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Donots</td>\n",
       "      <td>Chocolate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   timestamp  y_test  y_pred country   store    product\n",
       "0 2016-12-01     NaN   224.0  Canada  Donots  Chocolate\n",
       "1 2016-12-02   212.0   226.0  Canada  Donots  Chocolate\n",
       "2 2016-12-03   210.0   231.0  Canada  Donots  Chocolate\n",
       "3 2016-12-04   231.0   242.0  Canada  Donots  Chocolate\n",
       "4 2016-12-05     NaN   225.0  Canada  Donots  Chocolate"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linreg_model = LinearRegression()\n",
    "linreg_predictions = run_pipeline(linreg_model)\n",
    "\n",
    "linreg_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "_cell_id": "KwaNMN2Xv4t8MOwo"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2728, 6)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linreg_predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢–∞–∫–∂–µ –∫–∞–∫ —Å –º–æ–¥–µ–ª—å—é ETNA –æ—Ç—Ä–∏—Å—É–µ–º –Ω–∞ –≥—Ä–∞—Ñ–∏–∫–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ü–µ–ª–µ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—å—é."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "_cell_id": "1YEdqMWugyI8levc"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "513a0479a1274f06a5b66ed50632616c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='country', options=('Canada', 'Finland', 'Italy', 'Kenya', 'Norway'‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@widgets.interact(\n",
    "    country=widgets.Dropdown(options=linreg_predictions[\"country\"].unique()),\n",
    "    store=widgets.Dropdown(options=linreg_predictions[\"store\"].unique()),\n",
    "    products=widgets.Dropdown(options=linreg_predictions[\"product\"].unique()),\n",
    "    start_date=widgets.DatePicker(value=linreg_predictions[\"timestamp\"].min()),\n",
    ")\n",
    "def show_linreg_predictions(country: str, store: str, products: str, start_date: date):  # noqa: D103\n",
    "    start_date = pd.Timestamp(start_date)\n",
    "    plot_df = linreg_predictions[\n",
    "        (linreg_predictions[\"country\"] == country)\n",
    "        & (linreg_predictions[\"store\"] == store)\n",
    "        & (linreg_predictions[\"product\"] == products)\n",
    "        & (linreg_predictions[\"timestamp\"].dt.date >= start_date.date())\n",
    "    ]\n",
    "\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=1, figsize=(15, 4))\n",
    "\n",
    "    axs.plot(plot_df[\"timestamp\"], plot_df[\"y_pred\"], marker=\"o\", label=\"pred\")\n",
    "    # —É–±–µ—Ä—ë–º –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è, —á—Ç–æ–±—ã —Ç–æ—á–∫–∏ —Å–æ–µ–¥–∏–Ω–∏–ª–∏—Å—å –º–µ–∂–¥—É —Å–æ–±–æ–π - —á–∏—Å—Ç–æ –¥–ª—è –∫—Ä–∞—Å–æ—Ç—ã\n",
    "    plot_df = plot_df.dropna()\n",
    "    axs.plot(plot_df[\"timestamp\"], plot_df[\"y_test\"], marker=\"o\", label=\"true\")\n",
    "    axs.set_xlabel(\"–î–∞—Ç–∞\")\n",
    "    axs.set_ylabel(\"–ö–æ–ª-–≤–æ –ø—Ä–æ–¥–∞–Ω–Ω—ã—Ö –ø—ã—à–µ–∫\")\n",
    "    axs.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ò —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–∞–π–ª, —á—Ç–æ–±—ã –Ω–µ –ø–æ—Ç–µ—Ä—è—Ç—å."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "_cell_id": "0pFIP4bxZOrZYfV2"
   },
   "outputs": [],
   "source": [
    "pred_dpath = Path(\"forecast_predictions\")\n",
    "pred_dpath.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pred_fpath = pred_dpath / \"linear_regression_predictions.csv\"\n",
    "linreg_predictions.to_csv(pred_fpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGBM : –ë—É—Å—Ç–∏–Ω–≥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –æ–±—É—á–∏–º –º–æ–¥–µ–ª—å –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ –±—É—Å—Ç–∏–Ω–≥–∞."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "_cell_id": "3syPU8Y7N2LfAeGa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/90 'Canada' - 'Donots' - 'Chocolate' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001058 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2498, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 223.421157\n",
      "2/90 'Canada' - 'Donots' - 'Coffee' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000898 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2498, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 655.552842\n",
      "3/90 'Canada' - 'Donots' - 'Cream' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001115 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2498, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 1450.107286\n",
      "4/90 'Canada' - 'Donots' - 'Strawberry Jam' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001023 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2498, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 763.661729\n",
      "5/90 'Canada' - 'Donots' - 'Sugar Powder' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001013 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2498, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 1199.730985\n",
      "6/90 'Canada' - 'Fluffy Bounce' - 'Chocolate' processing ...\n",
      "6/90 'Canada' - 'Fluffy Bounce' - 'Coffee' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001076 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12287\n",
      "[LightGBM] [Info] Number of data points in the train set: 2498, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 323.781879\n",
      "7/90 'Canada' - 'Fluffy Bounce' - 'Cream' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001063 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2498, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 716.158927\n",
      "8/90 'Canada' - 'Fluffy Bounce' - 'Strawberry Jam' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001024 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2498, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 378.072058\n",
      "9/90 'Canada' - 'Fluffy Bounce' - 'Sugar Powder' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001131 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2498, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 592.843074\n",
      "10/90 'Canada' - 'Pyshka' - 'Chocolate' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000885 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12362\n",
      "[LightGBM] [Info] Number of data points in the train set: 2498, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 243.759190\n",
      "11/90 'Canada' - 'Pyshka' - 'Coffee' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000961 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2498, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 773.361089\n",
      "12/90 'Canada' - 'Pyshka' - 'Cream' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001199 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2498, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 1713.745797\n",
      "13/90 'Canada' - 'Pyshka' - 'Strawberry Jam' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001039 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2498, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 904.898319\n",
      "14/90 'Canada' - 'Pyshka' - 'Sugar Powder' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001169 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2498, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 1419.087270\n",
      "15/90 'Finland' - 'Donots' - 'Chocolate' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000858 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10757\n",
      "[LightGBM] [Info] Number of data points in the train set: 2498, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 198.364692\n",
      "16/90 'Finland' - 'Donots' - 'Coffee' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001042 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2498, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 641.981986\n",
      "17/90 'Finland' - 'Donots' - 'Cream' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000985 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2498, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 1424.184948\n",
      "18/90 'Finland' - 'Donots' - 'Strawberry Jam' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001079 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2498, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 751.958767\n",
      "19/90 'Finland' - 'Donots' - 'Sugar Powder' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001019 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2498, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 1175.108887\n",
      "20/90 'Finland' - 'Fluffy Bounce' - 'Chocolate' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000818 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9797\n",
      "[LightGBM] [Info] Number of data points in the train set: 2498, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 97.722178\n",
      "21/90 'Finland' - 'Fluffy Bounce' - 'Coffee' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001052 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12152\n",
      "[LightGBM] [Info] Number of data points in the train set: 2498, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 317.217374\n",
      "22/90 'Finland' - 'Fluffy Bounce' - 'Cream' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000960 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2498, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 703.277422\n",
      "23/90 'Finland' - 'Fluffy Bounce' - 'Strawberry Jam' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001011 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2498, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 372.005204\n",
      "24/90 'Finland' - 'Fluffy Bounce' - 'Sugar Powder' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000871 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2498, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 581.419536\n",
      "25/90 'Finland' - 'Pyshka' - 'Chocolate' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000971 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11102\n",
      "[LightGBM] [Info] Number of data points in the train set: 2498, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 234.510008\n",
      "26/90 'Finland' - 'Pyshka' - 'Coffee' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000978 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2498, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 758.447958\n",
      "27/90 'Finland' - 'Pyshka' - 'Cream' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000995 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2498, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 1682.744996\n",
      "28/90 'Finland' - 'Pyshka' - 'Strawberry Jam' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000990 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2498, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 890.984788\n",
      "29/90 'Finland' - 'Pyshka' - 'Sugar Powder' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001013 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2498, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 1387.982786\n",
      "30/90 'Italy' - 'Donots' - 'Chocolate' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000933 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10292\n",
      "[LightGBM] [Info] Number of data points in the train set: 2499, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 143.493798\n",
      "31/90 'Italy' - 'Donots' - 'Coffee' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000805 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2499, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 465.537815\n",
      "32/90 'Italy' - 'Donots' - 'Cream' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001088 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2499, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 1031.364146\n",
      "33/90 'Italy' - 'Donots' - 'Strawberry Jam' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001016 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2499, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 544.560224\n",
      "34/90 'Italy' - 'Donots' - 'Sugar Powder' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001294 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2499, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 851.634654\n",
      "35/90 'Italy' - 'Fluffy Bounce' - 'Chocolate' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000982 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9542\n",
      "[LightGBM] [Info] Number of data points in the train set: 2499, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 70.593037\n",
      "36/90 'Italy' - 'Fluffy Bounce' - 'Coffee' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001061 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11237\n",
      "[LightGBM] [Info] Number of data points in the train set: 2499, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 229.917967\n",
      "37/90 'Italy' - 'Fluffy Bounce' - 'Cream' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001003 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12362\n",
      "[LightGBM] [Info] Number of data points in the train set: 2499, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 509.466587\n",
      "38/90 'Italy' - 'Fluffy Bounce' - 'Strawberry Jam' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000997 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12167\n",
      "[LightGBM] [Info] Number of data points in the train set: 2499, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 268.876751\n",
      "39/90 'Italy' - 'Fluffy Bounce' - 'Sugar Powder' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000871 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12287\n",
      "[LightGBM] [Info] Number of data points in the train set: 2499, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 421.589036\n",
      "40/90 'Italy' - 'Pyshka' - 'Chocolate' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000979 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10562\n",
      "[LightGBM] [Info] Number of data points in the train set: 2499, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 169.763505\n",
      "41/90 'Italy' - 'Pyshka' - 'Coffee' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001076 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2499, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 550.161265\n",
      "42/90 'Italy' - 'Pyshka' - 'Cream' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001065 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2499, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 1219.592237\n",
      "43/90 'Italy' - 'Pyshka' - 'Strawberry Jam' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000959 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2499, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 644.228491\n",
      "44/90 'Italy' - 'Pyshka' - 'Sugar Powder' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000894 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2499, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 1007.847139\n",
      "45/90 'Kenya' - 'Donots' - 'Chocolate' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000951 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12362\n",
      "[LightGBM] [Info] Number of data points in the train set: 2498, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 5.229406\n",
      "46/90 'Kenya' - 'Donots' - 'Coffee' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000550 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5492\n",
      "[LightGBM] [Info] Number of data points in the train set: 2498, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 15.614892\n",
      "47/90 'Kenya' - 'Donots' - 'Cream' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000805 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7547\n",
      "[LightGBM] [Info] Number of data points in the train set: 2498, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 35.217374\n",
      "48/90 'Kenya' - 'Donots' - 'Strawberry Jam' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000529 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6422\n",
      "[LightGBM] [Info] Number of data points in the train set: 2498, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 18.389512\n",
      "49/90 'Kenya' - 'Donots' - 'Sugar Powder' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000805 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7082\n",
      "[LightGBM] [Info] Number of data points in the train set: 2498, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 28.997998\n",
      "50/90 'Kenya' - 'Fluffy Bounce' - 'Chocolate' processing ...\n",
      "50/90 'Kenya' - 'Fluffy Bounce' - 'Coffee' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000538 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4982\n",
      "[LightGBM] [Info] Number of data points in the train set: 2498, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 7.498309\n",
      "51/90 'Kenya' - 'Fluffy Bounce' - 'Cream' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000567 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4577\n",
      "[LightGBM] [Info] Number of data points in the train set: 2498, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 17.123699\n",
      "52/90 'Kenya' - 'Fluffy Bounce' - 'Strawberry Jam' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000446 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3902\n",
      "[LightGBM] [Info] Number of data points in the train set: 2498, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 8.807192\n",
      "53/90 'Kenya' - 'Fluffy Bounce' - 'Sugar Powder' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000535 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4157\n",
      "[LightGBM] [Info] Number of data points in the train set: 2498, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 14.094876\n",
      "54/90 'Kenya' - 'Pyshka' - 'Chocolate' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000891 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12107\n",
      "[LightGBM] [Info] Number of data points in the train set: 2498, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 5.700512\n",
      "55/90 'Kenya' - 'Pyshka' - 'Coffee' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000608 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6242\n",
      "[LightGBM] [Info] Number of data points in the train set: 2498, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 18.564051\n",
      "56/90 'Kenya' - 'Pyshka' - 'Cream' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000686 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8012\n",
      "[LightGBM] [Info] Number of data points in the train set: 2498, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 41.646517\n",
      "57/90 'Kenya' - 'Pyshka' - 'Strawberry Jam' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000699 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7187\n",
      "[LightGBM] [Info] Number of data points in the train set: 2498, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 21.798639\n",
      "58/90 'Kenya' - 'Pyshka' - 'Sugar Powder' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000592 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7592\n",
      "[LightGBM] [Info] Number of data points in the train set: 2498, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 34.427142\n",
      "59/90 'Norway' - 'Donots' - 'Chocolate' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000898 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2499, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 378.651060\n",
      "60/90 'Norway' - 'Donots' - 'Coffee' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000871 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2499, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 1226.501401\n",
      "61/90 'Norway' - 'Donots' - 'Cream' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000993 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2499, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 2711.737495\n",
      "62/90 'Norway' - 'Donots' - 'Strawberry Jam' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000928 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2499, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 1436.365346\n",
      "63/90 'Norway' - 'Donots' - 'Sugar Powder' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000983 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2499, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 2241.661064\n",
      "64/90 'Norway' - 'Fluffy Bounce' - 'Chocolate' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000940 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11012\n",
      "[LightGBM] [Info] Number of data points in the train set: 2499, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 186.642657\n",
      "65/90 'Norway' - 'Fluffy Bounce' - 'Coffee' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000843 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2499, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 604.633053\n",
      "66/90 'Norway' - 'Fluffy Bounce' - 'Cream' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000944 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2499, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 1342.216487\n",
      "67/90 'Norway' - 'Fluffy Bounce' - 'Strawberry Jam' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001030 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2499, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 709.060824\n",
      "68/90 'Norway' - 'Fluffy Bounce' - 'Sugar Powder' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001012 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2499, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 1108.016006\n",
      "69/90 'Norway' - 'Pyshka' - 'Chocolate' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001022 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2499, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 446.936375\n",
      "70/90 'Norway' - 'Pyshka' - 'Coffee' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000882 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2499, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 1445.029612\n",
      "71/90 'Norway' - 'Pyshka' - 'Cream' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001210 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2499, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 3205.389356\n",
      "72/90 'Norway' - 'Pyshka' - 'Strawberry Jam' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001166 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2499, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 1693.968387\n",
      "73/90 'Norway' - 'Pyshka' - 'Sugar Powder' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001138 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2499, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 2648.170868\n",
      "74/90 'Singapore' - 'Donots' - 'Chocolate' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001016 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11147\n",
      "[LightGBM] [Info] Number of data points in the train set: 2498, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 228.879103\n",
      "75/90 'Singapore' - 'Donots' - 'Coffee' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001187 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2498, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 741.389111\n",
      "76/90 'Singapore' - 'Donots' - 'Cream' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001010 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2498, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 1643.154123\n",
      "77/90 'Singapore' - 'Donots' - 'Strawberry Jam' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000991 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2498, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 867.407126\n",
      "78/90 'Singapore' - 'Donots' - 'Sugar Powder' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000992 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2498, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 1357.083667\n",
      "79/90 'Singapore' - 'Fluffy Bounce' - 'Chocolate' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000980 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9977\n",
      "[LightGBM] [Info] Number of data points in the train set: 2498, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 112.807446\n",
      "80/90 'Singapore' - 'Fluffy Bounce' - 'Coffee' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001009 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12362\n",
      "[LightGBM] [Info] Number of data points in the train set: 2498, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 366.274620\n",
      "81/90 'Singapore' - 'Fluffy Bounce' - 'Cream' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000965 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2498, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 813.911129\n",
      "82/90 'Singapore' - 'Fluffy Bounce' - 'Strawberry Jam' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001094 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2498, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 428.262210\n",
      "83/90 'Singapore' - 'Fluffy Bounce' - 'Sugar Powder' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001064 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2498, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 671.515212\n",
      "84/90 'Singapore' - 'Pyshka' - 'Chocolate' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001053 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 11417\n",
      "[LightGBM] [Info] Number of data points in the train set: 2498, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 270.774219\n",
      "85/90 'Singapore' - 'Pyshka' - 'Coffee' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001136 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2498, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 875.787030\n",
      "86/90 'Singapore' - 'Pyshka' - 'Cream' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001155 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2498, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 1937.058046\n",
      "87/90 'Singapore' - 'Pyshka' - 'Strawberry Jam' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001555 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2498, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 1024.977182\n",
      "88/90 'Singapore' - 'Pyshka' - 'Sugar Powder' processing ...\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001621 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2498, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 1604.396317\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>y_test</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>country</th>\n",
       "      <th>store</th>\n",
       "      <th>product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>212.0</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Donots</td>\n",
       "      <td>Chocolate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-12-02</td>\n",
       "      <td>212.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Donots</td>\n",
       "      <td>Chocolate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-12-03</td>\n",
       "      <td>210.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Donots</td>\n",
       "      <td>Chocolate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-12-04</td>\n",
       "      <td>231.0</td>\n",
       "      <td>231.0</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Donots</td>\n",
       "      <td>Chocolate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-12-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>218.0</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Donots</td>\n",
       "      <td>Chocolate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   timestamp  y_test  y_pred country   store    product\n",
       "0 2016-12-01     NaN   212.0  Canada  Donots  Chocolate\n",
       "1 2016-12-02   212.0   211.0  Canada  Donots  Chocolate\n",
       "2 2016-12-03   210.0   210.0  Canada  Donots  Chocolate\n",
       "3 2016-12-04   231.0   231.0  Canada  Donots  Chocolate\n",
       "4 2016-12-05     NaN   218.0  Canada  Donots  Chocolate"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm_model = LGBMRegressor(random_state=42)\n",
    "lgbm_predictions = run_pipeline(lgbm_model)\n",
    "\n",
    "lgbm_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "_cell_id": "3peG9LfBQeIMKQJC"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2728, 6)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm_predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–£–≥–∞–¥–∞–π—Ç–µ, —á—Ç–æ? –û—Ç—Ä–∏—Å—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "_cell_id": "2HScDF5S2npQiTpY"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "560b01ae42eb440ba50f004431978bea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='country', options=('Canada', 'Finland', 'Italy', 'Kenya', 'Norway'‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@widgets.interact(\n",
    "    country=widgets.Dropdown(options=lgbm_predictions[\"country\"].unique()),\n",
    "    store=widgets.Dropdown(options=lgbm_predictions[\"store\"].unique()),\n",
    "    products=widgets.Dropdown(options=lgbm_predictions[\"product\"].unique()),\n",
    "    start_date=widgets.DatePicker(value=lgbm_predictions[\"timestamp\"].min()),\n",
    ")\n",
    "def show_lgbm_predictions(country: str, store: str, products: str, start_date: date):  # noqa: D103\n",
    "    start_date = pd.Timestamp(start_date)\n",
    "    plot_df = lgbm_predictions[\n",
    "        (lgbm_predictions[\"country\"] == country)\n",
    "        & (lgbm_predictions[\"store\"] == store)\n",
    "        & (lgbm_predictions[\"product\"] == products)\n",
    "        & (lgbm_predictions[\"timestamp\"].dt.date >= start_date.date())\n",
    "    ]\n",
    "\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=1, figsize=(15, 4))\n",
    "\n",
    "    axs.plot(plot_df[\"timestamp\"], plot_df[\"y_pred\"], marker=\"o\", label=\"pred\")\n",
    "    # —É–±–µ—Ä—ë–º –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è, —á—Ç–æ–±—ã —Ç–æ—á–∫–∏ —Å–æ–µ–¥–∏–Ω–∏–ª–∏—Å—å –º–µ–∂–¥—É —Å–æ–±–æ–π - —á–∏—Å—Ç–æ –¥–ª—è –∫—Ä–∞—Å–æ—Ç—ã\n",
    "    plot_df = plot_df.dropna()\n",
    "    axs.plot(plot_df[\"timestamp\"], plot_df[\"y_test\"], marker=\"o\", label=\"true\")\n",
    "    axs.set_xlabel(\"–î–∞—Ç–∞\")\n",
    "    axs.set_ylabel(\"–ö–æ–ª-–≤–æ –ø—Ä–æ–¥–∞–Ω–Ω—ã—Ö –ø—ã—à–µ–∫\")\n",
    "    axs.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ñ–∞–π–ª."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "_cell_id": "hQzkCxpBtiSin6Et"
   },
   "outputs": [],
   "source": [
    "pred_dpath = Path(\"forecast_predictions\")\n",
    "pred_dpath.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pred_fpath = pred_dpath / \"lgbm_predictions.csv\"\n",
    "lgbm_predictions.to_csv(pred_fpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–£—Ä–∞! –ú–æ–¥–µ–ª–∏ –æ–±—É—á–µ–Ω—ã, —Ç–µ–ø–µ—Ä—å –æ—Å—Ç–∞–ª–æ—Å—å —Å–∞–º–æ–µ –≤–∫—É—Å–Ω–æ–µ - –ø–æ—Å—á–∏—Ç–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏, —Å—Ä–∞–≤–Ω–∏—Ç—å –º–æ–¥–µ–ª–∏ –∏ —Å–¥–µ–ª–∞—Ç—å –≤—ã–≤–æ–¥—ã. \n",
    "\n",
    "–≠—Ç–∏–º –∑–∞–π–º—ë–º—Å—è –≤ —Å–ª–µ–¥—É—é—â–µ–º –Ω–æ—É—Ç–±—É–∫–µ. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ó–∞–∫–ª—é—á–µ–Ω–∏–µ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í —ç—Ç–æ–º –Ω–æ—É—Ç–±—É–∫–µ –≤—ã –ø–æ–∑–Ω–∞–∫–æ–º–∏–ª–∏—Å—å —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–π –º–æ–¥–µ–ª—å—é —Å –ø–∞—Ñ–æ—Å–Ω—ã–º –Ω–∞–∑–≤–∞–Ω–∏–µ–º –•–æ–ª—å—Ç–∞-–í–∏–Ω—Ç–µ—Ä—Å–∞. \n",
    "\n",
    "–ü–æ—Å–º–æ—Ç—Ä–µ–ª–∏ –∫–∞–∫ –Ω–∞—Ç—è–Ω—É—Ç—å –∑–Ω–∞–∫–æ–º—ã–µ –º–æ–¥–µ–ª–∏ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –Ω–∞ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ. \n",
    "\n",
    "–ò –ø–æ–≥—Ä—É–∑–∏–ª–∏—Å—å –≤ —É–¥–∏–≤–∏—Ç–µ–ª—å–Ω—ã–π –º–∏—Ä –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –í–æ–ø—Ä–æ—Å—ã –¥–ª—è –∑–∞–∫—Ä–µ–ø–ª–µ–Ω–∏—è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. –í —á—ë–º —Ä–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É —Ç—Ä–µ–Ω–¥–æ–º –∏ —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç—å—é? \n",
    "2. –ú–æ–∂–µ—Ç –ª–∏ –±—ã—Ç—å —Ç–∞–∫, —á—Ç–æ –≤ —Ä—è–¥—É –µ—Å—Ç—å —Ç—Ä–µ–Ω–¥, –Ω–æ –Ω–µ—Ç —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç–∏? \n",
    "3. –ú–æ–∂–Ω–æ –ª–∏ –æ–±—É—á–∏—Ç—å —Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –±–µ–∑ –ª–∞–≥–Ω—É—Ç—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤? \n",
    "4. –ö–∞–∫ –æ–±—É—á–∏—Ç—å —Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—É—é –º–æ–¥–µ–ª—å, –µ—Å–ª–∏ –≤ –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤? \n",
    "5. –ß—Ç–æ –Ω—É–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å, —á—Ç–æ–±—ã –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –•–æ–ª—å—Ç–∞-–í–∏–Ω—Ç–µ—Ä—Å–∞, –µ—Å–ª–∏ –≤ –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
